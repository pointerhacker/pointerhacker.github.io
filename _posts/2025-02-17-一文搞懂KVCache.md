---
layout: post
title: 一文搞懂KVCache
categories: [LLM]
tags: LLM
---

## 简介
**概述**：

1. 简介：我们将探索键值（KV）缓存如何通过在内存使用和计算时间之间进行巧妙的权衡来使其生成文本之类的语言模型更快地生成文本。
2. MLA和其他技巧：然后，我们将研究最近的11篇研究论文，其中包括DeepSeek的多头潜在注意力（MLA），它们基于这个基本思想，以使LLM推论提出更高的时间效率。

## 理解问题：为什么文本生成很慢

让我们从一个简单的类比开始。想象一下，您正在写一个故事，对于您写的每个新词，您需要重新阅读到目前为止的整个故事以保持一致性。您的故事的时间越长，您花费的时间就越多。这正是大型语言模型在文本生成过程中所面临的。



### 基本构件：Self-Attention

现代语言模型的核心是一种称为**Self-Attention**的机制。对于长度为n的token的序列（将token视为与字大致相对应的)，每个token都需要“查看”或“注意”所有其他令牌才能理解上下文。

这个全面的查找过程的计算成本随序列长度增长：

- 对于 n 个token，每个token都需要查看所有n个token
- 这意味着成本与 $$ n×n=n^2$$ 成正比
- 在数学符号中，我们将其写为 $$O(n^2)$$ 复杂性



### 真正的问题：单位时间只能生成一个Token

> 在代码层面、在生成attion score 的时候 
>
> - [ 1,dim ] * [ dim,seq ] * [seq*dim]
>   - [ dim,seq ] * [seq*dim] 计算导致的 $$n^2$$
>   - [ 1,dim ]  = 1

当语言模型生成文本时，它一次只能生成一个Token，而这重计算是昂贵的：

1. **First token**: Look at 1 token $$(cost: O(1^2)$$
2. **Second token**: Look at 2 tokens  $$(cost: O(2^2)$$
3. **Third token**: Look at 3 tokens  $$(cost: O(3^2)$$
4. And so on until the n*n*-th token: Look at n*n* tokens  $$(cost: O(n^2)$$

如果我们将所有这些成本添加到生成长度 n 的顺序中，我们将得到：

$$O(1^{2}+2^{2}+3^{2}+\cdots +n^{2})\approx O(n^{3})$$ 

$$O(n^{3})$$  成本意味着随着您的文本越来越长，生成文本需要的时间增长很快。

例如，生成序列的长度如果增加2杯，那么需要的时间大约需要大约八倍！显然，我们需要一种更好的方法。



## 解决方案：Key-Value (KV) Cache

![KV 缓存示意图](http://pointerhacker.github.io/imgs/posts/deekseek-kv/kv-cache-optimization.png)

KVCache背后的关键思想是我们正在做很多多余的工作。生成每个新令牌时，我们正在为我们之前已经处理过的所有以前的令牌重新计算内容。让我们看看如何解决此问题。

### 什么是KVCache

> 键值缓存充当自回归生成模型的内存库，模型把先前词元的自注意力层算得的键值对存于此处。在 transformer 架构中，自注意力层通过将查询与键相乘以计算注意力分数，并由此生成值向量的加权矩阵。存储了这些信息后，模型无需冗余计算，而仅需直接从缓存中检索先前词元的键和值。下图直观地解释了键值缓存功能，当计算第 `K+1` 个词元的注意力分数时，我们不需要重新计算所有先前词元的键和值，而仅需从缓存中取出它们并串接至当前向量。该做法可以让文本生成更快、更高效。

把键值缓存（KV cache）想象成一个智能记事本，当我们第一次看到每个标记（token）时，就在上面写下关于它的重要信息。对于每个令牌，我们计算并存储两个东西：

- **key**(k)：可以将其视为一种寻址机制。它有助于确定这个标记（token）对于未来的标记有多大的相关性
- **value**(v): 可以把这看作是当这个标记被发现是相关的时候所使用的实际信息

从数学上讲，我们计算为：

- $$k=xW_K$$ （其中 x 是token， $$W_K$$ 是一种可学习的线性变化
- $$v=xW_V$$ （其中 x 是token， $$W_V$$ 是一种可学习的线性变化

在生成一个新令牌时，我们使用它的查询（与计算密钥的方式类似），通过将其与所有存储的Key进行比较，在我们的缓存中找到相关信息。然后使用匹配值来帮助生成Token。【专业的角度就是一次attion计算】



### KV缓存如何使事情更快

使用KVCache，该过程变得更加有效：

1. 当我们看到新的token时，我们只需要计算一次key和value
2. 对于所有未来的token，我们可以从缓存中查找这些预计的值
3. 这意味着每个新token只需要做少量的新工作，而不是重做所有以前的计算

权衡很明显：

- 我们使用更多内存来存储所有键和值。对于一个模型来说主要包括以下几个部分
  - *L* layers
  - *H* attention heads
  - Sequence length n
  - Key/value 纬度 $$d_{k}$$。总内存成本为 $$ L \times H \times n \times d_{k} \times 2$$ 个值（系数2是因为同时考虑了键和值）。 这随序列长度呈线性增长$$ (O(n)) $$，但对于大型模型来说，常数因子可能相当大 。 
- 但是作为回报，我们将计算成本从 $$O(n^3)$$ 降低到 $$O(n^2)$$

要了解为什么是 $$O(n^2)$$ ，让我们看一下每个步骤的成本：

> 在代码层面、在生成attion score 的时候 
>
> - [ 1,dim ] * [ dim,seq ] * [seq*dim]
>   - [ dim,seq ] * [seq*dim] 取出导致的 n
>   - [ 1,dim ]  = 1

- **Step 1**: Process 1 token → cost O(1)
- **Step 2**: Process 1 new token + look at 1 cached token → cost O(2)
- **Step 3**: Process 1 new token + look at 2 cached tokens → cost O(3)
- And so on... 

把这些加起来：

$$ O(1 + 2 + 3 + \cdots + n) = O(n^{2})$$



## 缓存挑战：为什么我们需要更好的解决方案

尽管KV缓存是一种强大的优化，但它带有大量的内存成本。让我们使用现代大型语言模型（如Llama3 70B）来看一个具体的示例：

- *L*=80 layers
- *H*=64 attention heads
- *B*=8 batch size of 8 sequences
- $$d_k$$=128 key/value dimension

8个序列的1000个令牌所需的内存是：

$$L \times H \times B \times n \times d_{k} \times 2 \times 2\text{ bytes}=80 \times 64 \times 8 \times 1000 \times 128 \times 2 \times 2\text{ bytes}=20.97\text{GB}$$ 

这种大量的内存使用造成了一些挑战：

- **Scales linearly** ：与序列长度线性相关
- **Multiplies**：乘以批处理大小以进行并行处理
- **Limits**：限制我们可以处理的最大上下文长度
- **Constrains** 约束在内存限制设备上的部署

这些挑战在研究界引发了一波创新，从而导致了优化KV缓存使用的各种技术。让我们探索这些尖端的解决方案。



## 我们能否改进朴素的键值缓存（Key-Value caches）呢？

以下论文代表了键值缓存优化方面的关键创新。我们将通过三种主要方法来探索这些论文：令牌选择、事后压缩技术和架构重新设计。

### 令牌选择和修剪方法

#### 1) [Heavy-Hitter Oracle (H2O)](https://arxiv.org/abs/2306.14048)

![img](http://pointerhacker.github.io/imgs/posts/deekseek-kv/h2o_alg.png)

H2O 引入了在键值缓存中识别和保留重要标记的概念：

- 高影响力标记：H2O 识别在生成过程中具有最高累积注意力分数的标记，这些标记遵循幂律分布。这些标记对于模型功能至关重要，并在缓存中被优先考虑。

- 动态次模驱逐：该方法将缓存管理构建为一个具有次模目标函数𝐹(𝑆) 的优化问题，该函数量化了标记集𝑆的重要性。

    $$F(S)=\sum_{i\in S}A_{i}$$ 

- 其中，$$𝐴_𝑖$$是标记（token）𝑖的累积注意力得分。缓存（cache）$$𝑆_𝑡$$通过以下方式进行更新。

   $$S_t = \operatorname{argmax}_{S\subseteq S_{t - 1}\cup\{i\},|S|\leq k} F(S)$$ 

> 该公式的意思是在所有满足“$$S$$ 是 $$S_{t - 1}\cup\{i\}$$ 的子集且 $$S$$ 中元素个数不超过 $$k$$ ” 的集合 $$S$$ 中，找到一个能使函数 $$F(S)$$ 取得最大值的集合，并将其赋值给 $$S_t$$

确保每一步最多只有一个标记（token）被逐出。这种贪心算法在计算上是高效的，并且在次模约束下保证接近最优的性能。

结果：在精度损失可忽略不计的情况下，实现了键值缓存大小 5 倍的缩减，并且吞吐量提高了高达 29 倍。



#### 2) [StreamLLM](https://arxiv.org/abs/2309.17453) 

![img](http://pointerhacker.github.io/imgs/posts/deekseek-kv/streamingLLM.png)

**注意力汇现象**：首先，论文发现了一个有趣的自回归LLMs现象，即初始令牌吸引了大量注意力，即使它们在语义上不重要。这种现象被称为“注意力汇”。

- 没有这些注意力汇聚标记，朴素窗口注意力的性能会下降。

**StreamingLLM框架**：基于上述分析，StreamingLLM框架通过保留初始令牌的KV状态和滑动窗口的KV状态来稳定注意力计算。具体步骤如下：

- 在滑动窗口的KV缓存中加入几个初始令牌的KV状态。
- 通过这种方式，StreamingLLM能够在不需要微调的情况下，使LLMs处理无限长度的文本。

**预训练模型**：为了进一步提高流式部署的性能，论文建议在预训练过程中添加一个占位符令牌作为专用的注意力汇。通过在所有训练样本的开头添加一个可学习的令牌，可以单独作为一个注意力汇，从而减少对多个初始令牌的依赖。

#### 3) [Value-Aware Token Pruning (VATP)](https://arxiv.org/abs/2406.12335)

> Attention Score is not All You Need for Token Importance Indicator in KV Cache Reduction: Value Also Matters

![img](http://pointerhacker.github.io/imgs/posts/deekseek-kv/vatp.png)

- 具体来说，现有的令牌剪枝方法仅依赖于注意力分数来评估令牌的重要性，但作者发现值向量的范数分布不均匀，质疑了仅依赖注意力分数的可靠性。

这篇论文提出了一种新的令牌剪枝方法，称为值感知令牌剪枝（VATP），用于解决LLMs中KV缓存减少的问题。具体来说，

**注意力分数分析**：首先，作者分析了注意力机制输出，其定义为：

$$\text{Attention}(Q, K, V)_t = \sum_{i\leq t} a_{i}^{t}v_{i}$$ 

其中，$$a_i^t$$ 是查询令牌t对令牌i的注意力分数，$$v_i$$ 是令牌i的值状态。每个令牌对注意力输出的影响由注意力分数和值向量共同决定。

**值向量范数分析**：作者观察到值向量的$$\ell_{1}$$ 范数在所有层和头上分布不均，且注意力汇聚令牌的$$\ell_{1}$$ 范数较小。这表明仅依赖注意力分数可能会忽略这些重要令牌。

**VATP方法**：为了综合考虑注意力分数和值向量范数，作者提出了一个新的令牌重要性评估指标：

 $$I_{k}^{t} = S_{k}^{t} \cdot \left\lVert v_{k} \right\rVert_{1}$$ 

其中，$$S_{k}^{t}$$是令牌k在解码步t的累积注意力分数，$$\left\lVert v_{k} \right\rVert_{1}$$ 是令牌k的值向量的$$\ell_{1}$$ 范数。通过计算每个令牌的注意力分数和值向量范数的乘积，可以更全面地评估令牌的重要性。

**注意力汇聚令牌处理**：由于注意力汇聚令牌的$$\ell_{1}$$ 范数较小，根据VATP指标，这些令牌的重要性评分会被显著降低。为了避免移除这些令牌导致性能下降，作者选择保留前F个令牌，其中F=20（对于LLaMA2-7B-chat）和F=40（对于Vicuna-v1.5-7B-16k）。



##### 结果与分析

1. **主要结果**：在LLaMA2-7B-chat模型上，VATP方法在12个任务中优于*H*2*O*，在13个任务中优于Scissorhands。在Vicuna-v1.5-7B-16k模型上，VATP方法在12个任务中优于*H*2*O*，在14个任务中优于Scissorhands。尽管在某些任务中VATP未能超越基线，但其性能仍然非常接近基线。



### 事后压缩技术

这些方法在保留标准 transformer架构的同时压缩或优化键值缓存。

##### 4) [Adaptive KV Compression (FastGen)](https://arxiv.org/pdf/2310.01801)

> 如何在大语言模型（LLMS）中进行自适应的键值缓存压缩，以减少内存占用并提高生成推理的效率。

![img](http://pointerhacker.github.io/imgs/posts/deekseek-kv/fastgen_1.png)

这篇论文提出了FastGen方法，用于解决大语言模型中KV缓存压缩的问题。具体来说，基于运行时观察到的注意力模式的自适应压缩。在提示编码期间，FastGen 会识别注意力模式，并选择能在保留注意力恢复的同时最小化内存成本的压缩策略 C*。

1. **模型分析**：首先，使用高效的性能分析算法识别注意力模块的结构模式。通过对提示编码的结果进行分析，选择最适合每个注意力头的压缩策略。
2. **自适应KV缓存构建**：在提示编码阶段，进行模型分析以识别不同注意力头的结构特征。然后，在令牌生成阶段，根据每个令牌的压缩策略管理KV缓存。
3. **压缩策略**：提出了五种基本的KV缓存压缩策略：特殊令牌、标点符号、局部性、频率（重击者）和混合策略。混合策略通过贪心方法构建，主要包括特殊令牌、特殊令牌加标点符号、特殊令牌加频率、特殊令牌加频率加局部性和全量缓存。

压缩策略包括：

- 特殊令牌（ $$C_{special}$$）：仅保留特殊令牌。
- 局部性（$$C_{local}$$） : 是指驱逐超出相对距离𝑟𝑙的标记。
- 频率（ $$C_{frequent}$$）：保持具有较高累积注意分数的令牌（ $$r_f$$ ）。
- 混合政策结合了策略，$$从 C_special$$开始，并适用于每个头部：

 $$\mathcal{C}=\{C_{\text{special}},C_{\text{special}} + C_{\text{punct}},\ldots,C_{\text{full}}\}.$$ 

Token生成

- 在解码过程中，预选的压缩策略有效地管理KV缓存：

  $$K_{C_i},V_{C_i}=f(K,V,C_i).$$ 



- 公式1：用于选择最优压缩策略的优化问题。具体来说，公式1的目标是最小化压缩后的KV缓存内存成本，同时保证恢复比率T满足条件：

   $$C^{*}=\underset{C\in C}{\arg \min}\text{CacheMemoryCost}(C)\quad \text{s.t.}\quad |A - \text{softmax}(QKT)|\leq 1 - T$$

  其中，$C$是所有可行的压缩策略集合，$\text{CacheMemoryCost}(C)$是压缩策略$C$的目标内存预算，$T$是预定义的超参数，表示策略恢复注意力图$A$的程度。 



##### 5) [Dynamic Memory Compression (DMC)](https://arxiv.org/pdf/2403.09636)

![img](http://pointerhacker.github.io/imgs/posts/deekseek-kv/dmc.png)

这篇论文提出了动态内存压缩（DMC）用于解決LLMs在推理时的内存效率问题。具体来说，

1. **DMC的基本原理**：DMC在每个时间步决定是将当前的键值表示追加到缓存中，还是将其与缓存中的最后一个元素进行加权平均。通过这种方式，内存的增长是亚线性的，介于传统的Transformers和状态空间语言模型之间。

2. 单头键值缓存更新：在单头情况下，DMC的键值缓存更新过程
    如下：

   - 预测决策变量 $$a_t$$ 和重要性变量 $$w_t$$。

   - 根据 $$a_t$$的值，决定将当前的键值表示追加到缓存中还是进行加权平均。

   - 公式如下：

     ![image-20250217115453044](http://pointerhacker.github.io/imgs/posts/deekseek-kv/image-20250217115453044.png)
    - 通过这种方式：DMC的缓存长度$l$为$l = \sum_{i = 1}^{t}(1 - \alpha_{i})\leq t$，而传统的Transformers中$l = t$。 

3. **训练过程**：为了使LLMs具备DMC行为，通过在少量原始预训练数据上进行继续预训练，逐步增加压缩率。具体步骤包括：

   - ﻿使用梯度下降和连续松弛决策变量。

   - ﻿定义部分累积状态以处理 a €［O，工的情况。

   - ﻿﻿在训练过程中使用加性掩码来模拟推理时的行为。

DMC引入了自适应令牌合并：

- 决策机制：在时间 t 时，预测合并决策 αt 和权重 ωt。

   $$\alpha_{t}=\lfloor\operatorname{sigmoid}(k_{t}[0])\rceil,\quad \omega_{t}=\operatorname{sigmoid}(q_{t}[0]).$$ 

##### 训练： 

- 对$\alpha_t$使用Gumbel - Sigmoid松弛法，以便通过梯度下降进行端到端训练：

   $$\alpha_{t}\sim \text{Gumbel - Sigmoid}(k_{t}[0],\tau)$$ 

  其中$\tau$是温度参数。 

  

- 优化一个组合目标： 

  $$\mathcal{L}=\mathcal{L}_{\text{LM}}+\lambda\max\left(0,\frac{n}{\text{CR}}-\sum_{t}\alpha_{t}\right)$$ 
  
  其中$\mathcal{L}_{LM}$是语言建模损失，第二项鼓励模型达到目标压缩比（CR）。 ### 公式



##### 6) [L2 Norm-Based Compression](https://arxiv.org/pdf/2406.11430)

该问题的研究难点包括：现有方法通常涉及复杂的算法或显著的计算开销，限制了其实用性；此外，许多后处理压缩算法依赖于注意力分数，这些分数**无法与FlashAttention兼容**

这篇论文提出了一种基于键嵌入$$L_2$$范数的简单而有效的KV缓存压缩策略。具体来说，

1. **注意力分布分析：**首先，作者分析了仅解码器Transformer模型中的注意力分布，发现大多数层中注意力分配模式保持一致。令人惊讶的是，键嵌入的$$L_2$$范数与注意力分数之间存在明显的相关性，**低$$L_{2}$$范数的键嵌入通常在解码过程中导致高注意力分数**

2. **压缩策略**：基于上述观察，作者提出了一种压缩策略，即仅保留具有最低$$L_2$$范数的键及其对应的值。这种方法不需要额外的训练或显著的修改，可以直接应用于任何基于Transformer的解码器仅LLM。

    基于范数的选择：对于一组缓存键$K = \{k_1, k_2, \ldots, k_n\}$，计算并对键的范数进行排序 。

   $$\left\lVert k_{i}\right\rVert_{2}=\sqrt{\sum_{j = 1}^{d}k_{i,j}^{2}}$$ 

3. **注意力损失估计**：为了评估压缩的影响，作者定义了由于压缩而导致的注意力损失。具体来说，给定一个提示序列，LLM首先将其编码为KV缓存，然后自回归地生成下一个标记。在压缩过程中，某些键值对被丢弃，作者定义了由于压缩而导致的注意力损失为**丟弃的键值对的注意力分数之和**。

4. **相关系数ALR**：为了衡量理想注意力分数压缩与基于*L*2范数压缩之间的差异，作者引入了一个相关系数ALR（Attention Loss Ratio），其定义为：

​      $$\mathcal{Y}_{l,h}=\sum_{m = 1}^{n}\mathcal{Y}_{l,h}^{m}$$ 

文字： 其中，$\mathcal{Y}_{l,h}^{m}$表示在第l层、第h个头的压缩方法下，丢弃m对键值对后的注意力损失。



### 模型结构重新设计

这些方法会更改transformer结构，以更有效地处理KV缓存，通常将压缩直接纳入模型结构。

### 7) [Multi-Query Attention (MQA)](https://arxiv.org/pdf/2305.13245)

![img](http://pointerhacker.github.io/imgs/posts/deekseek-kv/mqa.png)

**关键思想**：MQA 通过在所有查询头中共享单个键值头来减小 KV 缓存大小，取代了传统的多头注意力（MHA）。

 $$K = XW_{K}, \quad V = XW_{V}$$

其中 K 和 V 是共享的键和值现形变化。

**好处**：将 KV 缓存大小减少了（注意力头的数量），显着降低了内存带宽开销。

**权衡：**虽然 MQA 速度更快，但它经常遭受质量下降，尤其是在需要不同注意力模式的任务中。

### 8) [Group-Query Attention (GQA)](https://arxiv.org/abs/2305.13245)

![img](http://pointerhacker.github.io/imgs/posts/deekseek-kv/gqa.png)

**关键思想：**GQA 在完全多头注意力和 MQA 之间进行插值，以在推理速度和模型质量之间提供可扩展的权衡。它将查询头分为 G 组，其中每组共享一个单一的键值头。

$$K_{\text{group}} = \frac{1}{|G|} \sum_{h\in G} K_{h}, \quad V_{\text{group}} = \frac{1}{|G|} \sum_{h\in G} V_{h}$$ 

- GQA-1：等效于MQA（G=1）。
- GQA-𝐻: 它等同于 MHA（即 “𝐺=𝐻”）。

Uptraining: GQA 可以通过微调被引入到现有的预训练模型中。

- 首先，通过将键和值头进行平均池化分组，将 MHA（多头注意力机制）检查点转换为 GQA。
- 然后，对模型进行简短的微调（“uptrain”，即再次训练）以适应新的注意力模式。
- 这个适应过程仅需要原始预训练计算量的 5%，因此非常高效。
- 最终得到的模型在保持质量的同时获得了 GQA 的内存优势。

```python
def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
    """
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    if n_rep == 1:
        return hidden_states
    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
```





### 9) [Multi-head Latent Attention (MLA)](https://arxiv.org/abs/2405.04434)

![img](http://pointerhacker.github.io/imgs/posts/deekseek-kv/mla.png)

DeepSeek 的多头潜在注意力（MLA）采用了一种新颖的方法来减少键值缓存开销

虽然 MQA和 GQA 通过 head-sharing 来实现这一目标，但 MLA却采用了一种低秩潜在压缩技术，这种技术保持了多个注意力头的优势。

- MLA通过在重建之前将键(keys)和值(values)压缩成低维潜在向量来减少KV缓存的大小。

  它将键值嵌入下投影到压缩的潜在空间，相关公式如下：

  $$c_{KV,t} = W_{DKV}h_t, \quad k_C = W_{UK}c_{KV,t}, \quad v_C = W_{UV}c_{KV,t}$$

  其中：

  - $W_{DKV}$ 是下投影矩阵
  - $W_{UK}$, $W_{UV}$ 是键和值的上投影矩阵

- 与MQA(Multi-Query Attention)完全共享注意力头的方式不同，它通过压缩表示保持了每个头部的灵活性。

- 它引入了旋转位置嵌入(Rotary Positional Embeddings, RoPE)来解耦位置感知的键：

  $$k_R = RoPE(W_{KR}h_t), \quad k_t = [k_C; k_R]$$

  这种方法通过只缓存压缩的潜在向量 $c_{KV}$ 和位置键 $k_R$ 进一步减少了KV缓存存储空间。

```python
def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        output_attentions: bool = False,
        use_cache: bool = False,
        **kwargs,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        if "padding_mask" in kwargs:
            warnings.warn(
                "Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`"
            )
        bsz, q_len, _ = hidden_states.size()

        if self.q_lora_rank is None:
            q = self.q_proj(hidden_states)
        else:
            # bsz,q_len,d -> bsz,q_len,q_lora_rank
            q = self.q_b_proj(self.q_a_layernorm(self.q_a_proj(hidden_states)))
        
        #  bsz,q_len,q_lora_rank ->  bsz,q_len,num_heads,q_head_dim
        # -> bsz,num_heads,q_len,q_head_dim
        q = q.view(bsz, q_len, self.num_heads, self.q_head_dim).transpose(1, 2)
        # bsz,num_heads,q_len,q_head_dim -> 
        #   q_nope: bsz,num_heads,q_len,qk_nope_head_dim
        #   q_pe: bsz,num_heads,q_len,qk_rope_head_dim
        q_nope, q_pe = torch.split(
            q, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1
        )

        # bsz,q_len,d -> bsz,q_len, kv_lora_rank + qk_rope_head_dim,
        compressed_kv = self.kv_a_proj_with_mqa(hidden_states)
        # bsz,q_len, kv_lora_rank + qk_rope_head_dim,
        # compressed_kv:  bsz,q_len, kv_lora_rank
        # k_pe:  bsz,q_len, qk_rope_head_dim
        
        compressed_kv, k_pe = torch.split(
            compressed_kv, [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1
        )
        
        #  bsz,q_len, qk_rope_head_dim -> bsz,q_len, 1, qk_rope_head_dim -> bsz,1, q_len, qk_rope_head_dim
        k_pe = k_pe.view(bsz, q_len, 1, self.qk_rope_head_dim).transpose(1, 2)
        
        #  bsz,q_len, kv_lora_rank ->bsz,q_len, num_heads * (q_head_dim - qk_rope_head_dim + v_head_dim)
        # -> bsz, q_len, self.num_heads, qk_nope_head_dim + v_head_dim
        # ->  bsz, num_heads, q_len, qk_nope_head_dim + v_head_dim
        kv = (
            self.kv_b_proj(self.kv_a_layernorm(compressed_kv))
            .view(bsz, q_len, self.num_heads, self.qk_nope_head_dim + self.v_head_dim)
            .transpose(1, 2)
        )

        # bsz, num_heads, q_len, qk_nope_head_dim + v_head_dim
        # k_nope: bsz，num_heads, q_len, qk_nope_head_dim
        # value_states: bsz,  num_heads, q_len, v_head_dim
        k_nope, value_states = torch.split(
            kv, [self.qk_nope_head_dim, self.v_head_dim], dim=-1
        )
        
        # q_len
        kv_seq_len = value_states.shape[-2]
        if past_key_value is not None:
            if self.layer_idx is None:
                raise ValueError(
                    f"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} "
                    "for auto-regressive decoding with k/v caching, please make sure to initialize the attention class "
                    "with a layer index."
                )
            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)
        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)

        #   q_pe: bsz,num_heads,q_len,qk_rope_head_dim
        #   k_pe:  bsz,1, q_len, qk_rope_head_dim
        q_pe, k_pe = apply_rotary_pos_emb(q_pe, k_pe, cos, sin, position_ids)

        # bsz,num_heads,q_len,q_head_dim
        query_states = k_pe.new_empty(bsz, self.num_heads, q_len, self.q_head_dim)
        query_states[:, :, :, : self.qk_nope_head_dim] = q_nope
        query_states[:, :, :, self.qk_nope_head_dim :] = q_pe

        # # bsz,num_heads,q_len,q_head_dim
        key_states = k_pe.new_empty(bsz, self.num_heads, q_len, self.q_head_dim)
        key_states[:, :, :, : self.qk_nope_head_dim] = k_nope
        key_states[:, :, :, self.qk_nope_head_dim :] = k_pe
        if past_key_value is not None:
            cache_kwargs = {"sin": sin, "cos": cos}  # Specific to RoPE models
            key_states, value_states = past_key_value.update(
                key_states, value_states, self.layer_idx, cache_kwargs
            )

        attn_weights = (
            torch.matmul(query_states, key_states.transpose(2, 3)) * self.softmax_scale
        )
        
        attn_output = torch.matmul(attn_weights, value_states)
        
         return attn_output, attn_weights, past_key_value
```





### 10) [SnapKV](https://arxiv.org/pdf/2404.14469)

![img](http://pointerhacker.github.io/imgs/posts/deekseek-kv/snapKV.png)

用于解决LLMs在处理长输入序列时KV缓存扩展带来的效率和内存问题。具体来说，

- **观察模式一致性**：研究发现，在生成过程中，每个注意力头始终关注特定的提示注意力特征。这些特征可以从提示末尾的“观察”窗口中获得。

- **投票机制**：通过投票机制选择重要的KV位置。具体公式如下：

   $$C = \sum_{i = 0}^{L_{\text{obs}}} W_{\text{obs}}[:, i, :], \quad I = \text{Top}_{k}(C, k)$$ 



### 11) [You Only Cache Once (YOCO)](https://arxiv.org/pdf/2405.05254)

Yoco修改了用于缓存的Transformers体系结构：

- 全局缓存：使用单个共享KV缓存的解码器编码器设计。
- 复杂性降低：将存储器空间从 O(N×L) 降低到 O(N+L)，其中 N 是序列长度， L 是层的数量。
- 高效注意力机制下的自解码器采用滑动窗口注意力或者门控保留机制，能够实现恒定的内存使用（复杂度为 O (C)，其中 C 是一个小的窗口大小）。
