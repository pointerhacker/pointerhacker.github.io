---
layout: post
title: ä¸€æ–‡ææ‡‚KVCache
categories: [LLM]
tags: LLM
---

## ç®€ä»‹
**æ¦‚è¿°**ï¼š

1. ç®€ä»‹ï¼šæˆ‘ä»¬å°†æ¢ç´¢é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜å¦‚ä½•é€šè¿‡åœ¨å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æ—¶é—´ä¹‹é—´è¿›è¡Œå·§å¦™çš„æƒè¡¡æ¥ä½¿å…¶ç”Ÿæˆæ–‡æœ¬ä¹‹ç±»çš„è¯­è¨€æ¨¡å‹æ›´å¿«åœ°ç”Ÿæˆæ–‡æœ¬ã€‚
2. MLAå’Œå…¶ä»–æŠ€å·§ï¼šç„¶åï¼Œæˆ‘ä»¬å°†ç ”ç©¶æœ€è¿‘çš„11ç¯‡ç ”ç©¶è®ºæ–‡ï¼Œå…¶ä¸­åŒ…æ‹¬DeepSeekçš„å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›ï¼ˆMLAï¼‰ï¼Œå®ƒä»¬åŸºäºè¿™ä¸ªåŸºæœ¬æ€æƒ³ï¼Œä»¥ä½¿LLMæ¨è®ºæå‡ºæ›´é«˜çš„æ—¶é—´æ•ˆç‡ã€‚

## ç†è§£é—®é¢˜ï¼šä¸ºä»€ä¹ˆæ–‡æœ¬ç”Ÿæˆå¾ˆæ…¢

è®©æˆ‘ä»¬ä»ä¸€ä¸ªç®€å•çš„ç±»æ¯”å¼€å§‹ã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œæ‚¨æ­£åœ¨å†™ä¸€ä¸ªæ•…äº‹ï¼Œå¯¹äºæ‚¨å†™çš„æ¯ä¸ªæ–°è¯ï¼Œæ‚¨éœ€è¦é‡æ–°é˜…è¯»åˆ°ç›®å‰ä¸ºæ­¢çš„æ•´ä¸ªæ•…äº‹ä»¥ä¿æŒä¸€è‡´æ€§ã€‚æ‚¨çš„æ•…äº‹çš„æ—¶é—´è¶Šé•¿ï¼Œæ‚¨èŠ±è´¹çš„æ—¶é—´å°±è¶Šå¤šã€‚è¿™æ­£æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ä¸­æ‰€é¢ä¸´çš„ã€‚



### åŸºæœ¬æ„ä»¶ï¼šSelf-Attention

ç°ä»£è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒæ˜¯ä¸€ç§ç§°ä¸º**Self-Attention**çš„æœºåˆ¶ã€‚å¯¹äºé•¿åº¦ä¸ºnçš„tokençš„åºåˆ—ï¼ˆå°†tokenè§†ä¸ºä¸å­—å¤§è‡´ç›¸å¯¹åº”çš„)ï¼Œæ¯ä¸ªtokenéƒ½éœ€è¦â€œæŸ¥çœ‹â€æˆ–â€œæ³¨æ„â€æ‰€æœ‰å…¶ä»–ä»¤ç‰Œæ‰èƒ½ç†è§£ä¸Šä¸‹æ–‡ã€‚

è¿™ä¸ªå…¨é¢çš„æŸ¥æ‰¾è¿‡ç¨‹çš„è®¡ç®—æˆæœ¬éšåºåˆ—é•¿åº¦å¢é•¿ï¼š

- å¯¹äº n ä¸ªtokenï¼Œæ¯ä¸ªtokenéƒ½éœ€è¦æŸ¥çœ‹æ‰€æœ‰nä¸ªtoken
- è¿™æ„å‘³ç€æˆæœ¬ä¸ $$ nÃ—n=n^2$$ æˆæ­£æ¯”
- åœ¨æ•°å­¦ç¬¦å·ä¸­ï¼Œæˆ‘ä»¬å°†å…¶å†™ä¸º $$O(n^2)$$ å¤æ‚æ€§



### çœŸæ­£çš„é—®é¢˜ï¼šå•ä½æ—¶é—´åªèƒ½ç”Ÿæˆä¸€ä¸ªToken

> åœ¨ä»£ç å±‚é¢ã€åœ¨ç”Ÿæˆattion score çš„æ—¶å€™ 
>
> - [ 1,dim ] * [ dim,seq ] * [seq*dim]
>   - [ dim,seq ] * [seq*dim] è®¡ç®—å¯¼è‡´çš„ $$n^2$$
>   - [ 1,dim ]  = 1

å½“è¯­è¨€æ¨¡å‹ç”Ÿæˆæ–‡æœ¬æ—¶ï¼Œå®ƒä¸€æ¬¡åªèƒ½ç”Ÿæˆä¸€ä¸ªTokenï¼Œè€Œè¿™é‡è®¡ç®—æ˜¯æ˜‚è´µçš„ï¼š

1. **First token**: Look at 1 token $$(cost: O(1^2)$$
2. **Second token**: Look at 2 tokens  $$(cost: O(2^2)$$
3. **Third token**: Look at 3 tokens  $$(cost: O(3^2)$$
4. And so on until the n*n*-th token: Look at n*n* tokens  $$(cost: O(n^2)$$

å¦‚æœæˆ‘ä»¬å°†æ‰€æœ‰è¿™äº›æˆæœ¬æ·»åŠ åˆ°ç”Ÿæˆé•¿åº¦ n çš„é¡ºåºä¸­ï¼Œæˆ‘ä»¬å°†å¾—åˆ°ï¼š

$$O(1^{2}+2^{2}+3^{2}+\cdots +n^{2})\approx O(n^{3})$$ 

$$O(n^{3})$$  æˆæœ¬æ„å‘³ç€éšç€æ‚¨çš„æ–‡æœ¬è¶Šæ¥è¶Šé•¿ï¼Œç”Ÿæˆæ–‡æœ¬éœ€è¦çš„æ—¶é—´å¢é•¿å¾ˆå¿«ã€‚

ä¾‹å¦‚ï¼Œç”Ÿæˆåºåˆ—çš„é•¿åº¦å¦‚æœå¢åŠ 2æ¯ï¼Œé‚£ä¹ˆéœ€è¦çš„æ—¶é—´å¤§çº¦éœ€è¦å¤§çº¦å…«å€ï¼æ˜¾ç„¶ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ç§æ›´å¥½çš„æ–¹æ³•ã€‚



## è§£å†³æ–¹æ¡ˆï¼šKey-Value (KV) Cache

![KV ç¼“å­˜ç¤ºæ„å›¾](http://pointerhacker.github.io/imgs/posts/deekseek-kv/kv-cache-optimization.png)

KVCacheèƒŒåçš„å…³é”®æ€æƒ³æ˜¯æˆ‘ä»¬æ­£åœ¨åšå¾ˆå¤šå¤šä½™çš„å·¥ä½œã€‚ç”Ÿæˆæ¯ä¸ªæ–°ä»¤ç‰Œæ—¶ï¼Œæˆ‘ä»¬æ­£åœ¨ä¸ºæˆ‘ä»¬ä¹‹å‰å·²ç»å¤„ç†è¿‡çš„æ‰€æœ‰ä»¥å‰çš„ä»¤ç‰Œé‡æ–°è®¡ç®—å†…å®¹ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•è§£å†³æ­¤é—®é¢˜ã€‚

### ä»€ä¹ˆæ˜¯KVCache

> é”®å€¼ç¼“å­˜å……å½“è‡ªå›å½’ç”Ÿæˆæ¨¡å‹çš„å†…å­˜åº“ï¼Œæ¨¡å‹æŠŠå…ˆå‰è¯å…ƒçš„è‡ªæ³¨æ„åŠ›å±‚ç®—å¾—çš„é”®å€¼å¯¹å­˜äºæ­¤å¤„ã€‚åœ¨ transformer æ¶æ„ä¸­ï¼Œè‡ªæ³¨æ„åŠ›å±‚é€šè¿‡å°†æŸ¥è¯¢ä¸é”®ç›¸ä¹˜ä»¥è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼Œå¹¶ç”±æ­¤ç”Ÿæˆå€¼å‘é‡çš„åŠ æƒçŸ©é˜µã€‚å­˜å‚¨äº†è¿™äº›ä¿¡æ¯åï¼Œæ¨¡å‹æ— éœ€å†—ä½™è®¡ç®—ï¼Œè€Œä»…éœ€ç›´æ¥ä»ç¼“å­˜ä¸­æ£€ç´¢å…ˆå‰è¯å…ƒçš„é”®å’Œå€¼ã€‚ä¸‹å›¾ç›´è§‚åœ°è§£é‡Šäº†é”®å€¼ç¼“å­˜åŠŸèƒ½ï¼Œå½“è®¡ç®—ç¬¬ `K+1` ä¸ªè¯å…ƒçš„æ³¨æ„åŠ›åˆ†æ•°æ—¶ï¼Œæˆ‘ä»¬ä¸éœ€è¦é‡æ–°è®¡ç®—æ‰€æœ‰å…ˆå‰è¯å…ƒçš„é”®å’Œå€¼ï¼Œè€Œä»…éœ€ä»ç¼“å­˜ä¸­å–å‡ºå®ƒä»¬å¹¶ä¸²æ¥è‡³å½“å‰å‘é‡ã€‚è¯¥åšæ³•å¯ä»¥è®©æ–‡æœ¬ç”Ÿæˆæ›´å¿«ã€æ›´é«˜æ•ˆã€‚

æŠŠé”®å€¼ç¼“å­˜ï¼ˆKV cacheï¼‰æƒ³è±¡æˆä¸€ä¸ªæ™ºèƒ½è®°äº‹æœ¬ï¼Œå½“æˆ‘ä»¬ç¬¬ä¸€æ¬¡çœ‹åˆ°æ¯ä¸ªæ ‡è®°ï¼ˆtokenï¼‰æ—¶ï¼Œå°±åœ¨ä¸Šé¢å†™ä¸‹å…³äºå®ƒçš„é‡è¦ä¿¡æ¯ã€‚å¯¹äºæ¯ä¸ªä»¤ç‰Œï¼Œæˆ‘ä»¬è®¡ç®—å¹¶å­˜å‚¨ä¸¤ä¸ªä¸œè¥¿ï¼š

- **key**(k)ï¼šå¯ä»¥å°†å…¶è§†ä¸ºä¸€ç§å¯»å€æœºåˆ¶ã€‚å®ƒæœ‰åŠ©äºç¡®å®šè¿™ä¸ªæ ‡è®°ï¼ˆtokenï¼‰å¯¹äºæœªæ¥çš„æ ‡è®°æœ‰å¤šå¤§çš„ç›¸å…³æ€§
- **value**(v): å¯ä»¥æŠŠè¿™çœ‹ä½œæ˜¯å½“è¿™ä¸ªæ ‡è®°è¢«å‘ç°æ˜¯ç›¸å…³çš„æ—¶å€™æ‰€ä½¿ç”¨çš„å®é™…ä¿¡æ¯

ä»æ•°å­¦ä¸Šè®²ï¼Œæˆ‘ä»¬è®¡ç®—ä¸ºï¼š

- $$k=xW_K$$ ï¼ˆå…¶ä¸­ x æ˜¯tokenï¼Œ $$W_K$$ æ˜¯ä¸€ç§å¯å­¦ä¹ çš„çº¿æ€§å˜åŒ–
- $$v=xW_V$$ ï¼ˆå…¶ä¸­ x æ˜¯tokenï¼Œ $$W_V$$ æ˜¯ä¸€ç§å¯å­¦ä¹ çš„çº¿æ€§å˜åŒ–

åœ¨ç”Ÿæˆä¸€ä¸ªæ–°ä»¤ç‰Œæ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨å®ƒçš„æŸ¥è¯¢ï¼ˆä¸è®¡ç®—å¯†é’¥çš„æ–¹å¼ç±»ä¼¼ï¼‰ï¼Œé€šè¿‡å°†å…¶ä¸æ‰€æœ‰å­˜å‚¨çš„Keyè¿›è¡Œæ¯”è¾ƒï¼Œåœ¨æˆ‘ä»¬çš„ç¼“å­˜ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚ç„¶åä½¿ç”¨åŒ¹é…å€¼æ¥å¸®åŠ©ç”ŸæˆTokenã€‚ã€ä¸“ä¸šçš„è§’åº¦å°±æ˜¯ä¸€æ¬¡attionè®¡ç®—ã€‘



### KVç¼“å­˜å¦‚ä½•ä½¿äº‹æƒ…æ›´å¿«

ä½¿ç”¨KVCacheï¼Œè¯¥è¿‡ç¨‹å˜å¾—æ›´åŠ æœ‰æ•ˆï¼š

1. å½“æˆ‘ä»¬çœ‹åˆ°æ–°çš„tokenæ—¶ï¼Œæˆ‘ä»¬åªéœ€è¦è®¡ç®—ä¸€æ¬¡keyå’Œvalue
2. å¯¹äºæ‰€æœ‰æœªæ¥çš„tokenï¼Œæˆ‘ä»¬å¯ä»¥ä»ç¼“å­˜ä¸­æŸ¥æ‰¾è¿™äº›é¢„è®¡çš„å€¼
3. è¿™æ„å‘³ç€æ¯ä¸ªæ–°tokenåªéœ€è¦åšå°‘é‡çš„æ–°å·¥ä½œï¼Œè€Œä¸æ˜¯é‡åšæ‰€æœ‰ä»¥å‰çš„è®¡ç®—

æƒè¡¡å¾ˆæ˜æ˜¾ï¼š

- æˆ‘ä»¬ä½¿ç”¨æ›´å¤šå†…å­˜æ¥å­˜å‚¨æ‰€æœ‰é”®å’Œå€¼ã€‚å¯¹äºä¸€ä¸ªæ¨¡å‹æ¥è¯´ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†
  - *L* layers
  - *H* attention heads
  - Sequence length n
  - Key/value çº¬åº¦ $$d_{k}$$ã€‚æ€»å†…å­˜æˆæœ¬ä¸º $$ L \times H \times n \times d_{k} \times 2$$ ä¸ªå€¼ï¼ˆç³»æ•°2æ˜¯å› ä¸ºåŒæ—¶è€ƒè™‘äº†é”®å’Œå€¼ï¼‰ã€‚ è¿™éšåºåˆ—é•¿åº¦å‘ˆçº¿æ€§å¢é•¿$$ (O(n)) $$ï¼Œä½†å¯¹äºå¤§å‹æ¨¡å‹æ¥è¯´ï¼Œå¸¸æ•°å› å­å¯èƒ½ç›¸å½“å¤§ ã€‚ 
- ä½†æ˜¯ä½œä¸ºå›æŠ¥ï¼Œæˆ‘ä»¬å°†è®¡ç®—æˆæœ¬ä» $$O(n^3)$$ é™ä½åˆ° $$O(n^2)$$

è¦äº†è§£ä¸ºä»€ä¹ˆæ˜¯ $$O(n^2)$$ ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹æ¯ä¸ªæ­¥éª¤çš„æˆæœ¬ï¼š

> åœ¨ä»£ç å±‚é¢ã€åœ¨ç”Ÿæˆattion score çš„æ—¶å€™ 
>
> - [ 1,dim ] * [ dim,seq ] * [seq*dim]
>   - [ dim,seq ] * [seq*dim] å–å‡ºå¯¼è‡´çš„ n
>   - [ 1,dim ]  = 1

- **Step 1**: Process 1 token â†’ cost O(1)
- **Step 2**: Process 1 new token + look at 1 cached token â†’ cost O(2)
- **Step 3**: Process 1 new token + look at 2 cached tokens â†’ cost O(3)
- And so on... 

æŠŠè¿™äº›åŠ èµ·æ¥ï¼š

$$ O(1 + 2 + 3 + \cdots + n) = O(n^{2})$$



## ç¼“å­˜æŒ‘æˆ˜ï¼šä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦æ›´å¥½çš„è§£å†³æ–¹æ¡ˆ

å°½ç®¡KVç¼“å­˜æ˜¯ä¸€ç§å¼ºå¤§çš„ä¼˜åŒ–ï¼Œä½†å®ƒå¸¦æœ‰å¤§é‡çš„å†…å­˜æˆæœ¬ã€‚è®©æˆ‘ä»¬ä½¿ç”¨ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚Llama3 70Bï¼‰æ¥çœ‹ä¸€ä¸ªå…·ä½“çš„ç¤ºä¾‹ï¼š

- *L*=80 layers
- *H*=64 attention heads
- *B*=8 batch size of 8 sequences
- $$d_k$$=128 key/value dimension

8ä¸ªåºåˆ—çš„1000ä¸ªä»¤ç‰Œæ‰€éœ€çš„å†…å­˜æ˜¯ï¼š

$$L \times H \times B \times n \times d_{k} \times 2 \times 2\text{ bytes}=80 \times 64 \times 8 \times 1000 \times 128 \times 2 \times 2\text{ bytes}=20.97\text{GB}$$ 

è¿™ç§å¤§é‡çš„å†…å­˜ä½¿ç”¨é€ æˆäº†ä¸€äº›æŒ‘æˆ˜ï¼š

- **Scales linearly** ï¼šä¸åºåˆ—é•¿åº¦çº¿æ€§ç›¸å…³
- **Multiplies**ï¼šä¹˜ä»¥æ‰¹å¤„ç†å¤§å°ä»¥è¿›è¡Œå¹¶è¡Œå¤„ç†
- **Limits**ï¼šé™åˆ¶æˆ‘ä»¬å¯ä»¥å¤„ç†çš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
- **Constrains** çº¦æŸåœ¨å†…å­˜é™åˆ¶è®¾å¤‡ä¸Šçš„éƒ¨ç½²

è¿™äº›æŒ‘æˆ˜åœ¨ç ”ç©¶ç•Œå¼•å‘äº†ä¸€æ³¢åˆ›æ–°ï¼Œä»è€Œå¯¼è‡´äº†ä¼˜åŒ–KVç¼“å­˜ä½¿ç”¨çš„å„ç§æŠ€æœ¯ã€‚è®©æˆ‘ä»¬æ¢ç´¢è¿™äº›å°–ç«¯çš„è§£å†³æ–¹æ¡ˆã€‚



## æˆ‘ä»¬èƒ½å¦æ”¹è¿›æœ´ç´ çš„é”®å€¼ç¼“å­˜ï¼ˆKey-Value cachesï¼‰å‘¢ï¼Ÿ

ä»¥ä¸‹è®ºæ–‡ä»£è¡¨äº†é”®å€¼ç¼“å­˜ä¼˜åŒ–æ–¹é¢çš„å…³é”®åˆ›æ–°ã€‚æˆ‘ä»¬å°†é€šè¿‡ä¸‰ç§ä¸»è¦æ–¹æ³•æ¥æ¢ç´¢è¿™äº›è®ºæ–‡ï¼šä»¤ç‰Œé€‰æ‹©ã€äº‹åå‹ç¼©æŠ€æœ¯å’Œæ¶æ„é‡æ–°è®¾è®¡ã€‚

### ä»¤ç‰Œé€‰æ‹©å’Œä¿®å‰ªæ–¹æ³•

#### 1) [Heavy-Hitter Oracle (H2O)](https://arxiv.org/abs/2306.14048)

![img](http://pointerhacker.github.io/imgs/posts/deekseek-kv/h2o_alg.png)

H2O å¼•å…¥äº†åœ¨é”®å€¼ç¼“å­˜ä¸­è¯†åˆ«å’Œä¿ç•™é‡è¦æ ‡è®°çš„æ¦‚å¿µï¼š

- é«˜å½±å“åŠ›æ ‡è®°ï¼šH2O è¯†åˆ«åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å…·æœ‰æœ€é«˜ç´¯ç§¯æ³¨æ„åŠ›åˆ†æ•°çš„æ ‡è®°ï¼Œè¿™äº›æ ‡è®°éµå¾ªå¹‚å¾‹åˆ†å¸ƒã€‚è¿™äº›æ ‡è®°å¯¹äºæ¨¡å‹åŠŸèƒ½è‡³å…³é‡è¦ï¼Œå¹¶åœ¨ç¼“å­˜ä¸­è¢«ä¼˜å…ˆè€ƒè™‘ã€‚

- åŠ¨æ€æ¬¡æ¨¡é©±é€ï¼šè¯¥æ–¹æ³•å°†ç¼“å­˜ç®¡ç†æ„å»ºä¸ºä¸€ä¸ªå…·æœ‰æ¬¡æ¨¡ç›®æ ‡å‡½æ•°ğ¹(ğ‘†) çš„ä¼˜åŒ–é—®é¢˜ï¼Œè¯¥å‡½æ•°é‡åŒ–äº†æ ‡è®°é›†ğ‘†çš„é‡è¦æ€§ã€‚

    $$F(S)=\sum_{i\in S}A_{i}$$ 

- å…¶ä¸­ï¼Œ$$ğ´_ğ‘–$$æ˜¯æ ‡è®°ï¼ˆtokenï¼‰ğ‘–çš„ç´¯ç§¯æ³¨æ„åŠ›å¾—åˆ†ã€‚ç¼“å­˜ï¼ˆcacheï¼‰$$ğ‘†_ğ‘¡$$é€šè¿‡ä»¥ä¸‹æ–¹å¼è¿›è¡Œæ›´æ–°ã€‚

   $$S_t = \operatorname{argmax}_{S\subseteq S_{t - 1}\cup\{i\},|S|\leq k} F(S)$$ 

> è¯¥å…¬å¼çš„æ„æ€æ˜¯åœ¨æ‰€æœ‰æ»¡è¶³â€œ$$S$$ æ˜¯ $$S_{t - 1}\cup\{i\}$$ çš„å­é›†ä¸” $$S$$ ä¸­å…ƒç´ ä¸ªæ•°ä¸è¶…è¿‡ $$k$$ â€ çš„é›†åˆ $$S$$ ä¸­ï¼Œæ‰¾åˆ°ä¸€ä¸ªèƒ½ä½¿å‡½æ•° $$F(S)$$ å–å¾—æœ€å¤§å€¼çš„é›†åˆï¼Œå¹¶å°†å…¶èµ‹å€¼ç»™ $$S_t$$

ç¡®ä¿æ¯ä¸€æ­¥æœ€å¤šåªæœ‰ä¸€ä¸ªæ ‡è®°ï¼ˆtokenï¼‰è¢«é€å‡ºã€‚è¿™ç§è´ªå¿ƒç®—æ³•åœ¨è®¡ç®—ä¸Šæ˜¯é«˜æ•ˆçš„ï¼Œå¹¶ä¸”åœ¨æ¬¡æ¨¡çº¦æŸä¸‹ä¿è¯æ¥è¿‘æœ€ä¼˜çš„æ€§èƒ½ã€‚

ç»“æœï¼šåœ¨ç²¾åº¦æŸå¤±å¯å¿½ç•¥ä¸è®¡çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†é”®å€¼ç¼“å­˜å¤§å° 5 å€çš„ç¼©å‡ï¼Œå¹¶ä¸”ååé‡æé«˜äº†é«˜è¾¾ 29 å€ã€‚



#### 2) [StreamLLM](https://arxiv.org/abs/2309.17453) 

![img](http://pointerhacker.github.io/imgs/posts/deekseek-kv/streamingLLM.png)

**æ³¨æ„åŠ›æ±‡ç°è±¡**ï¼šé¦–å…ˆï¼Œè®ºæ–‡å‘ç°äº†ä¸€ä¸ªæœ‰è¶£çš„è‡ªå›å½’LLMsç°è±¡ï¼Œå³åˆå§‹ä»¤ç‰Œå¸å¼•äº†å¤§é‡æ³¨æ„åŠ›ï¼Œå³ä½¿å®ƒä»¬åœ¨è¯­ä¹‰ä¸Šä¸é‡è¦ã€‚è¿™ç§ç°è±¡è¢«ç§°ä¸ºâ€œæ³¨æ„åŠ›æ±‡â€ã€‚

- æ²¡æœ‰è¿™äº›æ³¨æ„åŠ›æ±‡èšæ ‡è®°ï¼Œæœ´ç´ çª—å£æ³¨æ„åŠ›çš„æ€§èƒ½ä¼šä¸‹é™ã€‚

**StreamingLLMæ¡†æ¶**ï¼šåŸºäºä¸Šè¿°åˆ†æï¼ŒStreamingLLMæ¡†æ¶é€šè¿‡ä¿ç•™åˆå§‹ä»¤ç‰Œçš„KVçŠ¶æ€å’Œæ»‘åŠ¨çª—å£çš„KVçŠ¶æ€æ¥ç¨³å®šæ³¨æ„åŠ›è®¡ç®—ã€‚å…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š

- åœ¨æ»‘åŠ¨çª—å£çš„KVç¼“å­˜ä¸­åŠ å…¥å‡ ä¸ªåˆå§‹ä»¤ç‰Œçš„KVçŠ¶æ€ã€‚
- é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒStreamingLLMèƒ½å¤Ÿåœ¨ä¸éœ€è¦å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œä½¿LLMså¤„ç†æ— é™é•¿åº¦çš„æ–‡æœ¬ã€‚

**é¢„è®­ç»ƒæ¨¡å‹**ï¼šä¸ºäº†è¿›ä¸€æ­¥æé«˜æµå¼éƒ¨ç½²çš„æ€§èƒ½ï¼Œè®ºæ–‡å»ºè®®åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­æ·»åŠ ä¸€ä¸ªå ä½ç¬¦ä»¤ç‰Œä½œä¸ºä¸“ç”¨çš„æ³¨æ„åŠ›æ±‡ã€‚é€šè¿‡åœ¨æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„å¼€å¤´æ·»åŠ ä¸€ä¸ªå¯å­¦ä¹ çš„ä»¤ç‰Œï¼Œå¯ä»¥å•ç‹¬ä½œä¸ºä¸€ä¸ªæ³¨æ„åŠ›æ±‡ï¼Œä»è€Œå‡å°‘å¯¹å¤šä¸ªåˆå§‹ä»¤ç‰Œçš„ä¾èµ–ã€‚

#### 3) [Value-Aware Token Pruning (VATP)](https://arxiv.org/abs/2406.12335)

> Attention Score is not All You Need for Token Importance Indicator in KV Cache Reduction: Value Also Matters

![img](http://pointerhacker.github.io/imgs/posts/deekseek-kv/vatp.png)

- å…·ä½“æ¥è¯´ï¼Œç°æœ‰çš„ä»¤ç‰Œå‰ªææ–¹æ³•ä»…ä¾èµ–äºæ³¨æ„åŠ›åˆ†æ•°æ¥è¯„ä¼°ä»¤ç‰Œçš„é‡è¦æ€§ï¼Œä½†ä½œè€…å‘ç°å€¼å‘é‡çš„èŒƒæ•°åˆ†å¸ƒä¸å‡åŒ€ï¼Œè´¨ç–‘äº†ä»…ä¾èµ–æ³¨æ„åŠ›åˆ†æ•°çš„å¯é æ€§ã€‚

è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä»¤ç‰Œå‰ªææ–¹æ³•ï¼Œç§°ä¸ºå€¼æ„ŸçŸ¥ä»¤ç‰Œå‰ªæï¼ˆVATPï¼‰ï¼Œç”¨äºè§£å†³LLMsä¸­KVç¼“å­˜å‡å°‘çš„é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œ

**æ³¨æ„åŠ›åˆ†æ•°åˆ†æ**ï¼šé¦–å…ˆï¼Œä½œè€…åˆ†æäº†æ³¨æ„åŠ›æœºåˆ¶è¾“å‡ºï¼Œå…¶å®šä¹‰ä¸ºï¼š

$$\text{Attention}(Q, K, V)_t = \sum_{i\leq t} a_{i}^{t}v_{i}$$ 

å…¶ä¸­ï¼Œ$$a_i^t$$ æ˜¯æŸ¥è¯¢ä»¤ç‰Œtå¯¹ä»¤ç‰Œiçš„æ³¨æ„åŠ›åˆ†æ•°ï¼Œ$$v_i$$ æ˜¯ä»¤ç‰Œiçš„å€¼çŠ¶æ€ã€‚æ¯ä¸ªä»¤ç‰Œå¯¹æ³¨æ„åŠ›è¾“å‡ºçš„å½±å“ç”±æ³¨æ„åŠ›åˆ†æ•°å’Œå€¼å‘é‡å…±åŒå†³å®šã€‚

**å€¼å‘é‡èŒƒæ•°åˆ†æ**ï¼šä½œè€…è§‚å¯Ÿåˆ°å€¼å‘é‡çš„$$\ell_{1}$$ èŒƒæ•°åœ¨æ‰€æœ‰å±‚å’Œå¤´ä¸Šåˆ†å¸ƒä¸å‡ï¼Œä¸”æ³¨æ„åŠ›æ±‡èšä»¤ç‰Œçš„$$\ell_{1}$$ èŒƒæ•°è¾ƒå°ã€‚è¿™è¡¨æ˜ä»…ä¾èµ–æ³¨æ„åŠ›åˆ†æ•°å¯èƒ½ä¼šå¿½ç•¥è¿™äº›é‡è¦ä»¤ç‰Œã€‚

**VATPæ–¹æ³•**ï¼šä¸ºäº†ç»¼åˆè€ƒè™‘æ³¨æ„åŠ›åˆ†æ•°å’Œå€¼å‘é‡èŒƒæ•°ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªæ–°çš„ä»¤ç‰Œé‡è¦æ€§è¯„ä¼°æŒ‡æ ‡ï¼š

 $$I_{k}^{t} = S_{k}^{t} \cdot \left\lVert v_{k} \right\rVert_{1}$$ 

å…¶ä¸­ï¼Œ$$S_{k}^{t}$$æ˜¯ä»¤ç‰Œkåœ¨è§£ç æ­¥tçš„ç´¯ç§¯æ³¨æ„åŠ›åˆ†æ•°ï¼Œ$$\left\lVert v_{k} \right\rVert_{1}$$ æ˜¯ä»¤ç‰Œkçš„å€¼å‘é‡çš„$$\ell_{1}$$ èŒƒæ•°ã€‚é€šè¿‡è®¡ç®—æ¯ä¸ªä»¤ç‰Œçš„æ³¨æ„åŠ›åˆ†æ•°å’Œå€¼å‘é‡èŒƒæ•°çš„ä¹˜ç§¯ï¼Œå¯ä»¥æ›´å…¨é¢åœ°è¯„ä¼°ä»¤ç‰Œçš„é‡è¦æ€§ã€‚

**æ³¨æ„åŠ›æ±‡èšä»¤ç‰Œå¤„ç†**ï¼šç”±äºæ³¨æ„åŠ›æ±‡èšä»¤ç‰Œçš„$$\ell_{1}$$ èŒƒæ•°è¾ƒå°ï¼Œæ ¹æ®VATPæŒ‡æ ‡ï¼Œè¿™äº›ä»¤ç‰Œçš„é‡è¦æ€§è¯„åˆ†ä¼šè¢«æ˜¾è‘—é™ä½ã€‚ä¸ºäº†é¿å…ç§»é™¤è¿™äº›ä»¤ç‰Œå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œä½œè€…é€‰æ‹©ä¿ç•™å‰Fä¸ªä»¤ç‰Œï¼Œå…¶ä¸­F=20ï¼ˆå¯¹äºLLaMA2-7B-chatï¼‰å’ŒF=40ï¼ˆå¯¹äºVicuna-v1.5-7B-16kï¼‰ã€‚



##### ç»“æœä¸åˆ†æ

1. **ä¸»è¦ç»“æœ**ï¼šåœ¨LLaMA2-7B-chatæ¨¡å‹ä¸Šï¼ŒVATPæ–¹æ³•åœ¨12ä¸ªä»»åŠ¡ä¸­ä¼˜äº*H*2*O*ï¼Œåœ¨13ä¸ªä»»åŠ¡ä¸­ä¼˜äºScissorhandsã€‚åœ¨Vicuna-v1.5-7B-16kæ¨¡å‹ä¸Šï¼ŒVATPæ–¹æ³•åœ¨12ä¸ªä»»åŠ¡ä¸­ä¼˜äº*H*2*O*ï¼Œåœ¨14ä¸ªä»»åŠ¡ä¸­ä¼˜äºScissorhandsã€‚å°½ç®¡åœ¨æŸäº›ä»»åŠ¡ä¸­VATPæœªèƒ½è¶…è¶ŠåŸºçº¿ï¼Œä½†å…¶æ€§èƒ½ä»ç„¶éå¸¸æ¥è¿‘åŸºçº¿ã€‚



### äº‹åå‹ç¼©æŠ€æœ¯

è¿™äº›æ–¹æ³•åœ¨ä¿ç•™æ ‡å‡† transformeræ¶æ„çš„åŒæ—¶å‹ç¼©æˆ–ä¼˜åŒ–é”®å€¼ç¼“å­˜ã€‚

##### 4) [Adaptive KV Compression (FastGen)](https://arxiv.org/pdf/2310.01801)

> å¦‚ä½•åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMSï¼‰ä¸­è¿›è¡Œè‡ªé€‚åº”çš„é”®å€¼ç¼“å­˜å‹ç¼©ï¼Œä»¥å‡å°‘å†…å­˜å ç”¨å¹¶æé«˜ç”Ÿæˆæ¨ç†çš„æ•ˆç‡ã€‚

![img](http://pointerhacker.github.io/imgs/posts/deekseek-kv/fastgen_1.png)

è¿™ç¯‡è®ºæ–‡æå‡ºäº†FastGenæ–¹æ³•ï¼Œç”¨äºè§£å†³å¤§è¯­è¨€æ¨¡å‹ä¸­KVç¼“å­˜å‹ç¼©çš„é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼ŒåŸºäºè¿è¡Œæ—¶è§‚å¯Ÿåˆ°çš„æ³¨æ„åŠ›æ¨¡å¼çš„è‡ªé€‚åº”å‹ç¼©ã€‚åœ¨æç¤ºç¼–ç æœŸé—´ï¼ŒFastGen ä¼šè¯†åˆ«æ³¨æ„åŠ›æ¨¡å¼ï¼Œå¹¶é€‰æ‹©èƒ½åœ¨ä¿ç•™æ³¨æ„åŠ›æ¢å¤çš„åŒæ—¶æœ€å°åŒ–å†…å­˜æˆæœ¬çš„å‹ç¼©ç­–ç•¥ C*ã€‚

1. **æ¨¡å‹åˆ†æ**ï¼šé¦–å…ˆï¼Œä½¿ç”¨é«˜æ•ˆçš„æ€§èƒ½åˆ†æç®—æ³•è¯†åˆ«æ³¨æ„åŠ›æ¨¡å—çš„ç»“æ„æ¨¡å¼ã€‚é€šè¿‡å¯¹æç¤ºç¼–ç çš„ç»“æœè¿›è¡Œåˆ†æï¼Œé€‰æ‹©æœ€é€‚åˆæ¯ä¸ªæ³¨æ„åŠ›å¤´çš„å‹ç¼©ç­–ç•¥ã€‚
2. **è‡ªé€‚åº”KVç¼“å­˜æ„å»º**ï¼šåœ¨æç¤ºç¼–ç é˜¶æ®µï¼Œè¿›è¡Œæ¨¡å‹åˆ†æä»¥è¯†åˆ«ä¸åŒæ³¨æ„åŠ›å¤´çš„ç»“æ„ç‰¹å¾ã€‚ç„¶åï¼Œåœ¨ä»¤ç‰Œç”Ÿæˆé˜¶æ®µï¼Œæ ¹æ®æ¯ä¸ªä»¤ç‰Œçš„å‹ç¼©ç­–ç•¥ç®¡ç†KVç¼“å­˜ã€‚
3. **å‹ç¼©ç­–ç•¥**ï¼šæå‡ºäº†äº”ç§åŸºæœ¬çš„KVç¼“å­˜å‹ç¼©ç­–ç•¥ï¼šç‰¹æ®Šä»¤ç‰Œã€æ ‡ç‚¹ç¬¦å·ã€å±€éƒ¨æ€§ã€é¢‘ç‡ï¼ˆé‡å‡»è€…ï¼‰å’Œæ··åˆç­–ç•¥ã€‚æ··åˆç­–ç•¥é€šè¿‡è´ªå¿ƒæ–¹æ³•æ„å»ºï¼Œä¸»è¦åŒ…æ‹¬ç‰¹æ®Šä»¤ç‰Œã€ç‰¹æ®Šä»¤ç‰ŒåŠ æ ‡ç‚¹ç¬¦å·ã€ç‰¹æ®Šä»¤ç‰ŒåŠ é¢‘ç‡ã€ç‰¹æ®Šä»¤ç‰ŒåŠ é¢‘ç‡åŠ å±€éƒ¨æ€§å’Œå…¨é‡ç¼“å­˜ã€‚

å‹ç¼©ç­–ç•¥åŒ…æ‹¬ï¼š

- ç‰¹æ®Šä»¤ç‰Œï¼ˆ $$C_{special}$$ï¼‰ï¼šä»…ä¿ç•™ç‰¹æ®Šä»¤ç‰Œã€‚
- å±€éƒ¨æ€§ï¼ˆ$$C_{local}$$ï¼‰ : æ˜¯æŒ‡é©±é€è¶…å‡ºç›¸å¯¹è·ç¦»ğ‘Ÿğ‘™çš„æ ‡è®°ã€‚
- é¢‘ç‡ï¼ˆ $$C_{frequent}$$ï¼‰ï¼šä¿æŒå…·æœ‰è¾ƒé«˜ç´¯ç§¯æ³¨æ„åˆ†æ•°çš„ä»¤ç‰Œï¼ˆ $$r_f$$ ï¼‰ã€‚
- æ··åˆæ”¿ç­–ç»“åˆäº†ç­–ç•¥ï¼Œ$$ä» C_special$$å¼€å§‹ï¼Œå¹¶é€‚ç”¨äºæ¯ä¸ªå¤´éƒ¨ï¼š

 $$\mathcal{C}=\{C_{\text{special}},C_{\text{special}} + C_{\text{punct}},\ldots,C_{\text{full}}\}.$$ 

Tokenç”Ÿæˆ

- åœ¨è§£ç è¿‡ç¨‹ä¸­ï¼Œé¢„é€‰çš„å‹ç¼©ç­–ç•¥æœ‰æ•ˆåœ°ç®¡ç†KVç¼“å­˜ï¼š

  $$K_{C_i},V_{C_i}=f(K,V,C_i).$$ 



- å…¬å¼1ï¼šç”¨äºé€‰æ‹©æœ€ä¼˜å‹ç¼©ç­–ç•¥çš„ä¼˜åŒ–é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œå…¬å¼1çš„ç›®æ ‡æ˜¯æœ€å°åŒ–å‹ç¼©åçš„KVç¼“å­˜å†…å­˜æˆæœ¬ï¼ŒåŒæ—¶ä¿è¯æ¢å¤æ¯”ç‡Tæ»¡è¶³æ¡ä»¶ï¼š

   $$C^{*}=\underset{C\in C}{\arg \min}\text{CacheMemoryCost}(C)\quad \text{s.t.}\quad |A - \text{softmax}(QKT)|\leq 1 - T$$

  å…¶ä¸­ï¼Œ$C$æ˜¯æ‰€æœ‰å¯è¡Œçš„å‹ç¼©ç­–ç•¥é›†åˆï¼Œ$\text{CacheMemoryCost}(C)$æ˜¯å‹ç¼©ç­–ç•¥$C$çš„ç›®æ ‡å†…å­˜é¢„ç®—ï¼Œ$T$æ˜¯é¢„å®šä¹‰çš„è¶…å‚æ•°ï¼Œè¡¨ç¤ºç­–ç•¥æ¢å¤æ³¨æ„åŠ›å›¾$A$çš„ç¨‹åº¦ã€‚ 



##### 5) [Dynamic Memory Compression (DMC)](https://arxiv.org/pdf/2403.09636)

![img](http://pointerhacker.github.io/imgs/posts/deekseek-kv/dmc.png)

è¿™ç¯‡è®ºæ–‡æå‡ºäº†åŠ¨æ€å†…å­˜å‹ç¼©ï¼ˆDMCï¼‰ç”¨äºè§£æ±ºLLMsåœ¨æ¨ç†æ—¶çš„å†…å­˜æ•ˆç‡é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œ

1. **DMCçš„åŸºæœ¬åŸç†**ï¼šDMCåœ¨æ¯ä¸ªæ—¶é—´æ­¥å†³å®šæ˜¯å°†å½“å‰çš„é”®å€¼è¡¨ç¤ºè¿½åŠ åˆ°ç¼“å­˜ä¸­ï¼Œè¿˜æ˜¯å°†å…¶ä¸ç¼“å­˜ä¸­çš„æœ€åä¸€ä¸ªå…ƒç´ è¿›è¡ŒåŠ æƒå¹³å‡ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå†…å­˜çš„å¢é•¿æ˜¯äºšçº¿æ€§çš„ï¼Œä»‹äºä¼ ç»Ÿçš„Transformerså’ŒçŠ¶æ€ç©ºé—´è¯­è¨€æ¨¡å‹ä¹‹é—´ã€‚

2. å•å¤´é”®å€¼ç¼“å­˜æ›´æ–°ï¼šåœ¨å•å¤´æƒ…å†µä¸‹ï¼ŒDMCçš„é”®å€¼ç¼“å­˜æ›´æ–°è¿‡ç¨‹
    å¦‚ä¸‹ï¼š

   - é¢„æµ‹å†³ç­–å˜é‡ $$a_t$$ å’Œé‡è¦æ€§å˜é‡ $$w_t$$ã€‚

   - æ ¹æ® $$a_t$$çš„å€¼ï¼Œå†³å®šå°†å½“å‰çš„é”®å€¼è¡¨ç¤ºè¿½åŠ åˆ°ç¼“å­˜ä¸­è¿˜æ˜¯è¿›è¡ŒåŠ æƒå¹³å‡ã€‚

   - å…¬å¼å¦‚ä¸‹ï¼š

     ![image-20250217115453044](http://pointerhacker.github.io/imgs/posts/deekseek-kv/image-20250217115453044.png)
    - é€šè¿‡è¿™ç§æ–¹å¼ï¼šDMCçš„ç¼“å­˜é•¿åº¦$l$ä¸º$l = \sum_{i = 1}^{t}(1 - \alpha_{i})\leq t$ï¼Œè€Œä¼ ç»Ÿçš„Transformersä¸­$l = t$ã€‚ 

3. **è®­ç»ƒè¿‡ç¨‹**ï¼šä¸ºäº†ä½¿LLMså…·å¤‡DMCè¡Œä¸ºï¼Œé€šè¿‡åœ¨å°‘é‡åŸå§‹é¢„è®­ç»ƒæ•°æ®ä¸Šè¿›è¡Œç»§ç»­é¢„è®­ç»ƒï¼Œé€æ­¥å¢åŠ å‹ç¼©ç‡ã€‚å…·ä½“æ­¥éª¤åŒ…æ‹¬ï¼š

   - ï»¿ä½¿ç”¨æ¢¯åº¦ä¸‹é™å’Œè¿ç»­æ¾å¼›å†³ç­–å˜é‡ã€‚

   - ï»¿å®šä¹‰éƒ¨åˆ†ç´¯ç§¯çŠ¶æ€ä»¥å¤„ç† a â‚¬ï¼»Oï¼Œå·¥çš„æƒ…å†µã€‚

   - ï»¿ï»¿åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨åŠ æ€§æ©ç æ¥æ¨¡æ‹Ÿæ¨ç†æ—¶çš„è¡Œä¸ºã€‚

DMCå¼•å…¥äº†è‡ªé€‚åº”ä»¤ç‰Œåˆå¹¶ï¼š

- å†³ç­–æœºåˆ¶ï¼šåœ¨æ—¶é—´ t æ—¶ï¼Œé¢„æµ‹åˆå¹¶å†³ç­– Î±t å’Œæƒé‡ Ï‰tã€‚

   $$\alpha_{t}=\lfloor\operatorname{sigmoid}(k_{t}[0])\rceil,\quad \omega_{t}=\operatorname{sigmoid}(q_{t}[0]).$$ 

##### è®­ç»ƒï¼š 

- å¯¹$\alpha_t$ä½¿ç”¨Gumbel - Sigmoidæ¾å¼›æ³•ï¼Œä»¥ä¾¿é€šè¿‡æ¢¯åº¦ä¸‹é™è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒï¼š

   $$\alpha_{t}\sim \text{Gumbel - Sigmoid}(k_{t}[0],\tau)$$ 

  å…¶ä¸­$\tau$æ˜¯æ¸©åº¦å‚æ•°ã€‚ 

  

- ä¼˜åŒ–ä¸€ä¸ªç»„åˆç›®æ ‡ï¼š 

  $$\mathcal{L}=\mathcal{L}_{\text{LM}}+\lambda\max\left(0,\frac{n}{\text{CR}}-\sum_{t}\alpha_{t}\right)$$ 
  
  å…¶ä¸­$\mathcal{L}_{LM}$æ˜¯è¯­è¨€å»ºæ¨¡æŸå¤±ï¼Œç¬¬äºŒé¡¹é¼“åŠ±æ¨¡å‹è¾¾åˆ°ç›®æ ‡å‹ç¼©æ¯”ï¼ˆCRï¼‰ã€‚ ### å…¬å¼



##### 6) [L2 Norm-Based Compression](https://arxiv.org/pdf/2406.11430)

è¯¥é—®é¢˜çš„ç ”ç©¶éš¾ç‚¹åŒ…æ‹¬ï¼šç°æœ‰æ–¹æ³•é€šå¸¸æ¶‰åŠå¤æ‚çš„ç®—æ³•æˆ–æ˜¾è‘—çš„è®¡ç®—å¼€é”€ï¼Œé™åˆ¶äº†å…¶å®ç”¨æ€§ï¼›æ­¤å¤–ï¼Œè®¸å¤šåå¤„ç†å‹ç¼©ç®—æ³•ä¾èµ–äºæ³¨æ„åŠ›åˆ†æ•°ï¼Œè¿™äº›åˆ†æ•°**æ— æ³•ä¸FlashAttentionå…¼å®¹**

è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºé”®åµŒå…¥$$L_2$$èŒƒæ•°çš„ç®€å•è€Œæœ‰æ•ˆçš„KVç¼“å­˜å‹ç¼©ç­–ç•¥ã€‚å…·ä½“æ¥è¯´ï¼Œ

1. **æ³¨æ„åŠ›åˆ†å¸ƒåˆ†æï¼š**é¦–å…ˆï¼Œä½œè€…åˆ†æäº†ä»…è§£ç å™¨Transformeræ¨¡å‹ä¸­çš„æ³¨æ„åŠ›åˆ†å¸ƒï¼Œå‘ç°å¤§å¤šæ•°å±‚ä¸­æ³¨æ„åŠ›åˆ†é…æ¨¡å¼ä¿æŒä¸€è‡´ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œé”®åµŒå…¥çš„$$L_2$$èŒƒæ•°ä¸æ³¨æ„åŠ›åˆ†æ•°ä¹‹é—´å­˜åœ¨æ˜æ˜¾çš„ç›¸å…³æ€§ï¼Œ**ä½$$L_{2}$$èŒƒæ•°çš„é”®åµŒå…¥é€šå¸¸åœ¨è§£ç è¿‡ç¨‹ä¸­å¯¼è‡´é«˜æ³¨æ„åŠ›åˆ†æ•°**

2. **å‹ç¼©ç­–ç•¥**ï¼šåŸºäºä¸Šè¿°è§‚å¯Ÿï¼Œä½œè€…æå‡ºäº†ä¸€ç§å‹ç¼©ç­–ç•¥ï¼Œå³ä»…ä¿ç•™å…·æœ‰æœ€ä½$$L_2$$èŒƒæ•°çš„é”®åŠå…¶å¯¹åº”çš„å€¼ã€‚è¿™ç§æ–¹æ³•ä¸éœ€è¦é¢å¤–çš„è®­ç»ƒæˆ–æ˜¾è‘—çš„ä¿®æ”¹ï¼Œå¯ä»¥ç›´æ¥åº”ç”¨äºä»»ä½•åŸºäºTransformerçš„è§£ç å™¨ä»…LLMã€‚

    åŸºäºèŒƒæ•°çš„é€‰æ‹©ï¼šå¯¹äºä¸€ç»„ç¼“å­˜é”®$K = \{k_1, k_2, \ldots, k_n\}$ï¼Œè®¡ç®—å¹¶å¯¹é”®çš„èŒƒæ•°è¿›è¡Œæ’åº ã€‚

   $$\left\lVert k_{i}\right\rVert_{2}=\sqrt{\sum_{j = 1}^{d}k_{i,j}^{2}}$$ 

3. **æ³¨æ„åŠ›æŸå¤±ä¼°è®¡**ï¼šä¸ºäº†è¯„ä¼°å‹ç¼©çš„å½±å“ï¼Œä½œè€…å®šä¹‰äº†ç”±äºå‹ç¼©è€Œå¯¼è‡´çš„æ³¨æ„åŠ›æŸå¤±ã€‚å…·ä½“æ¥è¯´ï¼Œç»™å®šä¸€ä¸ªæç¤ºåºåˆ—ï¼ŒLLMé¦–å…ˆå°†å…¶ç¼–ç ä¸ºKVç¼“å­˜ï¼Œç„¶åè‡ªå›å½’åœ°ç”Ÿæˆä¸‹ä¸€ä¸ªæ ‡è®°ã€‚åœ¨å‹ç¼©è¿‡ç¨‹ä¸­ï¼ŒæŸäº›é”®å€¼å¯¹è¢«ä¸¢å¼ƒï¼Œä½œè€…å®šä¹‰äº†ç”±äºå‹ç¼©è€Œå¯¼è‡´çš„æ³¨æ„åŠ›æŸå¤±ä¸º**ä¸Ÿå¼ƒçš„é”®å€¼å¯¹çš„æ³¨æ„åŠ›åˆ†æ•°ä¹‹å’Œ**ã€‚

4. **ç›¸å…³ç³»æ•°ALR**ï¼šä¸ºäº†è¡¡é‡ç†æƒ³æ³¨æ„åŠ›åˆ†æ•°å‹ç¼©ä¸åŸºäº*L*2èŒƒæ•°å‹ç¼©ä¹‹é—´çš„å·®å¼‚ï¼Œä½œè€…å¼•å…¥äº†ä¸€ä¸ªç›¸å…³ç³»æ•°ALRï¼ˆAttention Loss Ratioï¼‰ï¼Œå…¶å®šä¹‰ä¸ºï¼š

â€‹      $$\mathcal{Y}_{l,h}=\sum_{m = 1}^{n}\mathcal{Y}_{l,h}^{m}$$ 

æ–‡å­—ï¼š å…¶ä¸­ï¼Œ$\mathcal{Y}_{l,h}^{m}$è¡¨ç¤ºåœ¨ç¬¬lå±‚ã€ç¬¬hä¸ªå¤´çš„å‹ç¼©æ–¹æ³•ä¸‹ï¼Œä¸¢å¼ƒmå¯¹é”®å€¼å¯¹åçš„æ³¨æ„åŠ›æŸå¤±ã€‚



### æ¨¡å‹ç»“æ„é‡æ–°è®¾è®¡

è¿™äº›æ–¹æ³•ä¼šæ›´æ”¹transformerç»“æ„ï¼Œä»¥æ›´æœ‰æ•ˆåœ°å¤„ç†KVç¼“å­˜ï¼Œé€šå¸¸å°†å‹ç¼©ç›´æ¥çº³å…¥æ¨¡å‹ç»“æ„ã€‚

### 7) [Multi-Query Attention (MQA)](https://arxiv.org/pdf/2305.13245)

![img](http://pointerhacker.github.io/imgs/posts/deekseek-kv/mqa.png)

**å…³é”®æ€æƒ³**ï¼šMQA é€šè¿‡åœ¨æ‰€æœ‰æŸ¥è¯¢å¤´ä¸­å…±äº«å•ä¸ªé”®å€¼å¤´æ¥å‡å° KV ç¼“å­˜å¤§å°ï¼Œå–ä»£äº†ä¼ ç»Ÿçš„å¤šå¤´æ³¨æ„åŠ›ï¼ˆMHAï¼‰ã€‚

 $$K = XW_{K}, \quad V = XW_{V}$$

å…¶ä¸­ K å’Œ V æ˜¯å…±äº«çš„é”®å’Œå€¼ç°å½¢å˜åŒ–ã€‚

**å¥½å¤„**ï¼šå°† KV ç¼“å­˜å¤§å°å‡å°‘äº†ï¼ˆæ³¨æ„åŠ›å¤´çš„æ•°é‡ï¼‰ï¼Œæ˜¾ç€é™ä½äº†å†…å­˜å¸¦å®½å¼€é”€ã€‚

**æƒè¡¡ï¼š**è™½ç„¶ MQA é€Ÿåº¦æ›´å¿«ï¼Œä½†å®ƒç»å¸¸é­å—è´¨é‡ä¸‹é™ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ä¸åŒæ³¨æ„åŠ›æ¨¡å¼çš„ä»»åŠ¡ä¸­ã€‚

### 8) [Group-Query Attention (GQA)](https://arxiv.org/abs/2305.13245)

![img](http://pointerhacker.github.io/imgs/posts/deekseek-kv/gqa.png)

**å…³é”®æ€æƒ³ï¼š**GQA åœ¨å®Œå…¨å¤šå¤´æ³¨æ„åŠ›å’Œ MQA ä¹‹é—´è¿›è¡Œæ’å€¼ï¼Œä»¥åœ¨æ¨ç†é€Ÿåº¦å’Œæ¨¡å‹è´¨é‡ä¹‹é—´æä¾›å¯æ‰©å±•çš„æƒè¡¡ã€‚å®ƒå°†æŸ¥è¯¢å¤´åˆ†ä¸º G ç»„ï¼Œå…¶ä¸­æ¯ç»„å…±äº«ä¸€ä¸ªå•ä¸€çš„é”®å€¼å¤´ã€‚

$$K_{\text{group}} = \frac{1}{|G|} \sum_{h\in G} K_{h}, \quad V_{\text{group}} = \frac{1}{|G|} \sum_{h\in G} V_{h}$$ 

- GQA-1ï¼šç­‰æ•ˆäºMQAï¼ˆG=1ï¼‰ã€‚
- GQA-ğ»: å®ƒç­‰åŒäº MHAï¼ˆå³ â€œğº=ğ»â€ï¼‰ã€‚

Uptraining: GQA å¯ä»¥é€šè¿‡å¾®è°ƒè¢«å¼•å…¥åˆ°ç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹ä¸­ã€‚

- é¦–å…ˆï¼Œé€šè¿‡å°†é”®å’Œå€¼å¤´è¿›è¡Œå¹³å‡æ± åŒ–åˆ†ç»„ï¼Œå°† MHAï¼ˆå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼‰æ£€æŸ¥ç‚¹è½¬æ¢ä¸º GQAã€‚
- ç„¶åï¼Œå¯¹æ¨¡å‹è¿›è¡Œç®€çŸ­çš„å¾®è°ƒï¼ˆâ€œuptrainâ€ï¼Œå³å†æ¬¡è®­ç»ƒï¼‰ä»¥é€‚åº”æ–°çš„æ³¨æ„åŠ›æ¨¡å¼ã€‚
- è¿™ä¸ªé€‚åº”è¿‡ç¨‹ä»…éœ€è¦åŸå§‹é¢„è®­ç»ƒè®¡ç®—é‡çš„ 5%ï¼Œå› æ­¤éå¸¸é«˜æ•ˆã€‚
- æœ€ç»ˆå¾—åˆ°çš„æ¨¡å‹åœ¨ä¿æŒè´¨é‡çš„åŒæ—¶è·å¾—äº† GQA çš„å†…å­˜ä¼˜åŠ¿ã€‚

```python
def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
    """
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    if n_rep == 1:
        return hidden_states
    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
```





### 9) [Multi-head Latent Attention (MLA)](https://arxiv.org/abs/2405.04434)

![img](http://pointerhacker.github.io/imgs/posts/deekseek-kv/mla.png)

DeepSeek çš„å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›ï¼ˆMLAï¼‰é‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•æ¥å‡å°‘é”®å€¼ç¼“å­˜å¼€é”€

è™½ç„¶ MQAå’Œ GQA é€šè¿‡ head-sharing æ¥å®ç°è¿™ä¸€ç›®æ ‡ï¼Œä½† MLAå´é‡‡ç”¨äº†ä¸€ç§ä½ç§©æ½œåœ¨å‹ç¼©æŠ€æœ¯ï¼Œè¿™ç§æŠ€æœ¯ä¿æŒäº†å¤šä¸ªæ³¨æ„åŠ›å¤´çš„ä¼˜åŠ¿ã€‚

- MLAé€šè¿‡åœ¨é‡å»ºä¹‹å‰å°†é”®(keys)å’Œå€¼(values)å‹ç¼©æˆä½ç»´æ½œåœ¨å‘é‡æ¥å‡å°‘KVç¼“å­˜çš„å¤§å°ã€‚

  å®ƒå°†é”®å€¼åµŒå…¥ä¸‹æŠ•å½±åˆ°å‹ç¼©çš„æ½œåœ¨ç©ºé—´ï¼Œç›¸å…³å…¬å¼å¦‚ä¸‹ï¼š

  $$c_{KV,t} = W_{DKV}h_t, \quad k_C = W_{UK}c_{KV,t}, \quad v_C = W_{UV}c_{KV,t}$$

  å…¶ä¸­ï¼š

  - $W_{DKV}$ æ˜¯ä¸‹æŠ•å½±çŸ©é˜µ
  - $W_{UK}$, $W_{UV}$ æ˜¯é”®å’Œå€¼çš„ä¸ŠæŠ•å½±çŸ©é˜µ

- ä¸MQA(Multi-Query Attention)å®Œå…¨å…±äº«æ³¨æ„åŠ›å¤´çš„æ–¹å¼ä¸åŒï¼Œå®ƒé€šè¿‡å‹ç¼©è¡¨ç¤ºä¿æŒäº†æ¯ä¸ªå¤´éƒ¨çš„çµæ´»æ€§ã€‚

- å®ƒå¼•å…¥äº†æ—‹è½¬ä½ç½®åµŒå…¥(Rotary Positional Embeddings, RoPE)æ¥è§£è€¦ä½ç½®æ„ŸçŸ¥çš„é”®ï¼š

  $$k_R = RoPE(W_{KR}h_t), \quad k_t = [k_C; k_R]$$

  è¿™ç§æ–¹æ³•é€šè¿‡åªç¼“å­˜å‹ç¼©çš„æ½œåœ¨å‘é‡ $c_{KV}$ å’Œä½ç½®é”® $k_R$ è¿›ä¸€æ­¥å‡å°‘äº†KVç¼“å­˜å­˜å‚¨ç©ºé—´ã€‚

```python
def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        output_attentions: bool = False,
        use_cache: bool = False,
        **kwargs,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        if "padding_mask" in kwargs:
            warnings.warn(
                "Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`"
            )
        bsz, q_len, _ = hidden_states.size()

        if self.q_lora_rank is None:
            q = self.q_proj(hidden_states)
        else:
            # bsz,q_len,d -> bsz,q_len,q_lora_rank
            q = self.q_b_proj(self.q_a_layernorm(self.q_a_proj(hidden_states)))
        
        #  bsz,q_len,q_lora_rank ->  bsz,q_len,num_heads,q_head_dim
        # -> bsz,num_heads,q_len,q_head_dim
        q = q.view(bsz, q_len, self.num_heads, self.q_head_dim).transpose(1, 2)
        # bsz,num_heads,q_len,q_head_dim -> 
        #   q_nope: bsz,num_heads,q_len,qk_nope_head_dim
        #   q_pe: bsz,num_heads,q_len,qk_rope_head_dim
        q_nope, q_pe = torch.split(
            q, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1
        )

        # bsz,q_len,d -> bsz,q_len, kv_lora_rank + qk_rope_head_dim,
        compressed_kv = self.kv_a_proj_with_mqa(hidden_states)
        # bsz,q_len, kv_lora_rank + qk_rope_head_dim,
        # compressed_kv:  bsz,q_len, kv_lora_rank
        # k_pe:  bsz,q_len, qk_rope_head_dim
        
        compressed_kv, k_pe = torch.split(
            compressed_kv, [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1
        )
        
        #  bsz,q_len, qk_rope_head_dim -> bsz,q_len, 1, qk_rope_head_dim -> bsz,1, q_len, qk_rope_head_dim
        k_pe = k_pe.view(bsz, q_len, 1, self.qk_rope_head_dim).transpose(1, 2)
        
        #  bsz,q_len, kv_lora_rank ->bsz,q_len, num_heads * (q_head_dim - qk_rope_head_dim + v_head_dim)
        # -> bsz, q_len, self.num_heads, qk_nope_head_dim + v_head_dim
        # ->  bsz, num_heads, q_len, qk_nope_head_dim + v_head_dim
        kv = (
            self.kv_b_proj(self.kv_a_layernorm(compressed_kv))
            .view(bsz, q_len, self.num_heads, self.qk_nope_head_dim + self.v_head_dim)
            .transpose(1, 2)
        )

        # bsz, num_heads, q_len, qk_nope_head_dim + v_head_dim
        # k_nope: bszï¼Œnum_heads, q_len, qk_nope_head_dim
        # value_states: bsz,  num_heads, q_len, v_head_dim
        k_nope, value_states = torch.split(
            kv, [self.qk_nope_head_dim, self.v_head_dim], dim=-1
        )
        
        # q_len
        kv_seq_len = value_states.shape[-2]
        if past_key_value is not None:
            if self.layer_idx is None:
                raise ValueError(
                    f"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} "
                    "for auto-regressive decoding with k/v caching, please make sure to initialize the attention class "
                    "with a layer index."
                )
            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)
        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)

        #   q_pe: bsz,num_heads,q_len,qk_rope_head_dim
        #   k_pe:  bsz,1, q_len, qk_rope_head_dim
        q_pe, k_pe = apply_rotary_pos_emb(q_pe, k_pe, cos, sin, position_ids)

        # bsz,num_heads,q_len,q_head_dim
        query_states = k_pe.new_empty(bsz, self.num_heads, q_len, self.q_head_dim)
        query_states[:, :, :, : self.qk_nope_head_dim] = q_nope
        query_states[:, :, :, self.qk_nope_head_dim :] = q_pe

        # # bsz,num_heads,q_len,q_head_dim
        key_states = k_pe.new_empty(bsz, self.num_heads, q_len, self.q_head_dim)
        key_states[:, :, :, : self.qk_nope_head_dim] = k_nope
        key_states[:, :, :, self.qk_nope_head_dim :] = k_pe
        if past_key_value is not None:
            cache_kwargs = {"sin": sin, "cos": cos}  # Specific to RoPE models
            key_states, value_states = past_key_value.update(
                key_states, value_states, self.layer_idx, cache_kwargs
            )

        attn_weights = (
            torch.matmul(query_states, key_states.transpose(2, 3)) * self.softmax_scale
        )
        
        attn_output = torch.matmul(attn_weights, value_states)
        
         return attn_output, attn_weights, past_key_value
```





### 10) [SnapKV](https://arxiv.org/pdf/2404.14469)

![img](http://pointerhacker.github.io/imgs/posts/deekseek-kv/snapKV.png)

ç”¨äºè§£å†³LLMsåœ¨å¤„ç†é•¿è¾“å…¥åºåˆ—æ—¶KVç¼“å­˜æ‰©å±•å¸¦æ¥çš„æ•ˆç‡å’Œå†…å­˜é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œ

- **è§‚å¯Ÿæ¨¡å¼ä¸€è‡´æ€§**ï¼šç ”ç©¶å‘ç°ï¼Œåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ªæ³¨æ„åŠ›å¤´å§‹ç»ˆå…³æ³¨ç‰¹å®šçš„æç¤ºæ³¨æ„åŠ›ç‰¹å¾ã€‚è¿™äº›ç‰¹å¾å¯ä»¥ä»æç¤ºæœ«å°¾çš„â€œè§‚å¯Ÿâ€çª—å£ä¸­è·å¾—ã€‚

- **æŠ•ç¥¨æœºåˆ¶**ï¼šé€šè¿‡æŠ•ç¥¨æœºåˆ¶é€‰æ‹©é‡è¦çš„KVä½ç½®ã€‚å…·ä½“å…¬å¼å¦‚ä¸‹ï¼š

   $$C = \sum_{i = 0}^{L_{\text{obs}}} W_{\text{obs}}[:, i, :], \quad I = \text{Top}_{k}(C, k)$$ 



### 11) [You Only Cache Once (YOCO)](https://arxiv.org/pdf/2405.05254)

Yocoä¿®æ”¹äº†ç”¨äºç¼“å­˜çš„Transformersä½“ç³»ç»“æ„ï¼š

- å…¨å±€ç¼“å­˜ï¼šä½¿ç”¨å•ä¸ªå…±äº«KVç¼“å­˜çš„è§£ç å™¨ç¼–ç å™¨è®¾è®¡ã€‚
- å¤æ‚æ€§é™ä½ï¼šå°†å­˜å‚¨å™¨ç©ºé—´ä» O(NÃ—L) é™ä½åˆ° O(N+L)ï¼Œå…¶ä¸­ N æ˜¯åºåˆ—é•¿åº¦ï¼Œ L æ˜¯å±‚çš„æ•°é‡ã€‚
- é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶ä¸‹çš„è‡ªè§£ç å™¨é‡‡ç”¨æ»‘åŠ¨çª—å£æ³¨æ„åŠ›æˆ–è€…é—¨æ§ä¿ç•™æœºåˆ¶ï¼Œèƒ½å¤Ÿå®ç°æ’å®šçš„å†…å­˜ä½¿ç”¨ï¼ˆå¤æ‚åº¦ä¸º O (C)ï¼Œå…¶ä¸­ C æ˜¯ä¸€ä¸ªå°çš„çª—å£å¤§å°ï¼‰ã€‚
