---
layout: post
title: 从第一性原理让深度学习搭载V8引擎
categories: [LLM]
tags: LLM
---

## 简介

> 1. **“临时拼凑的方法”**：用户在优化深度学习模型时，常常没有固定的策略，而是根据经验或直觉尝试各种方法。这种做法被比喻为“炼金术”，因为炼金术更多依赖于试验和运气，而不是严格的科学方法。
> 2. **“从第一性原理出发”**：这是一种更系统的方法，即从基本原理出发来分析问题。例如，在深度学习中，理解模型的训练过程、损失函数、过拟合和欠拟合等基本概念，可以帮助排除一些无效的尝试。

如果您想提高深度学习模型的性能。您如何处理这样的任务？通常，人们回到了可能以前有效或在推文上看到的技巧。

- Use in-place operations中文意思是 “使用原地操作”。是指在不创建新的对象的情况下，直接对现有对象进行修改的操作。
- Set gradients to None! 将梯度设置为无！
- 安装Pytorch 1.10.0但不是1.10.1！”

可以理解为什么用户经常采取这样一种特别的方法，在现代系统（特别是深度学习）上的表现常常既让人感觉像科学，又像炼金术。话虽如此，从第一性原理进行推理仍然可以排除大量的方法，从而使问题更容易处理。

例如，在一个数据集上通过深度学习获得良好的性能也涉及很多猜测。但是，如果你的训练损失远低于你的测试损失，那么你处于 “过拟合” 状态，并且如果你试图增加模型的容量，那就是在浪费时间。

>  模型容量越大，拟合能力越强：如果模型容量过大，它会倾向于记住训练数据中的每一个细节，而不是学习数据的通用规律。

或者，如果你的训练损失与验证损失完全相同，那么如果你试图对模型进行正则化，那就是在浪费时间。

同样，您可以理解深度学习制度的效率，包括3种不同的组成部分。

1. Compute：花费在GPU计算实际浮点操作（flops）上的时间
2. Memory：花费在GPU中传输张量的时间
3. Overhead: 其它

就像训练机器学习模型一样，知道自己处于什么状态可以让你专注于重要的优化。

- 例如，如果你把所有时间都花在进行内存传输上（也就是说你处于内存带宽受限的状态），那么增加你的 GPU 的浮点运算次数（FLOPS）将不会有帮助。

- 另一方面，总结来说，如果程序的瓶颈是计算密集型任务（如大规模矩阵乘法），那么优化程序逻辑（如将代码重写为C++）对性能的提升作用非常有限。在这种情况下，优化矩阵乘法算法或升级硬件才是更有效的解决方案。

所以，如果你想让你的 GPU 保持高速运转，让我们来讨论一下你的系统可能在花费时间的三个组成部分——计算、内存带宽和开销。

注意：本文大部分内容将以 GPU 和 PyTorch 为例（因为我供职于 PyTorch 团队），但这些原理几乎适用于所有硬件和框架。

## Compute

从一个优化深度学习系统的角度来看，我们希望尽可能多地处于计算受限的状态。你为所有这 312 teraflops 算力付了钱，理想情况下，你应该得到这 312 teraflops运算能力。但是，为了使您的钱从昂贵的矩阵乘法中获得价值，您需要减少其他部分所花费的时间。

但是，为什么要关注最大化计算而不是说内存带宽呢？

原因很简单 —— 你可以减少开销或内存成本，但是在不改变你正在执行的实际操作的情况下，你（大多数情况下）无法减少所需的计算量。加剧最大化计算利用率难度的是计算能力相对于内存带宽的增长速度。

一种思考计算的方式是把它当作一个工厂，我们向工厂发送指令（开销），向其提供原材料（内存带宽），所有这些都旨在让工厂高效运转（计算）。因此，如果我们的工厂提高效率的速度要比我们提供的材料的速度更快，那么我们工厂实现其高峰效率变得更加困难。

![img](/Users/didi/note_book/images/deepv8/factory_doubled.png)

除了意味着机器学习系统工程师的永久工作保障外，这种计算资源利用难度的增加也使得理解我们的瓶颈变得更加重要。

现代机器学习加速器都有**专门用于矩阵乘法的硬件**，例如英伟达的 “Tensor Cores”

![img](/Users/didi/note_book/images/deepv8/a100_specs.png)

所以，如果你不进行矩阵乘法运算，你将只能达到 19.5 teraflops (万亿次浮点运算)，而不是所说的 312 teraflops。请注意，这并非 GPU 所独有——事实上，TPU 比 GPU 更不通用。

因此看起来，图形处理器（GPUs）在除矩阵乘法之外的所有事情上都要慢得多，这一事实似乎很成问题 —— 那我们的其他运算操作呢，比如层归一化（layer norm）或激活函数（activation functions）？

好吧，事实是，这些运算符在 FLOPS 方面只是舍入误差。

例如，让我们看看这篇论文中关于 BERT 不同运算符类型的 FLOP 计数表，其中“Tensor Contraction”=矩阵乘法。

![img](/Users/didi/note_book/images/deepv8/bert_flops.png)

你可以看到，总体而言，我们的非矩阵乘法运算仅占我们 FLOPS 的 0.2%，因此我们的 GPU 计算非矩阵乘法运算的速度慢 15 倍并不重要。但是，在这种情况下，归一化和逐点运算实际上分别实现了比矩阵乘法少 250 倍和 700 倍的 FLOPS。那么，为什么我们的非矩阵乘法操作比它们应该花费的时间多这么多呢？

回到我们的类比，罪魁祸首通常是从工厂运输材料需要多长时间。换句话说，内存带宽。



## Bandwidth

> 带宽

带宽成本本质上是将数据从一个地方转移到另一个地方的成本。这可能是将数据从CPU转移到GPU，从一个节点转移到另一个节点，甚至从**CUDA全局内存到CUDA共享内存**。

尤其是最后一个是我们在这里关注的内容，通常被称为**“带宽成本”或“内存带宽成本”**。【"bandwidth cost" or "memory bandwidth cost".】其他两个（通常称为“数据传输成本”和“网络成本”）当然很重要，但是进行分布式性能会使我永远不会完成这篇文章。

为了理解内存带宽成本是什么，让我们回到我们的工厂类比。尽管我们的工厂是我们从事实际工作的地方，但它并不适合作为批量存储单元。很大一部分原因在于，因为我们在这里进行实际工作，所以所有的存储都针对实际使用的快速性进行了优化（**SRAM**），而不是拥有大量存储。

那么，我们在哪里存储实际的结果和材料？典型的方法是拥有一个仓库，可能的地方很便宜，我们有很多空间（**DRAM**）。然后，我们可以从我们的工厂（内存带宽）运送耗材。

> SRAM（静态随机存取存储器）和 DRAM（动态随机存取存储器）

<img src="/Users/didi/note_book/images/deepv8/factory_bandwidth.png" alt="img" style="zoom: 25%;" />

将数据移入和移出我们的计算单元的成本被称为‘内存带宽’成本。顺便说一下，你的 GPU 的 DRAM 会出现在 nvidia-smi 中，并且是导致你常见的‘CUDA 内存不足’错误的主要原因。

要注意的一件事是，每次我们执行GPU内核时，我们都需要将数据从**GPU的DRAM（即仓库）移至GPU的DRAM。**

现在，想象一下，当我们执行像`torch.cos`这样的一般操作时会发生什么。我们需要将数据从我们的存储位置运送到仓库，然后对每一条数据进行少量的计算，最后再把存储送回。周围的东西非常昂贵。结果，我们这里几乎所有的时间都花在附近运输数据，而*不是*在实际的计算本身上。

由于我们把所有时间都花在内存带宽上，这样的操作被称为**内存受限操作**，这意味着我们没有在计算上花费很多时间。好的，这不是理想的。我们该怎么办？我们看一下一系列运算符可能是什么样子。

![img](/Users/didi/note_book/images/deepv8/multi_operators.png)

为什么我们一遍又一遍地【将相同的数据发送到全局内存，然后再返回到计算单元】？我们应该将数据放在工厂，执行所有计算，然后将其发送回！

![img](/Users/didi/note_book/images/deepv8/operator_fusion.png)

**这是操作融合 —— 深度学习编译器中最重要的优化。**简单来说，我们不是将数据写入全局内存然后再读取它，而是通过一次执行多个计算来省略额外的内存访问。例如，如果我们执行`x.cos().cos()` ，通常需要执行4个全局读取和写入。

```python
x1 = x.cos() # Read from x in global memory, write to x1
x2 = x1.cos() # Read from x1 in global memory, write to x2
```

但是，借助运算符融合，我们只需要进行两次全局内存的读取和写入操作。所以运算符融合将使其速度提高 2 倍。

```python
x2 = x.cos().cos() # Read from x in global memory, write to x2
```

有一些需要注意的地方，这使得事情有点棘手

首先，GPU需要知道执行当前操作时接下来会发生什么。因此，在急切模式下你不能进行这种优化，在这种模式中，PyTorch 一次运行一个操作符。：其次，我们实际上需要为此生成 CUDA 代码，这带来了一系列全新的麻烦问题。

并非所有的操作符融合都像逐点操作符那样简单。你可以将逐点操作符融合到归约操作中，或者将逐点操作符融合到矩阵乘法中。甚至矩阵乘法本身也可以被视为将广播乘法与归约操作融合的结果。

如果你对编写自定义 CUDA 内核感兴趣，那么这很可能是你会看到最大收益的地方。任何两个 PyTorch 操作符都提供了融合的机会，从而节省了它们之间读取/写入全局内存的内存带宽成本。

此外，许多现有的编译器通常可以执行 “简单” 的融合 ——NVFuser 和 XLA 就是两个例子。然而，自动化系统无法与人的创造力相媲美，所以如果你想尝试自己编写一些自定义的 CUDA 内核，Triton 是一个很好的起点。

最后，算子融合会带来一些令人惊讶的结果。其一，融合后的 “x.cos ().cos ()” 所花费的时间几乎与单独调用 “x.cos ()” 完全相同。这就是为什么激活函数的成本几乎都一样，尽管 “gelu” 显然比 “relu” 包含更多的操作。

这一事实对重计算/激活检查点带来了有趣的影响。本质上，进行额外的重新计算可能会减少内存带宽的使用，从而减少运行时间。因此，我们可以通过重计算同时降低内存使用和运行时间，我们利用这一点在 AOTAutograd 中构建了一个很酷的最小割优化过程。

您可以[在此处](https://dev-discuss.pytorch.org/t/min-cut-optimal-recomputation-i-e-activation-checkpointing-with-aotautograd/467/1)阅读更多有关它的信息（也可能会在以后的博客文章中介绍！）



### Reasoning about Memory-Bandwidth Costs

当涉及到推断你的操作是否受内存带宽限制时，一个计算器会很有帮助

对于简单的操作符，直接推断你的内存带宽是可行的。例如，A100 有  terabytes/second 的全局内存带宽，并且可以执行19.5  teraflops/second 的计算。

因此，如果你正在使用 32 bit floats（即 4 个字节），那么在 GPU 能够执行 20 万亿次操作的相同时间内，你可以加载 4000 亿个数字。

此外，为了执行一个简单的一元运算符（比如将一个张量乘以 2），我们实际上需要将张量写回全局内存

所以…… 在你的一元运算符中执行大约一百次操作之前，你将花费更多的时间进行内存访问而不是实际计算。借助像 NVFuser 这样的融合编译器，实际上我们自己来测量这个是相当容易的！您可以[在此处](https://colab.research.google.com/drive/1hEtorT5y9mcXHR0gpensD7oZfuyyxtu7?usp=sharing)查看Colab中的代码。

如果你写一个像这样的 PyTorch 函数

```python
import torch
import time

values = []
def calc_flops(func, sz, loop_iters):
  # sz 每次多少个浮点数
  # loop_iters 进行了多少次浮点运算
  inputs = [torch.randn(sz, device='cuda')]
  for _ in range(5):
    func(*inputs)
  torch.cuda.synchronize()
  iters = 100
  begin = time.time()
  for _ in range(iters):
    func(*inputs)
  torch.cuda.synchronize()
  t = (time.time()-begin)

  iters_per_s = iters/t # 每秒迭代的次数

  flops = sz * loop_iters # 计算一次有多少浮点数运算
  bytes_per_elem = 4 # 每个浮点数有多少字节
  num_reads = 1 # 多少次读
  num_writes = 1 # 多少次写
  mem_bw = sz * (num_reads + num_writes) * bytes_per_elem # 计算一次需要多少内存传输
  print(f"{loop_iters} ops")
  print(f"itr time: {t *1e6 / iters}")
  print(f"FLOPS: {iters_per_s * flops/1e12:.2f} TF/s")
  print(f"Mem B/W: {iters_per_s * mem_bw/1e9:.2f} GB/s")
  print()
  values.append((t / iters, iters_per_s * flops, iters_per_s * mem_bw))

for iters in [2**i for i in range(10)]:
  def f(x):
    for _ in range(iters):
      x = x * 2
    return x

  sz = 2**24
  f = torch.jit.trace(f, (torch.randn(sz, device='cuda')))
  with torch.jit.fuser("fuser2"):
    calc_flops(f, sz, iters)
  # print(values)
```

并且使用融合编译器进行基准测试，我们随后可以计算出不同重复值所达到的浮点运算次数（FLOPS）和内存带宽。增加重复次数是一种在不增加内存访问的情况下增加计算量的简单方法——这也就是所谓的提高计算强度。

具体来说，假设我们对这段代码进行基准测试，并找出每秒执行的迭代次数。那么，作为张量大小$$N$$的函数，我们将执行$$2\times N$$次内存访问和$$N\times\text{repeat}$$次浮点运算（FLOP）。因此，所达到的内存带宽将是$$\text{bytes\_per\_elem}\times 2\times N\times\text{itrs\_per\_second}$$，而达到的 FLOPS 将是$$N\times\text{repeat}\times\text{itrs\_per\_second}$$。现在，让我们根据计算强度绘制所实现的运行时间、浮点运算次数和内存带宽。

![img](/Users/didi/note_book/images/deepv8/microbench.png)

首先，请注意，直到我们执行64个乘法之前，运行时*根本*不会明显增加。这意味着在那一点之前，我们主要受内存带宽的限制 —— 我们的计算大部分时间处于闲置状态。

因此，我们一开始仅达到了微不足道的 0.2 万亿次浮点运算。当我们加倍计算强度时，该数字会线性增长，直到我们接近9.75 teraflops的峰值.

一旦我们靠近峰值teraflops，我们就被认为是“计算瓶颈”。

最后，你可以看到我们所实现的内存带宽一开始接近峰值，并且随着我们增加计算强度，它开始下降。这正是我们应该期望的，因为我们花了越来越多的时间执行实际的计算而不是访问内存。在这种情况下，很容易看出我们什么时候受计算限制，什么时候受内存限制。

- 在重复次数小于 32 时，我们的内存带宽达到饱和状态，而我们的计算能力未得到充分利用。
- 相反，一旦重复次数大于 64，我们就会看到我们的计算达到饱和状态（即接近峰值浮点运算次数），而我们使用的内存带宽开始下降。

对于较大的系统来说，通常更难确定是受计算限制还是受内存带宽限制，这通常是因为它们包含了计算密集型和内存密集型的混合组件。

一种衡量计算受限程度的常见方法是测量你**所达到的浮点运算次数（FLOPS）占峰值浮点运算次数的百分比。**

例如，如果你达到了峰值浮点运算次数（FLOPS）的 80%，那么你就知道你至少有 80% 是受计算限制的，这是相当不错的剩下的时间可能都花在了内存带宽操作上。

但是，除了记忆带宽成本外，还有一件事可能会导致您的GPU不去BRRRRR。



## Overhead 

> 开销

Overhead 是指代码花费时间在既不是传输张量也不是进行计算的任何事情上。

- 例如，
- 在Python解释器中花费的时间？
  - Overhead. 
- 在Pytorch框架上花费的时间？
  - Overhead.
- 启动CUDA内核花费的时间（但不执行它们）？
  - Also... overhead. 

开销是一个如此有害的问题的主要原因是现代图形处理器（GPUs）确实非常快。一个 A100 可以每秒执行 312 万亿次浮点运算（312 TeraFLOPS）。相比之下，Python 真的非常慢。在本地进行基准测试，Python 在一秒钟内可以执行 3200 万次加法运算。这意味着在 Python 执行一次浮点运算（FLOP）的时间里，一块 A100 芯片可能已经处理了 975 万次浮点运算。

更糟糕的是，Python解释器甚至不是开销的唯一来源——像PyTorch这样的框架在到达实际内核之前也有许多层的调度。如果你用PyTorch做同样的实验，我们每秒只能得到28万次操作。

当然，小张量并不是PyTorch的用武之地，但……如果你使用小张量（例如在科学计算中），你可能会发现与C++相比，PyTorch慢得令人难以置信。

例如，看一下 PyTorch 执行一次加法运算的火焰图（flamegraph profile）。那里的那个框吗？那是正在执行实际计算的部分。其他的一切都是纯粹的开销。

![img](/Users/didi/note_book/images/deepv8/flamegraph.png)

鉴于此，你可能会惊讶于还有人使用 PyTorch，但请记住，现代深度学习模型通常执行的是大规模的操作。

此外，像 PyTorch 这样的框架是异步执行的。

也就是说，在 PyTorch 运行一个 CUDA 内核时，它可以继续在后面排队更多 CUDA 内核。因此，只要 PyTorch 能够“领先于”CUDA 内核运行，大部分框架开销就会被完全隐藏！

![image-20250221170232849](/Users/didi/note_book/images/deepv8/image-20250221170232849.png)

- 如果我们的 GPU 运算器足够大，那么我们的 CPU 可以在 GPU 之前运行（因此 CPU 的开销就无关紧要了）

- 另一方面，如果我们的 GPU 运算单元太小，那么我们的 GPU 大部分时间将成为一个昂贵的镇纸。

那么，如何判断你是否处于这种状态呢？

由于Overhead 通常不会随着问题规模的增加而增加（而计算和内存会），最简单的方法是直接增加数据的大小。如果这样并没有使运行时间按比例增加，那么你就是受到开销限制的。例如，如果你将批量大小翻倍，但运行时间只增加了 10%，那么你很可能是受到开销限制的。

另一种方法是使用 PyTorch 分析器。在这里，粉色的线条实际上显示了 CPU 内核与 GPU 内核是如何对应的。

![img](/Users/didi/note_book/images/deepv8/overhead_tracer.png)

在 GPU 等待 CPU 开销时，GPU 上有很多间隙。

![img](/Users/didi/note_book/images/deepv8/no_overhead.png)

*Our CPU runs wayyy ahead of the GPU*

另外一点 —— 在 nvidia-smi 中的 “GPU-Util”（不是 “Volatile GPU-Util”）条目基本上是在测量底行中实际运行 GPU 内核的百分比是多少。因此，这是浏览Overhead 的另一种好方法。

这种Overhead 存在的主要原因是像 PyTorch 这样的框架具有高度的灵活性。本质上，需要花费大量时间来“弄清楚要做什么”。

这可能来自 Python（查找属性或分派到正确的函数）或 PyTorch 中的代码（PyTorch 的整个分派器）。例如，当你执行`a + b`时，需要发生以下步骤：

- Python 需要查找`a`上的`__add__`分派到什么。
- PyTorch 需要确定张量的许多属性（例如数据类型、设备以及是否需要自动梯度）来确定调用哪个内核。
- PyTorch 需要实际启动内核。

从根本上说，这种开销来自于在每一步都能做不同的事情的灵活性。如果你不需要这种灵活性，解决这种灵活性的一种方法是通过追踪将其固定下来，例如使用`jit.trace`、FX 或`jax.jit`。或者，你也可以在更低的级别上使用类似 CUDA Graphs 的东西来实现。

不幸的是，这会以牺牲灵活性为代价。我非常期待的一种可能让我们两全其美的方法是，在虚拟机级别进行内省，编写更接近“真正的”即时编译器（JIT）。关于这一点，可以参考 TorchDynamo。



## Conclusion

如果你想加快你的深度学习系统的速度，最重要的是要了解你的模型的瓶颈在哪里。这个瓶颈决定了加快你的系统速度的合适方法。通常，我看到研究人员和其他想加快他们的 PyTorch 代码的人在不了解他们处于什么状态的情况下盲目尝试。

| Performance Regime | Plausible Solutions                                          |
| ------------------ | ------------------------------------------------------------ |
| Overhead-Bound     | Tracing, Operator Fusion, don't use Python, a *real* JIT :^) |
| Bandwidth-Bound    | Operator Fusion 操作融合                                     |
| Compute-Bound      | Use Tensor Cores, give Nvidia more money 加钱                |

当然，可以说，用户需要考虑这些事情本身就反映了框架的不足。
PyTorch 的编译器或配置文件 API 并不总是……最容易使用，尽管这一直是关注的重点。

不管怎样，我发现了解系统的基本原理几乎总是有用的——希望这对你也有帮助。
