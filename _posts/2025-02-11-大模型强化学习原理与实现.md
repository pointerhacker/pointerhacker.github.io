---
layout: post
title: 大模型强化学习原理与实现
categories: [LLM,RL]
tags: RL
---
## 什么是SFT

通过利用预训练期间捕获的一般语言理解。并通过监督学习将其调整为目标任务，微调桥梁这一差距。

具体来说，对于大型语言模型而言，微调是至关重要的，因为使用全部数据进行重新训练在计算上是不可行的。

SFT指的是利用有标注的数据将一个预先训练好的语言模型（Language Model，简称 LLM）调整适应到特定的下游任务中

在有监督的微调中，微调数据是从一组预先经过验证的响应中收集而来的。这是与**无监督技术的**主要区别，其中数据未经验证。

- 如上所述，获得更好的答案，匹配您的业务*准则*。
- 提供新的特定 /私人数据，这些数据在培训步骤中不公开可用，以便LLM模型适合您的特定*知识库*。
- 教LLM回答新的（看不见的）*提示*；

### Trainer VS SFTTrainer

由于有监督的填充是*人类反馈（RLHF）的加强学习*的第一步，开发人员发现了独立一个类的必要性，同时增加了一些功能，如果仅使用`Trainer`则需要手动执行

因为如果您检查[源代码](https://github.com/lvwerra/trl/blob/main/trl/trainer/sft_trainer.py)，则`SFTTrainer`类从`Trainer`功能继承。

“然而，SFTTrainer类添加了一些功能，使其在处理大型语言模型（LLM）时的训练变得更加方便。”

- Support of `peft` lora Qlora
- Batch `packing`



### RLHF 几个重要步骤

1、定义指南是为了在决定输入的好答案和坏答案时保证独特的标准。

2、应该训练一个奖励模型（RM），它将根据准确性、相关性和对指南的遵守情况评估每个响应。

3、为了训练 RM，选择一些提示并发送给人工审阅者。我们称之为偏好数据（PD）

4、然后，审阅者与模型交互并手动评估和评估相应的输出。

5、收集的反馈以评级或排名的形式用于训练RM。

6、通过训练 RM，我们可以训练策略优化器，这是指导 LLM 微调的必需组件。

7、我们使用策略优化微调LLM。

8、这种迭代反馈循环允许模型逐渐从人类指导中学习并相应地改进其行为。

在这个总结中，描述了一些用于训练和评估语言模型的重要工件。这些工件包括：

1. **注释指南（Annotation Guidelines）**：这份指南用来定义什么是好的答案，什么是坏的答案。通过详细的标准和说明，确保人类标注者（Human Labellers）在为数据做出判断时有一致的理解和参考。

2. **偏好数据（Preference Data）**：这是在选择用于标注者处理的数据时使用的输入集合。偏好数据帮助引导标注者挑选合适的提示（prompts）来生成参考答案。

3. **生成评估输出（Generating the outputs for evaluators）**：这是指为评估人员生成的模型输出。评估人员使用这些输出来判断模型的性能，并根据标准评估这些输出是否符合预期。

4. **奖励模型（Reward Model, RM）**：这是一种用来优化模型行为的工具。奖励模型基于评估数据来训练，帮助模型学习如何生成更高质量的答案，符合注释指南定义的优良标准。

5. **优化策略（Policy, optimized with the data from the RM）**：通过使用从奖励模型获取的数据优化的一种策略。这一策略算法指导模型如何在给定的输入下生成最优的输出，最大化符合奖励模型标准的答案。

简言之，这些工件是一个系统中相互协作的要素，用来提升语言模型的表现质量。通过定义明确的标准、收集和评估数据，训练以及优化模型，系统能够持续改进生成答案的准确性和可靠性。

偏好数据的构建方式

1、例如，为了选择最佳提示，您可以拥有一组预定义的提示，拥有一个预定义的模板并即时生成一些提示，或者将/那些与数据库中的随机提示相结合。

2、对于选择答案，你可以将提示发送到特定版本的模型（你最后微调的那个）或者不同的检查点。

3、决定你想要对多少个答案进行排序：你可以只使用 2 个答案，并采用最佳 - 最差的二元排序方案，或者让标注员从 1 到 5 进行排序，例如。

4、始终首先为不同的排名级别定义注释指南，以尽量减少个人解释并使答案标准化



### 训练数据总结

**指令数据**用于微调步骤，并由提示和预期答案组成，旨在改善或纠正方式LLM根据提示生成文本。

**偏好数据**用来训练策略，以使模型区分，鉴于提示的n可接受答案比其他提示更好。



![Optimizing a Policy using TRL](http://pointerhacker.github.io/imgs/posts/mysft/part-2-policy.png)



## RLAIF

奖励数据有大模型生成

'''蒸馏或直接RLAIF
在通过LLM标注偏好之后，在典型的RLAIF设置中，会在获得的软标签（数值，从0到1）上训练一个奖励模型。
然后，对奖励分数的softmax应用交叉熵损失。
softmax将奖励模型输出的分数转换为概率分布。
最后，使用强化学习来训练RLAIF策略模型，利用奖励模型为模型响应分配奖励。

然而，Lee等人研究了一种替代方法，即直接使用LLM反馈作为奖励。
LLM被提示对生成的质量进行评分，评分范围为1到10。
在提示中引入了评分的维度，以便LLM能够做出更好的判断。
然后，计算每个分数（从1到10）的可能性，并将其归一化为概率分布，随后用于计算加权分数，该分数再次被归一化到-1到1的范围内。
这个分数可以直接用作奖励。'''



## Self-Play fIne-tuNing (SPIN)

*我们可以增强LLM在不获取人类通知数据的情况下进行改善？*

该游戏涉及几个步骤，通过这些步骤，训练后的语言模型（SFT LLM）逐步改进。流程如下：

1. **选择模型和数据集**：游戏开始时需要选择一个已经进行过特定任务微调（SFT）的语言模型，以及与此模型相关联的问答训练数据集。
2. **生成新的合成答案**：在第一轮中，使用这个已经微调的模型对数据集中的问题生成替代答案。以后每一轮将使用最新微调后的模型，以期其性能能达到最优水平。
   - 为了防止生成的新模型偏离上一版本过多，保持稳定性，会使用Kullback-Leibler（KL）正则化项进行约束。
3. **创建新的训练数据集**：创建一个新的数据集，其中包含原答案（被接受的答案）和新生成的合成答案（被拒绝的答案）对。
4. **在新的训练数据集上微调模型**：在新创建的数据集上进一步微调模型。微调目标是通过区别生成的（机器合成的）和人类提供的答案来改进模型的响应策略。
   - 这个区别的评估通过逻辑损失来进行，逻辑损失能够防止函数绝对值过大的情况。



这个游戏的结束标志即是模型无法再区分前一版本生成的响应和人类生成的响应。当达到这一点时，游戏中的主要角色（主要模型）和对手（对手模型）都已达到一个高水平：

1. **主要角色变得更擅长识别由人类生成的响应**：通过每一次迭代，主要模型在识别哪些回答是由人类而非机器生成方面逐渐提高了能力。

2. **对手模型变得更擅长生成类似人类的响应**：同时，对手模型也在不断改进，越来越擅长生成看起来像是由人类编写的答案。

**游戏结束条件**:

- **无法区分**：当最先进版本的主要模型无法再区分其前一版本生成的响应和人类生成的响应，游戏就结束了。这说明模型的生成结果已经达到了一个和人类数据无法区分的高水平。
- **概率分布收敛**：此时，生成数据的概率分布与人类数据的概率分布收敛，意味着模型生成的响应在统计上和目标数据没有区别，从而保证了更高的质量。

换句话说，当模型生成的回答完全无法区分开来，且在质量上达到与人类回答相同的水平，这个“游戏”也就达到了其设定的终点。



SPIN（Self-Play Incremental Learning）和DPO（Direct Preference Optimization）是两种用于改进语言模型（LLM）的方法，它们各有特点和适用场景。以下是二者的主要相似点与区别：

### 共同点：

1. **目标**：两者的目标都是通过某种方式细化和提升语言模型的性能，使生成的回答更贴近人类水平。

### 主要差异：

1. **数据需求**：
   - **SPIN**：仅依赖SFT（Supervised Fine-Tuning，监督微调）数据集，这使得它成为一种成本更低的方法，因为它不需要额外的偏好数据。
   - **DPO**：需要额外的偏好数据，这些数据通常要通过人工标注或者其他方式收集，因而在数据准备上成本更高。

2. **适用领域**：
   - **SPIN**：专为SFT方法量身定制，通过区分目标数据分布和较弱模型生成的数据分布来逐步改进模型。
   - **DPO**：设计用于强化学习（Reinforcement Learning，RL）下的微调，通过优化模型直接对用户偏好进行学习。

3. **实例水平的响应选择**：
   - **DPO**：在实例水平上，选择的响应需要被认为是更好的，即需要明确区分出优劣。
   - **SPIN**：关注于在分布层次上区分目标数据分布和较弱LLM生成的数据分布，然后逐步提高LLM的性能。

4. **迭代策略**：
   - **DPO**：采用单次迭代的方法，即通过一次训练使模型优化达到目标。
   - **SPIN**：采用迭代的自对弈策略（iterative self-play strategy），模型在几个轮次的训练中逐步改进。

### 总结：

- 如果需要一种成本较低、不需要额外偏好数据的方法来提升模型性能，**SPIN**可能更合适，因为它只需要现有的SFT数据。
- 如果有特定的偏好数据，并且希望通过强化学习直接优化模型以满足用户偏好，**DPO**可能是更理想的选择，尽管它在数据准备上成本更高。

通过理解这些差异和应用场景，选择适合的策略可以更有效地实现模型改进。





## Identity Preference Optimization (IPO)

**总结**

IPO是DeepMind团队针对RLHF和DPO的局限性提出的新方法，通过改进目标函数和算法设计，直接优化模型与数据的对齐，减少对成对偏好假设的依赖，旨在提升数据效率和泛化能力。其理论框架为后续研究提供了基础，而实际集成至TRL库则加速了应用落地。

关键点：ΨPO 和 IPO

一方面，研究人员发现RLHF（基于人类反馈的强化学习）和DPO（偏好优化决策）目标可以统一为一个更广义的目标，称为ΨPO，这为偏好学习提供了更广阔的理论基础。
通过对ΨPO的分析，他们指出了一些问题，比如弱正则化和过拟合的潜在风险。

通常，RLHF和DPO会使用KL正则化来确保大型语言模型（LLM）在每一步训练中逐渐改进，避免与原始的、非对齐的模型产生显著偏差。
然而，他们注意到一个问题：随着模型预测变得更确定（即更可预测），这种形式的正则化变得不那么有效。
理想情况下，正则化应区分小幅度和大幅度的改进，鼓励模型在不确定性较大的地方进行显著改进，而不是在模型已经自信的地方进行过度微调。
然而，在目前的场景中，正则化没有充分区分这些差异，导致学习过程缺乏细致入微性。

举例来说，这就像一个老师在教孩子阅读。
一开始，老师会进行小幅度的纠正（KL正则化）以确保学生在不养成坏习惯的情况下逐步提高。
然而，当他的阅读技能提高并变得更确定时，如果指导仍然保持同样的水平，那无论是孩子将“cat”和“cap”搞混还是将“butterfly”和“ball”搞混，纠正力度都是一样的。

另一方面，与传统方法可能使用提前停止这样的技巧进行正则化不同，他们引入了一种ΨPO的特例：Identity-PO（身份偏好优化）。
IPO在无需依赖奖励模型的情况下优化偏好，并在偏好确定的场景中保证KL正则化的有效性。

通过用恒等函数替代对数函数，IPO直接优化偏好（从对成对偏好学习而不是对数偏好学习）。
它比较偏好动作和不偏好动作与一个参考策略的差异，并结合一个正则化项来有效管理正则化。
这种正则化机制使IPO在适应训练数据和保持泛化能力之间取得平衡，这对于模型在未见数据上的表现至关重要。

继续之前的例子，如果孩子最初更喜欢阅读简单的单词而不是复杂的单词，通过使用恒等函数，奖励将是比例的和直接的（他们会得到贴纸而不是数值分数）。
指导方针（参考策略）将表明这种偏好，但正则化项将奖励他阅读简单单词，并鼓励他偶尔挑战复杂单词。

要了解更为理论化的方法和数学证明，请参阅原始论文。

## PPO原理

https://fse.studenttheses.ub.rug.nl/25709/1/mAI_2021_BickD.pdf

## PPO 实现

### 奖励模型

#### 模型结构

```python
class AutoModelForCausalLMWithRewardHead(nn.Module):
    def __init__(self, lm_backbone):
        super().__init__()
        self.lm_backbone = lm_backbone
        self.scalar_head = layer_init(
            nn.Linear(lm_backbone.config.hidden_size, 1),
            std=1 / np.sqrt(lm_backbone.config.hidden_size + 1),
        )
        self.reward_gain = torch.nn.Parameter(torch.tensor(1.0), requires_grad=True)
        self.reward_bias = torch.nn.Parameter(torch.tensor(0.0), requires_grad=True)

    def forward(self, **kwargs):
        output = self.lm_backbone(**kwargs)
        reward_latents = output.hidden_states[-1]
        # shape: [batch_size, length, hidden_size]
        last_reward_latents = reward_latents[:, -1, :]
        # shape: [batch_size, hidden_size]
        reward = self.scalar_head(last_reward_latents)
        # shape: [batch_size, 1]
        reward = self.reward_gain * reward + self.reward_bias
        return output, reward

      
 class AutoModelForCausalLMWithScalarHead(nn.Module):
    def __init__(self, lm_backbone):
        super().__init__()
        self.lm_backbone = lm_backbone
        self.scalar_head = layer_init(nn.Linear(lm_backbone.config.hidden_size, 1), std=0)

    def forward(self, **kwargs):
        # [batch_size, length, hidden_size]
        output = self.lm_backbone(**kwargs)
        # [batch,seq_len,1]
        return output, self.scalar_head(output.hidden_states[-1])
      
```

### 数据集

```python
query_token = tokenizer('text')["input_ids"]


Dataset({
    features: ['query', 'sample0', 'sample1', 'sample2', 'sample3', 'best'],
    num_rows: 6264
})
```



### 训练

- 奖励模型

```python
### step1 ##normalize
1.1 generate
queries = query_token
output = llm.generate(queries)
query_responses = torch.cat((queries, output.sequences[:, context_length:]), dim=1)

1.2 get_reward
for b in data:
	reward = reward_model(query_responses)
	rewards.append(reward)

mean, std = rewards.mean(), rewards.std()
target_mean, target_std = torch.tensor(0.0, device=device), torch.tensor(1.0, device=device)
gain = target_std / std
bias = target_mean - gain * mean

#这段代码的作用是计算奖励标准化的增益（gain）和偏差（bias），将其应用到模型中以调整奖励的分布，使其更适合后续的训练步骤
reward_model.reward_gain.data = gain
reward_model.reward_bias.data = bias


### step2 ##train
#奖励模型只训练一个 epcho 单一的 epcho 中，学习率会退火至零
lr = (1 - start / args.labels.num_train) * args.lr

mb_query = torch.from_numpy(np.stack(mb_data["query"]))
mb_best = torch.from_numpy(np.stack(mb_data["best"])).to(device)
mb_responses = [
	torch.from_numpy(np.stack(mb_data[f"sample{i}"])) for i in range(args.labels.num_labels)
]

for i in range(args.labels.num_labels):
    query_responses = torch.cat([mb_query, mb_responses[i]], dim=1)
    reward = get_reward(reward_model, query_responses, tokenizer)[1]
    predicted_rewards.append(reward.view(-1))
    predicted_rewards = torch.stack(
        predicted_rewards, dim=1
    )  # shape (batch_size, num_labels), basically a reward prediction for each label
    accuracy = (predicted_rewards.argmax(1) == mb_best).float().mean()
    loss = torch.nn.functional.cross_entropy(predicted_rewards, mb_best)

```

- 策略模型

```python
### step1 ##generate
output = llm.generate(input_ids=input_ids)
query_responses = torch.cat((queries, output.sequences[:, context_length:]), dim=1)
# 上下文长度
context_length = queries.shape[1]
responses = query_responses[:, context_length:]

### step2 ##价值生成 forward
	
  # 训练策略的 输出的对应标签token的概率 
  #[batch,seq_len,dim] #[batch,seq]
  output, full_values = policy(input_ids=input_ids,) # 原始输出和每一个token多value
  values = full_values[:, context_length - 1 : -1].squeeze(-1) # 生成每一个token多价值
  logits = output.logits[:, context_length - 1 : -1] # 原始生成的输出
  logprobs = torch.gather(logits, 2, responses.unsqueeze(-1)).squeeze(-1) #输出的对应标签token的概率 [batch,seq]
  
  #参考策略 输出的对应标签token的概率 
  ref_output, _ = forward(ref_policy, query_responses, tokenizer)
  ref_logits = ref_output.logits[:, context_length - 1 : -1]
  ref_logits /= args.task.temperature
  ref_all_logprobs = F.log_softmax(ref_logits, dim=-1)
  ref_logprobs = torch.gather(ref_all_logprobs, 2, responses.unsqueeze(-1)).squeeze(-1)


### step3 ##价值生成 forward
3.1 从给定位置（truncate_after）后的第一个出现的截断标记（truncate_token）开始进行截断。[在生成相应后的16-24toekn出现。后截断]
postprocessed_responses = torch.where(
    truncate_mask,
    torch.full_like(responses, tokenizer.pad_token_id),
    responses,
)

3.2 获取生成对应的奖励
scores = get_reward(reward_model, postprocessed_query_responses, tokenizer)[1].flatten()

3.3 过滤奖励
# 对响应进行过滤。确保样本中包含截断标记（truncate_token）。
            # 未通过该过滤器的响应将获得较低的（固定的）分数。
            # 仅对通过该过滤器的响应向人类进行查询。
scores = torch.where(
                filter_mask,
                scores,
                torch.full_like(scores, args.task.penalty_reward_value),
            )


3.4 计算真实的奖励
# batch,seq_len
kl = logprobs - ref_logprobs
non_score_reward = -kl_ctl.value * kl
rewards = non_score_reward.clone()
rewards[:, -1] += scores

3.5 # 计算优势和回报
for t in reversed(range(gen_length)):
        #[batch,1]
        nextvalues = values[:, t + 1] if t < gen_length - 1 else 0.0
        # td-error
        delta = rewards[:, t] + args.ppo.gamma * nextvalues - values[:, t]
        # lastgaelam是从下一时间步传递过来的优势估计.
        # 这是GAE的核心更新步骤。当前时间步的TD-Error（delta）加上带有衰减因子的先前GAE值。
        # args.ppo.gamma是折扣因子，args.ppo.lam是GAE中的平滑因子（又称lambda）。
        lastgaelam = delta + args.ppo.gamma * args.ppo.lam * lastgaelam
        # advantages_reversed[0].shape = batch,1
        advantages_reversed.append(lastgaelam)
# batch seq_len
advantages = torch.stack(advantages_reversed[::-1], axis=1) # 反转
returns = advantages + values

return_mean, return_var = returns.mean(), returns.var()
value_mean, value_var = values.mean(), values.var()

### step3 ##优化测策略
for epoch in range(4):
    batch_inds = np.random.permutation(batch_size)
    print("epoch:", epoch, "batch_inds:", batch_inds)
    for mini_batch_start in range(0, batch_size, mini_batch_size):
        mini_batch_end = mini_batch_start + mini_batch_size
        mini_batch_inds = batch_inds[mini_batch_start:mini_batch_end]
        
        # `optimizer.zero_grad()` set optimizer to zero for gradient accumulation
        for micro_batch_start in range(0, mini_batch_size, micro_batch_size):
            micro_batch_end = micro_batch_start + micro_batch_size 
            micro_batch_inds = mini_batch_inds[micro_batch_start:micro_batch_end]
            print("____⏩ a forward pass on", data[micro_batch_inds])
        # `optimizer.step()`
        print("⏪ a backward pass on", data[mini_batch_inds])
# 在每个micro_batc中的训练细节 见👇
```

- 优化策略

```python
### step1 ##拿到micro_batch的训练数据
mb_return = returns[micro_batch_inds]
mb_advantage = advantages[micro_batch_inds]
mb_values = values[micro_batch_inds]
mb_responses = responses[micro_batch_inds]
mb_query_responses = query_responses[micro_batch_inds]
mb_logprobs = logprobs[micro_batch_inds]

### step2 ##当前策略的输出
output, vpred_temp = forward(policy, mb_query_responses, tokenizer)
logits = output.logits[:, context_length - 1 : -1]
logits /= args.task.temperature
new_all_logprobs = F.log_softmax(logits, dim=-1)
new_logprobs = torch.gather(new_all_logprobs, 2, mb_responses.unsqueeze(-1)).squeeze(-1)
vpred = vpred_temp[:, context_length - 1 : -1].squeeze(-1)


### step3 ##价值函数损失
# 训练过程中输出的values剪切在原始的values附近
vpredclipped = torch.clamp(
    vpred,
    mb_values - args.ppo.cliprange_value,
    mb_values + args.ppo.cliprange_value,
)
# vf_losses1：使用原始预测值 vpred 和目标值 mb_return 之间的误差平方（均方误差）。
vf_losses1 = torch.square(vpred - mb_return)
# vf_losses2：使用裁剪后的预测值 vpredclipped 和目标值 mb_return 之间的误差平方（均方误差）。
vf_losses2 = torch.square(vpredclipped - mb_return)
vf_loss = 0.5 * torch.max(vf_losses1, vf_losses2).mean()
# 最终的值函数损失 vf_loss 是 vf_losses1 和 vf_losses2 两者中较大的一个，然后乘以 0.5 后求平均
vf_clipfrac = (vf_losses2 > vf_losses1).float().mean()


### step3 ##策略函数损失
# clip(推理的置信度的差距) x 价值优势
logprobs_diff = new_logprobs - mb_logprobs
ratio = torch.exp(logprobs_diff)
pg_losses = -mb_advantage * ratio
pg_losses2 = -mb_advantage * torch.clamp(ratio, 1.0 - args.ppo.cliprange, 1.0 + args.ppo.cliprange)
pg_loss = torch.max(pg_losses, pg_losses2).mean()

pg_clipfrac = (pg_losses2 > pg_losses).float().mean()


### step4 ##最终损失
loss = pg_loss + args.ppo.vf_coef * vf_loss
```





## DPO实现



### 数据集

 ```python
return {
        "prompt_input_ids": prompt_input_ids,
        "chosen_input_ids": chosen_input_ids,
        "rejected_input_ids": rejected_input_ids,
    }
 ```



### 推理

```python
compute_ref_log_probs -> concatenated_forward


### step1 :
Concatenate the `chosen` and `rejected` inputs from the batch into a single tensor for both the prompt and completion sequences.
shape: [2 * batch_size, prompt_length]
- output["prompt_input_ids"] = torch.cat([batch["prompt_input_ids"], batch["prompt_input_ids"]], dim=0)
shape: [2 * batch_size, max_completion_length]
- output["completion_input_ids"] = torch.cat(
            (
                pad_to_length(batch["chosen_input_ids"], max_completion_length, pad_value=padding_value),
                pad_to_length(batch["rejected_input_ids"], max_completion_length, pad_value=padding_value),
            ),
        )


### step2
shape: [2 * batch_size, prompt_length + max_completion_length]
input_ids = torch.cat((prompt_input_ids, completion_input_ids), dim=1)


### step3
outputs = model(input_ids, **model_kwargs)


### step4
labels = torch.roll(input_ids, shifts=-1, dims=1)
per_token_logps = torch.gather(logits.log_softmax(-1), dim=2, index=labels.unsqueeze(2)).squeeze(2)
per_token_logps = torch.roll(per_token_logps, shifts=1, dims=1)


### step5
all_logps = per_token_logps.sum(-1)
mean_chosen_logits = logits[:num_examples][loss_mask[:num_examples]].mean()
mean_rejected_logits = logits[num_examples:][loss_mask[num_examples:]].mean()


output["mean_chosen_logits"] = mean_chosen_logits
output["mean_rejected_logits"] = mean_rejected_logits
```

 

```python
compute_loss -> get_batch_loss_metrics


### step1
output["chosen_logps"] = all_logps[:num_examples]
output["rejected_logps"] = all_logps[num_examples:]
output["mean_chosen_logits"] = mean_chosen_logits
output["mean_rejected_logits"] = mean_rejected_logits
ref_chosen_logps
ref_rejected_logps


### step2


#### step2.1
get_batch_loss_metrics -> dpo_loss
chosen_logratios = chosen_logps - ref_chosen_logps
rejected_logratios = rejected_logps - ref_rejected_logps


#### step2.1
logratios = chosen_logps - rejected_logps
ref_logratios = ref_chosen_logps - ref_rejected_logps
logits = logratios - ref_logratios
```

 

### 损失

```python
"""
对于正样本(标签=1):


    -F.logsigmoid(self.beta * logits) * (1 - self.label_smoothing)


    当 logits 值大(预测正确)时,sigmoid(beta * logits) 接近 1
    此时 logsigmoid(beta * logits) 接近 0
    最终损失值很小，说明预测准确




对于负样本(标签=0):


    -F.logsigmoid(-self.beta * logits) * self.label_smoothing


    注意这里是 -self.beta * logits,负号很关键
    当 logits 值小(预测正确)时,sigmoid(-beta * logits) 接近 1
    此时 logsigmoid(-beta * logits) 接近 0
    最终损失值很小，说明预测准确


总结：
当模型输出更好的回答时(正样本),最小化-F.logsigmoid(self.beta * logits)
当模型输出较差的回答时(负样本),最小化-F.logsigmoid(-self.beta * logits)


"""
if self.loss_type == "sigmoid":
  losses = (
      -F.logsigmoid(self.beta * logits) * (1 - self.label_smoothing)
      - F.logsigmoid(-self.beta * logits) * self.label_smoothing
  )
```

 

## KTO

在这个系列的这篇文章中，我们将深入探讨 KTO，这是一种通过前景理论来对齐大语言模型的新方法。

### Prospect Theory 前景理论

这段话的核心是描述“前景理论”（Prospect Theory）中的一个关键发现，即“损失厌恶”（Loss Aversion）。前景理论是由心理学家丹尼尔·卡尼曼（Daniel Kahneman）和阿莫斯·特维尔斯基（Amos Tversky）提出的，用于描述人们在面临风险和不确定性时的决策行为。

> 根据这段话，损失厌恶是指人们对损失的情绪反应要比同等收益的情绪反应强烈得多。换句话说，失去100元给人带来的痛苦通常比得到100元带来的快乐更为强烈。
>
> 这种损失厌恶会导致人们在做选择时更倾向于选那些可能损失较少的选项。简而言之，人们会变得风险厌恶（risk averse），更加倾向于避免损失而不是追求潜在收益。
>
> 例如，如果给你两个选择：
>
> 1. 确定获得50元。
> 2. 有50%的机会获得100元，但另50%的机会什么都得不到



### HALOs and KTO

> 在损失函数中对人类偏见进行建模

流行的对齐方法，如近端策略优化（PPO）和直接偏好优化（DPO），对前景理论所描述的人类偏见进行建模。因此，它们可以被定义为**具有人类意识的损失函数，即 HALOs**。虽然没有区别使 HALO 优于非 HALO 本身，但目前的技术状况表明，在 HALO 定义下的损失函数比那些不起作用的损失函数工作得更好。（KTO）过程是一种优化技术，其目标是直接**最大化生成内容效用**，而**不是传统方法中的对偏好的对数似然**（log-likelihood）

> ---
>
> ### **1. 传统方法 vs. KTO**
>
> #### **传统偏好优化（如RLHF）**
>
> - **目标**: 通过收集人类对多个生成结果的偏好排序（例如A > B > C），训练模型使偏好排序的对数似然概率最大化。
> - **局限**: 
>   - 依赖高质量、无噪声的偏好标注数据，数据收集成本高。
>   - 假设偏好关系是严格且一致的，但实际标注可能存在模糊性或矛盾。
>   - 优化目标是“与数据一致”，而非直接满足用户的实际需求（例如生成内容的有用性、安全性）。
>
> #### **KTO**
>
> - **目标**: 直接优化生成内容的**效用值**（Utility），而非拟合偏好排序。
> - **关键思想**: 
>   - 引入心理学中的**前景理论（Prospect Theory）**（由Kahneman和Tversky提出），将人类对“收益”和“损失”的非对称心理感知建模到优化目标中。
>   - 例如，人对“损失100元”的厌恶感远强于“获得100元”的快乐，这种非对称性会影响对生成内容的效用评估。
>
> ---
>
> ### **2. KTO的核心技术**
>
> - **效用函数设计**: 
>
>   - 定义生成内容的质量评分函数（如安全性、有用性、流畅性），将其映射为效用值。
>   - 结合前景理论，对“正面效用”（如优质内容）和“负面效用”（如有害内容）赋予不同的权重，反映人类心理的非对称性。
>
> - **优化目标**: 
>
>   - 直接最大化生成内容的期望效用，而非间接拟合偏好排序。
>
>   - 数学形式可简化为：  
>     $$
>     \max_{\theta} \mathbb{E}_{x \sim p_{\theta}}[U(x)] - \lambda \cdot \text{Risk}(x)
>     $$
>     其中$$ U(x) $$是效用函数，$$ \text{Risk}(x) $$ 是风险惩罚项（如生成有害内容的概率），$$ \lambda $$是权衡系数。
>
> ---

KTO仅需要一个二进制信号，即输出是否是可取的，这比偏好更容易获得数据。

在损失函数中对人类偏见进行建模的这一概念是否会为对齐带来实际好处？

在实验上比较了不同参数大小的Pythia和Llama模型以及相同的设置和数据后，证明了HALO在所有尺度上都匹配或优于非HALO，即使差距仅在参数130亿或更多的模型中显着。



### Kahneman-Tversky Optimization

KTO 建立在 KL 约束的 RLHF 目标函数之上，插入了来自卡尼曼 - 特沃斯基人类效用模型的表达式，同时也进行了一些修改以使其与大语言模型兼容。它的工作原理是添加一个 KL 惩罚项，如果模型以通用方式增加了一个理想示例的奖励，该惩罚项就会增加。这迫使模型去学习是什么使得输出是令人满意的，以便在保持 KL（Kullback-Leibler 散度）不变的情况下增加奖励。

在将近端策略优化算法（PPO）与直接策略优化算法（DPO）进行比较时，他们发现 + 1/-1 的奖励信号即便没有比 DPO 表现得更好，也和 DPO 表现得一样好。

这意味着，不用使用比较两个提示的数据并根据用户喜欢哪个提示进行排名，而是如果指令或提示是有用或可接受的，则由正面信号组成，或者如果没有有用，则为负。

<img src="http://pointerhacker.github.io/imgs/posts/mysft/part-7-overview.png" alt="img" style="zoom:33%;" />

由于这一见解，他们利用卡尼曼 - 特沃斯基的人类效用模型开发了一种光环（HALO），这使得他们能够仅通过一个二元信号（对于给定的输入，输出是否是理想的）直接针对效用进行优化。该过程称为Kahneman-Tversky优化（KTO），可访问与优先数据相比，更丰富，更便宜且更易于收集的数据。



经过一些实验，出现了两个非常有趣的结果：

- 在没有首先进行SFT的情况下，DPO一致的模型倾向于漫步并幻觉整个对话。KTO不会遭受这种现象。
- 在保留不良数据的同时丢弃了90％的理想示例之后，KTO一致的Llama-7b模型仍然胜过DPO。这意味着偏好对不一定是KTO数据的来源。



### 什么时候到KTO和何时去DPO？

如果可用的人类反馈是二进制格式，或者如果期望和不受欢迎的例子之间存在不平衡，则KTO擅长。

如果数据是偏好的形式，那么选择就变得不太清楚。理论分析表明，如果数据具有相对较低的噪声和不强制性，DPO将更好地工作，因为KTO不适合使用KTO。但是，如果有噪音，最好的案例可以确保KTO的表现优于DPO。

事实证明，大多数可公开的数据集被证明是嘈杂的，人类的偏好相互矛盾。这解释了为什么KTO在实验中匹配或超过DPO性能的原因。人工智能反馈也可能是嘈杂的和不及物的，因此在这种情况下，DPO也可能是最佳选择。









## KTO实现

### 数据集的预处理

```python
### step 1
dataset:
prompt:
completion:

#### 默认数据集合
1.1 maybe_apply_chat_template
	 用户与机器人的表达放在使用自然语言结构话描述的模版中

1.2 _tokenize
  
  prompt_input_ids: 输入id
  answer_input_ids: 答案的id

1.3 _process_tokens ['prefix'] = ''
   1、一句大于max_len 按照 truncation_mode[keep_start,keep_end] 前截断或者后截断prompt
   2、如果截断后 prompt+ anwser 还是 大雨max_len 再截断 anwser
 		> 优先截断输入
   3、['prefix']completion_input_ids = prompt_input_ids + answer_input_ids
   4、['prefix']completion_labels = ['prefix']completion_input_ids
     ['prefix']completion_labels[: len(batch['prefix']prompt_input_ids)] = [label_pad_token_id]
          * len(['prefix']prompt_input_ids)
      

#### KL数据集

2.1 _get_kl_dataset
#这段代码意在通过交换每个批次中的匹配对，创建用于估计KL项的配对样本。具体来说，就是将一组匹配的样本对（x_1, y_1）、...、（x_n, y_n）重新排列成（x_1, y_n）、...、（x_n, y_1），即（x'_1, y'_1）、...、（x'_n, y'_n）
	batch["answer_input_ids"] = [batch["answer_input_ids"][-1]] + batch["answer_input_ids"][:-1]

2.2 _process_tokens ['prefix'] = 'KL'
   1、一句大于max_len 按照 truncation_mode[keep_start,keep_end] 前截断或者后截断prompt
   2、如果截断后 prompt+ anwser 还是 大雨max_len 再截断 anwser
 		> 优先截断输入
   3、['prefix']completion_input_ids = prompt_input_ids + answer_input_ids
   4、['prefix']completion_labels = ['prefix']completion_input_ids
     ['prefix']completion_labels[: len(batch['prefix']prompt_input_ids)] = [label_pad_token_id]
          * len(['prefix']prompt_input_ids)

#### 合并数据集
3.1 最终的数据集合
Dataset({
    features: ['prompt', 'completion', 'label', 'prompt_input_ids', 'prompt_attention_mask', 'answer_input_ids', 'answer_attention_mask', 'completion_input_ids', 'completion_attention_mask', 'completion_labels', 'KL_prompt', 'KL_completion', 'KL_label', 'KL_prompt_input_ids', 'KL_prompt_attention_mask', 'KL_completion_input_ids', 'KL_completion_attention_mask', 'KL_completion_labels'],
    num_rows: 1500
})


```





### 训练

#### get_train_dataloader

```python
### compute_reference_log_probs 
completion_logits = self.ref_model(completion_input_ids)
KL_logits = self.ref_model(KL_completion_input_ids)

## 同时作用于 completion_logits、KL_logits

### compute_reference_log_probs -> get_batch_logps
> 每个生成标签的token对应的概率
labels = ['prefix']completion_labels
logits completion_logits or KL_logits
#获取掩码
loss_mask = labels != label_pad_token_id
#生成对应的token概率
per_token_logps = torch.gather(logits.log_softmax(-1), dim=2, index=labels.unsqueeze(2)).squeeze(2)
return (per_token_logps * loss_mask).sum(-1)

往dataset里面增加列
reference_logps :  原始模型推理的每个token的概率
reference_KL_logps : 移位后的每个token的概率
```



#### compute_loss

```python
compute_loss -> get_batch_loss_metrics

get_batch_loss_metrics -> forward

### step1 ##生成概率分布
### 参考KL
KL_logits = model(KL_completion_input_ids)
# get_batch_logps 
> 每个生成标签的token对应的概率
loss_mask = labels != label_pad_token_id
per_token_logps = torch.gather(logits.log_softmax(-1), dim=2, index=labels.unsqueeze(2)).squeeze(2)
return (per_token_logps * loss_mask).sum(-1)

### 原始数据
completion_logits = model(completion_input_ids).logits
#get_batch_logps 
> 每个生成标签的token对应的概率
loss_mask = labels != label_pad_token_id
per_token_logps = torch.gather(logits.log_softmax(-1), dim=2, index=labels.unsqueeze(2)).squeeze(2)
return (per_token_logps * loss_mask).sum(-1)

### step2 ##抽取出有效的标签
chosen_idx: 这个批次中标签位True的索引
rejected_idx: 这个批次中标签位False的索引
# 概率分布 batch,seq_len
chosen_logps = completion_logps[chosen_idx, ...]
rejected_logps = completion_logps[rejected_idx, ...]
# 原始输出 batch,seq_len,dim
chosen_logits = completion_logits[chosen_idx, ...]
rejected_logits = completion_logits[rejected_idx, ...]

### step3 ##返回
return (chosen_logps, rejected_logps, chosen_logits, rejected_logits, KL_logps)

重命名为
policy_chosen_logps,policy_rejected_logps,policy_chosen_logits,policy_rejected_logits,policy_KL_logps
```



```python
reference_chosen_logps = batch["reference_logps"][chosen_idx, ...]
reference_rejected_logps = batch["reference_logps"][rejected_idx, ...]
reference_KL_logps = batch["reference_KL_logps"]
```



```python
get_batch_loss_metrics -> kto_loss

### step1 ##KL
kl = (policy_KL_logps - reference_KL_logps).mean()

### step1 ##Chosen losses
chosen_logratios = policy_chosen_logps - reference_chosen_logps
#这里是 chosen_logratios - kl，目标是让chosen样本的log ratio大于KL散度。换句话说，我们希望被选择样本的概率比较高。
chosen_losses = 1 - F.sigmoid(self.beta * (chosen_logratios - kl))
chosen_rewards = self.beta * chosen_logratios.detach()

### step2 ##Rejected losses
rejected_logratios = policy_rejected_logps - reference_rejected_logps
# 这里是 kl - rejected_logratios，目标是让KL散度大于rejected样本的log ratio。也就是说，我们希望被拒绝样本的概率比较低。
rejected_losses = 1 - F.sigmoid(self.beta * (kl - rejected_logratios))
rejected_rewards = self.beta * rejected_logratios.detach()

### step3 ##Final losses
losses = torch.cat(
        (self.desirable_weight * chosen_losses, self.undesirable_weight * rejected_losses),
        0,
    )
```







## ORPO

> 在SFT期间直接实施RL

这正是**Odds Ratio Preference Optimization（ORPO）**所建议的。

当我们想根据我们的需求量身定制LLM时，我们会应用**指令调整和偏好对齐**。

但是，这种方法涉及**几个模型和训练阶段**以实现预期的结果（以其SFT，RM和PPO步骤或以其SFT和DPO阶段为单位）。在所有这些当中，SFT 在实现成功的融合方面起着至关重要的作用。尽管以前的研究已经阐明了**SFT**在对齐中的相关性，但研究人员对其进行了深入分析并发现了缺点。SFT**增加了获得所需令牌的可能性，但它也提高了产生不希望结果的可能性**。这导致寻找一种机制，该机制仍然可以使模型适应特定域，但同时惩罚了不希望的响应。

### Odds Ratio Preference Optimization (ORPO)

> 优势比偏好



![img](http://pointerhacker.github.io/imgs/posts/mysft/part-8-alignments.png)

ORPO**在单个过程中结合了指令调整和偏好对齐**，使其在无参考模型和计算上更有效。

ORPO 通过使用基于优势比的损失与传统的负对数似然损失 log-likelihood loss（NLL）一起对不良响应进行惩罚，从而创建了一个新的目标，这使得它能够区分有利响应和不利响应。

因此，它包括两个主要组成部分：

- **SFT损失：**常规语言因果建模的NLL损失函数，最大化产生参考令牌的概率。
- **相对比率损失**：最大化受欢迎的响应的产生和不利的反应之间的几率。

这些组件共同引导大型语言模型适应特定领域的期望生成结果，并抑制被拒绝响应集中的生成结果。



为了评估这种方法，研究人员研究了其在各种模型尺寸上的性能，从较小的125-M参数模型到大型1.3-B参数，并使用了拟人化的HH-RLHF和二进制的Ultrafeptback（以及[Argilla的清洁版本](https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned)）偏好数据集。研究结果表明，ORPO的**表现优于其他算法**。



### 总结

ORPO采用基于似然比的新颖方法，为模型对齐提供了新视角，可能在资源上带来显著的效率提升。
它的方法既简单又有效，能够对语言模型进行微调，既适用于特定领域，也可以对其响应进行对齐。
在大型语言模型（LLMs）变得越来越多和实验越来越频繁的情况下，ORPO呈现出一种有价值的替代方案。
尽管还需要进一步的研究和实验，目前的结果是相当有希望的。



### 局限性

虽然ORPO展示了令人鼓舞的结果，但其在不同任务、领域或扩大到更大语言模型时的通用性尚需彻底检验。
对比分析更广泛的偏好对齐算法，而不仅仅是常引用的DPO和RLHF，将会有益。
此外，探索将ORPO与这些算法整合的潜力可能会提供有价值的见解。





## GRPO

>  Group Relative Policy Optimization (GRPO).小组相对策略优化

### 一、简介

在加强学习（RL）中，仅仅知道“您得分多少分”通常还不够。**仅追求高分**就可以导致各种副作用，例如过度探索，模型中的不稳定，甚至偏离合理政策的“捷径”行为。为了应对这些挑战，RL结合了几种机制，例如评论家（值函数），剪辑操作，参考模型以及最新的小组相对策略优化（GRPO）。为了使这些概念更直观，让我们进行类比：**将RL培训过程视为小学考试场景。**我们（正在接受培训的模型）就像学生试图获得高级成绩一样，我们的考试成绩就像奖励模型一样，而我们的父母根据我们的成绩分发零用钱类似于评论家。接下来，让我们一步一步地浏览一下为什么**仅最终分数**就不足，批评者，剪辑和参考模型如何发挥作用，最后GRPO如何扩展这些想法。



### 二、只使用奖励的天真方法：有什么问题？

假设我和我的弟弟在同一小学课上。老师对我们的考试进行了评分，并给出了“绝对得分”。我通常在100分中的80分以上，而我的兄弟经常得到30分。然后，我们将这些分数直接带给我们的父亲要求零花钱，这意味着我们的“奖励”（以RL的方式）只是我们的原始考试成绩。谁获得更高的分数会收到更多的零用钱。乍一看，这似乎很好。但是很快出现了两个大问题：

- **不公平**：如果我的兄弟通过大量的辛苦工作从30分提高到60分，那么与我通常的80分以上相比，他仍然显得苍白。
- **不稳定**：我自己追求更高的分数可能会导致我采用极端的学习方法（例如，在所有时间挤压，熬夜很晚）有时我可能只有95个，而其他时间只有60，因此我的分数（因此是奖励信号）急剧散发。

结果，**将绝对分数作为奖励**会导致巨大的奖励波动，而我的兄弟最终觉得不值得尝试以少量增量进行改进。

###  

### 数学表达式

$$\mathcal{I}*{\text{naive}}(\theta) = \mathbb{E}*{(q,o) \sim (\text{data}, \pi_\theta)}[r(o)]$$

这意味着“仅优化最终奖励”，我们可以遇到高度差异和部分改进的激励措施。换句话说，演员缺乏与其当前水平相匹配的**基线**，并阻碍了培训效率。



### 三、介绍评论家：使用“预测的分数”来提高奖励

认识到这个问题，爸爸意识到**“这不仅仅是绝对得分；这是关于您相对于当前水平的进步。”** 所以他决定：

- 将我的“预测得分线”设置为80分，而我的兄弟为40分。
- 如果我们在考试中超越了这些线条，我们会得到更多的零用钱。如果没有，我们几乎没有或没有。

因此，如果我的兄弟努力工作并从30到60跳，他比“预测的得分线” 20分，这将转化为巨大的奖励。同时，如果我留在80左右，增量增益较小，因此我不一定收到的收益不如他更多。这种安排**鼓励每个人**从自己的基准中提高，而不是纯粹比较绝对得分。当然，爸爸很忙，所以一旦设定了一条线，它不仅保持静态 - 他需要随着我们的进步保持**“重新调整”** 。如果我的兄弟水平达到60范围，那么40分的基准将不再公平。同样，如果我一直徘徊在85左右，爸爸可能也需要调整我的线路。换句话说，**爸爸还必须学习**，特别是关于我和我兄弟的进步速度。

### 数学表达式

在RL中，此“得分线”被称为**值函数**， Vψ(s)。我们的培训目标从“just reward”演变为“我们多于基准的胜利”，这是由优势表示的：

$$A_t = r_t - V_{\psi}(s_t)$$

对于给定的状态$$s_t$$和动作$$o_t$$如果实际奖励超出了评论家的期望，则意味着该动作的执行效果要比预期的要好。如果较低，则该动作的表现不佳。在最简单的配方中，我们优化了类似的内容：

$$\mathcal{J}_{\mathrm{adv}}(\theta)=\mathbb{E}[A(o)], \quad \text{where } A(o)=r(o)-V_{\psi}(o)$$

通过减去此“分数线”，我们减少了训练方面的差异，从而为超出预期的行动提供了更高的梯度信号，并惩罚了那些不足的行动。

##  

### 四、添加剪辑和最小操作：防止过度更新

即使有“得分线”，新问题也会出现。例如：

- 如果我突然突然**通过**测试并得分95或100，爸爸可能会给我一个巨大的奖励，促使我在下一次考试之前采用过于侵略性的学习模式。我的成绩可能在极端（95和60）之间摆动，从而导致巨大的奖励波动。

因此，爸爸决定在每个步骤中都可以更大程度地更新我的学习策略 - 他不会仅仅因为一个良好的测试而给我更多**的**零用钱。如果他付出了太多，我可能会转向极端探索。如果太少，我就不会动机。因此他必须找到平衡。

### 数学表达式

在**PPO（近端策略优化）**中，通过“剪辑”机制实现了这种平衡。PPO目标的核心包括：

$$\min \left(r_t(\theta)A_t,\text{clip}(r_t(\theta),1 - \varepsilon,1+\varepsilon)A_t\right),$$

其中：

$$r_t(\theta)=\frac{\pi_{\theta}(o_t|s_t)}{\pi_{\theta_{\text{old}}}(o_t|s_t)}$$

如果比率偏离1的比率，则将其夹在 $$[ 1−ε, 1+ε]$$ ，这**限制了**一个策略可以在一个更新中转移多少。

用更简单的术语：

- 得分100可以给我带来额外的奖励，但爸爸施加了“天花板”，所以我不会过分。然后，他将在下一场考试中重新评估，保持稳定的方法，而不是助长极端波动。



#### 五、参考模型：防止作弊和极端策略

即便如此，如果我只专注于高分，我可能会**采取可疑的策略**，例如，作弊或吓老师使我获得完美的成绩。显然，这打破了所有规则。在大型语言模型的领域中，类似的情况正在产生有害或捏造的内容，以人为地提高一些奖励指标。因此，爸爸设定了一个附加规则：

- “无论如何，您不能偏离原始，诚实的学习方法。如果您离基线太远，即使得分很高，我也会取消您的资格并扣留您的零用钱。”

这类似于从学期开始时标记**“参考线”** （即在初步监督微调之后）。您不能偏离原始策略，也不能面临罚款。



### 数学表达式

在PPO中，通过对**参考模型**（初始策略）添加KL罚款可以反映出来。具体而言，我们包括：

$$-\beta \mathcal{D}_{\mathrm{KL}}(\pi_{\theta} \| \pi_{\text{ref}})$$

在损失中。这使演员无法远离原始的，明智的政策，避免了“作弊”或其他巨大的外界行为。

##  

### 六、GRPO：用“多个模拟平均值”替换值函数

有一天，爸爸说：“我没有时间继续评估您的学习进度并始终绘制新的分数线。为什么不先进行五组模拟测试，然后将其**平均得分**作为您的**预期得分呢**？如果您在实际测试中超过了该平均值，则表明您的表现要比您自己的期望更好，所以我会奖励您。否则，您将不会得到太多。”我和我的兄弟，以及可能更多的同学，每个人都可以依靠一组个人模拟测试，而不是爸爸必须不断调整的外部“价值网络”。到目前为止，我们看到PPO依赖于演员 +评论家 + Clip + KL罚款框架。但是，在大语言模型中（LLM ）场景，评论家（价值功能）**通常需要与演员一样大**，以准确评估状态，这可能是昂贵的，有时是不切实际的，尤其是如果您最终只有一个最终的奖励（例如最终答案质量） 。因此，**小组相对策略优化（GRPO）**步骤。它的核心想法：

- 对于评论家**没有单独的价值网络**，
- 从旧策略中采样相同问题或状态的多个输出，
- **将这些输出的平均奖励视为基线**，
- 超过平均水平的任何东西都会产生“积极的优势”，而低于平均水平的任何东西都会产生“负优势”。

同时，GRPO**保留了**PPO的剪辑和KL机制，以确保稳定，合规的更新。



### 数学表达式

根据DeepSeekmath的技术报告，GRPO目标（省略一些符号）是：
$$
J_{GRPO}(\theta) = \mathbb{E} \left[ \sum_{i=1}^{G} \left( \min \left( \frac{\pi_\theta (o_i)}{\pi_{\theta_{old}} (o_i)} A_i, \text{clip} \left( \frac{\pi_\theta (o_i)}{\pi_{\theta_{old}} (o_i)}, 1-\epsilon, 1+\epsilon \right) A_i \right) \right) - \beta \mathbb{D}_{KL} (\pi_\theta \| \pi_{ref}) \right],
$$

$$
A_i = \frac{r_i - \text{mean}(\{r_1, r_2, \cdots, r_G\})}{\text{std}(\{r_1, r_2, \cdots, r_G\})}
$$



通过平均来自同一问题的多个输出并归一化来计算“相对得分”。这样，**我们不再需要专用的价值函数**，但是我们仍然获得了动态的“得分线”，可以简化培训并保存资源。

##  



使用小学考试类比，我们从**原始的绝对分数**到PPO的完整机制（评论家，优势，剪辑，参考模型），然后逐步移动到**GRPO** （利用多个输出的平均得分来消除价值函数） 。以下是一些关键要点：

- **评论家的角色**：为每状态提供“合理的期望”，从而大大减少培训差异。
- **剪辑和最小机制**：约束更新幅度，以防止对单个“突破”检查过度反应。
- **参考模型**：劝阻“作弊”或极端偏差，以确保策略与其初始状态保持合理的一致性。
- **GRPO的优点**：在大型语言模型中，它消除了对单独的价值网络的需求，降低内存和计算成本，同时与“比较”奖励模型设计很好地保持一致。

就像爸爸改用“让孩子们自己模拟多次考试，然后将他们的平均值视为基准”的方式，Grpo避免了保持大规模的批评家，同时仍提供相对的奖励信号。它保留了PPO的稳定性和合规性，但简化了该过程。





### GRPO实现

```python
self.max_prompt_length = args.max_prompt_length
self.max_completion_length = args.max_completion_length  # = |o_i| in the GRPO paper
self.num_generations = args.num_generations  # = G in the GRPO paper


# 数据采样
>>> sampler = RepeatRandomSampler(["a", "b", "c", "d"], repeat_count=2)
>>> list(sampler)
[2, 2, 0, 0, 3, 3, 1, 1]
```

 

 

```python
training_step
 1、_prepare_inputs
 2、compute_loss


### 输入准备
_prepare_inputs


### step1 计算ref_model 的概率分布
#结构化 + 分词
prompts_text = maybe_apply_chat_template()['prompt']
prompt_inputs = self.processing_class(prompts_text)


# 生成文本
prompt_ids = prompt_ids[:, -self.max_prompt_length :] # 默认左截断
prompt_completion_ids = unwrapped_model.generate(prompt_ids)
prompt_ids = prompt_completion_ids[:, :prompt_length]
completion_ids = prompt_completion_ids[:, prompt_length:]


#  Mask everything after the first EOS token
completion_mask = (sequence_indices <= eos_idx.unsqueeze(1)).int()
attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)  # (B*G, P+C)


# 获取生成的每个token的概率
logits_to_keep = completion_ids.size(1)  # 我们只需要计算补全标记的对数概率。
ref_per_token_logps = self._get_per_token_logps(
    self.ref_model, prompt_completion_ids, attention_mask, logits_to_keep
)
#其中：
_get_per_token_logps
 # log_softmax = logits - log(sum(exp(logits)))
 token_log_probs = token_logits - logsumexp_values 


### step2 计算奖励期望
#根据奖励模型计算每一个输出对应的奖励
reward_inputs = reward_processing_class(texts)
rewards_per_func[:, 0] = reward_func(**reward_inputs).logits[:, 0] # Shape (B*G,)


# 因为输入是重复G次的 需要将G个输入为一组求平均 然后计算奖励优势
#[batch * G,reward_func_num=1] -> [batch * G]
# Sum the rewards from all reward functions
rewards = rewards_per_func.sum(dim=1)


# [batch * G,] -> [batch,G] -> [batch]
# Compute grouped-wise rewards
mean_grouped_rewards = rewards.view(-1, self.num_generations).mean(dim=1)
std_grouped_rewards = rewards.view(-1, self.num_generations).std(dim=1)


# batch -> [batch*G]
# Normalize the rewards to compute the advantages
mean_grouped_rewards = mean_grouped_rewards.repeat_interleave(self.num_generations, dim=0)
std_grouped_rewards = std_grouped_rewards.repeat_interleave(self.num_generations, dim=0)


# [batch * G,]_a - [batch * G,]_b
# 按照RepeatRandomSampler的方式对每一个输入重复采样G次
# 将相同输入的reward 求平均后变成 [batch * G,]_b
# 相剪后作为他的优势
advantages = (rewards - mean_grouped_rewards) / (std_grouped_rewards + 1e-4)


# 最终输入模型的结果
return {
      "prompt_ids": prompt_ids,
      "prompt_mask": prompt_mask,
      "completion_ids": completion_ids,
      "completion_mask": completion_mask,
      "ref_per_token_logps": ref_per_token_logps,
      "advantages": advantages,
  }
```

 

```python
training_step
 1、_prepare_inputs
 2、compute_loss


compute_loss
# 获取当前模型每个token的概率
prompt_ids, prompt_mask = inputs["prompt_ids"], inputs["prompt_mask"]
input_ids = torch.cat([prompt_ids, completion_ids], dim=1)


per_token_logps = self._get_per_token_logps(model, input_ids, attention_mask, logits_to_keep)


# Compute the KL divergence between the model and the reference model
# 计算模型与参考模型之间的 KL 散度。KL 散度（Kullback-Leibler divergence
ref_per_token_logps = inputs["ref_per_token_logps"]
per_token_kl = torch.exp(ref_per_token_logps - per_token_logps) - (ref_per_token_logps - per_token_logps) - 1


 # x - x.detach() allows for preserving gradients from x
 # x - x.detach () 允许保留来自 x 的梯度
advantages = inputs["advantages"]
# exp(log(a) - log(b)) = exp(log(a/b)) = a/b
# 这实际上等价于 p/p_old,即新旧策略的概率比率
per_token_loss = torch.exp(per_token_logps - per_token_logps.detach()) * advantages.unsqueeze(1)
per_token_loss = -(per_token_loss - self.beta * per_token_kl)
loss = ((per_token_loss * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)).mean()
```
