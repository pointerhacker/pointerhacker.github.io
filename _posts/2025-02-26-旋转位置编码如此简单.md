---
layout: post
title: 旋转位置编码如此简单
categories: [LLM]
tags: LLM
---

## 基本概念

首先论文中定义一个长度为 `N` 的输入序列为：

$$S_N = \{w_i\}_{i = 1}^N$$ 

其中 $$w_i$$ 表示输入序列中第 i 个 token，而输入序列 $$S_N$$ 对应的 embedding 表示为：

 $$E_N = \{x_i\}_{i = 1}^N$$

其中 `xi` 表示第 `i` 个 token `wi` 对应的 `d` 维词嵌入向量。接着在做 self-attention 之前，会用词嵌入向量计算 `q, k, v` 向量同时加入位置信息，函数公式表达如下：

 $$ \begin{align*} q_m &= f_q(x_m, m)\\ k_n &= f_k(x_n, n)\\ v_n &= f_v(x_n, n) \end{align*} $$ 

其中 `qm` 表示第 `m` 个 token 对应的词向量 `xm` 集成位置信息 `m` 之后的 query 向量。而 `kn` 和 `vn` 则表示第 `n` 个 token 对应的词向量 `xn` 集成位置信息 `n` 之后的 key 和 value 向量。

而基于 transformer 的位置编码方法都是着重于构造一个合适的 `f{q,k,v}` 函数形式。

而计算第 m 个词嵌入向量 `xm` 对应的 self-attention 输出结果，就是 `qm` 和其他 `kn` 都计算一个 attention score ，然后再将 attention score 乘以对应的 `vn` 再求和得到输出向量 `om`：
$$
a_{m,n} = \frac{exp(\frac{q_m^T k_n}{\sqrt{d}})}{\sum_{j = 1}^N exp(\frac{q_m^T k_j}{\sqrt{d}})}
$$
$$
o_m = \sum_{n = 1}^N a_{m,n}v_n
$$

## **绝对位置编码**

对于位置编码，常规的做法是在计算 query, key 和 value 向量之前，会计算一个位置编码向量 `pi` 加到词嵌入 `xi` 上，位置编码向量 `pi` 同样也是 `d` 维向量，然后再乘以对应的变换矩阵 `W{q,k,v}`：

 $$f_{\{q,k,v\}}(x_i, i) = W_{\{q,k,v\}}(x_i + p_i)$$ 

而经典的位置编码向量 `pi` 的计算方式是：

$$p_{i,2t} = sin(\frac{i}{10000^{\frac{2t}{d}}})$$ 

$$p_{i,2t + 1} = cos(\frac{i}{10000^{\frac{2t}{d}}})$$ 

其中 `p_{i,2t}` 表示位置 `d` 维度向量 `pi` 中的第 `2t` 位置分量也就是偶数索引位置的计算公式，而 `p_{i,2t+1}` 就对应第 `2t+1` 位置分量也就是奇数索引位置的计算公式。

python 代码如下：

```python
# position 就对应 token 序列中的位置索引 i
# hidden_dim 就对应词嵌入维度大小 d
# seq_len 表示 token 序列长度
def get_position_angle_vec(position):
    return [position / np.power(10000, 2 * (hid_j // 2) / hidden_dim) for hid_j in range(hidden_dim)]

# position_angle_vecs.shape = [seq_len, hidden_dim]
position_angle_vecs = np.array([get_position_angle_vec(pos_i) for pos_i in range(seq_len)])

# 分别计算奇偶索引位置对应的 sin 和 cos 值
position_angle_vecs[:, 0::2] = np.sin(position_angle_vecs[:, 0::2])  # dim 2t
position_angle_vecs[:, 1::2] = np.cos(position_angle_vecs[:, 1::2])  # dim 2t+1

# positional_embeddings.shape = [1, seq_len, hidden_dim]
positional_embeddings = torch.FloatTensor(position_angle_vecs).unsqueeze(0)
```



## 旋转式位置编码

论文中提到为利用token之间的相对位置信息，假定query向量$q_m$和key向量$k_n$之间的内积操作可由函数$g$表示，函数$g$输入为词嵌入向量$x_m$，$x_n$及其相对位置$m - n$：

 $$< f_q(x_m, m), f_k(x_n, n) >= g(x_m, x_n, m - n)$$ 

我们直接利用旋转矩阵的性质，推导会很简洁。接下来的目标就是找到一个等价的位置编码方式，从而使得上述关系成立。

假设 $$R_a$$ 表示角度为 $$a$$ 的旋转矩阵，那么 $$R$$ 具有如下性质：

1. $$R_a^T = R_{-a}$$
2. $$R_a R_b = R_{a+b}$$

回到旋转位置编码，我们可以去证明 $$\langle R_a X, R_b Y \rangle = \langle X, R_{b-a} Y \rangle$$，证明如下：

$$\begin{align}
\langle R_a X, R_b Y \rangle &= (R_a X)^T R_b Y\\
&= X^T R_a^T R_b Y\\
&= X^T R_{-a} R_b Y\\
&= X^T R_{b-a} Y\\
&= \langle X, R_{b-a} Y \rangle
\end{align}$$

这就证明上述关系是成立的，位置 m 的 query 和位置 n 的 key 的内积就是函数 `g`。


$$
\begin{align*}
&< f_q(x_m, m), f_k(x_n, n) >\\
=&\left(\begin{pmatrix}
\cos(m\theta)& -\sin(m\theta)\\
\sin(m\theta)& \cos(m\theta)
\end{pmatrix}\begin{pmatrix}
q_m^{(1)}\\
q_m^{(2)}
\end{pmatrix}\right)^T\left(\begin{pmatrix}
\cos(n\theta)& -\sin(n\theta)\\
\sin(n\theta)& \cos(n\theta)
\end{pmatrix}\begin{pmatrix}
k_n^{(1)}\\
k_n^{(2)}
\end{pmatrix}\right)\\
=&\begin{pmatrix}
q_m^{(1)}& q_m^{(2)}
\end{pmatrix}\begin{pmatrix}
\cos(m\theta)& \sin(m\theta)\\
-\sin(m\theta)& \cos(m\theta)
\end{pmatrix}\begin{pmatrix}
\cos(n\theta)& -\sin(n\theta)\\
\sin(n\theta)& \cos(n\theta)
\end{pmatrix}\begin{pmatrix}
k_n^{(1)}\\
k_n^{(2)}
\end{pmatrix}\\
=&\begin{pmatrix}
q_m^{(1)}& q_m^{(2)}
\end{pmatrix}\begin{pmatrix}
\cos(m\theta)\cos(n\theta) + \sin(m\theta)\sin(n\theta)& -\cos(m\theta)\sin(n\theta)+\sin(m\theta)\cos(n\theta)\\
-\sin(m\theta)\cos(n\theta)+\cos(m\theta)\sin(n\theta)& \sin(m\theta)\sin(n\theta)+\cos(m\theta)\cos(n\theta)
\end{pmatrix}\begin{pmatrix}
k_n^{(1)}\\
k_n^{(2)}
\end{pmatrix}\\
=&\begin{pmatrix}
q_m^{(1)}& q_m^{(2)}
\end{pmatrix}\begin{pmatrix}
\cos((m - n)\theta)& -\sin((m - n)\theta)\\
\sin((m - n)\theta)& \cos((m - n)\theta)
\end{pmatrix}\begin{pmatrix}
k_n^{(1)}\\
k_n^{(2)}
\end{pmatrix}
\end{align*}
$$


然后上面的讲解是假定的词嵌入维度是2维向量，而对于d >= 2` 的通用情况，则是将词嵌入向量元素按照两两一组分组，每组应用同样的旋转操作且每组的旋转角度计算方式如下：

  其中$$\theta_i = \frac{\pi}{10000^{2i/d}}$$，$$i = 0,1,\cdots,d/2 - 1$$。

其中$$\theta_i = m\cdot\frac{\pi}{10000^{2i/d}}$$，$$i = 0,1,\cdots,d/2 - 1$$。

在RoPE中，对于一个$$d$$维向量$$\mathbf{x}=(x_1,x_2,\cdots,x_d)$$，假设位置索引为$$m$$，将其分成偶数维度和奇数维度两部分，分别进行旋转操作。通常定义$$\theta = n\cdot\frac{\pi}{10000^{2i/d}}$$，其中$$i$$表示维度索引（$$i = 0,1,\cdots,d/2 - 1$$）。

所以简单来说 RoPE 的 self-attention 操作的流程是，对于 token 序列中的每个词嵌入向量，首先计算其对应的 query 和 key 向量，然后对每个 token 位置都计算对应的旋转位置编码，接着对每个 token 位置的 query 和 key 向量的元素按照 两两一组 应用旋转变换，最后再计算 query 和 key 之间的内积得到 self-attention 的计算结果。

论文中有个很直观的图片展示了旋转变换的过程：

![img](http://pointerhacker.github.io/imgs/posts/v2-64061f9e98e434c0313c14837d8890fb_1440w.jpg)

```python
def precompute_freqs_cis(dim: int, seq_len: int, theta: float = 10000.0):
    # 计算词向量元素两两分组之后，每组元素对应的旋转角度
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))
    # 生成 token 序列索引 t = [0, 1,..., seq_len-1]
    t = torch.arange(seq_len, device=freqs.device)
    # freqs.shape = [seq_len, dim // 2] 
    freqs = torch.outer(t, freqs).float()
    # torch.polar 的文档
    # https://pytorch.org/docs/stable/generated/torch.polar.html
    # 计算结果是个复数向量
    # out=abs⋅cos(angle)+abs⋅sin(angle)⋅j
    
    # 假设 freqs = [x, y]
    # 则 freqs_cis = [cos(x) + sin(x)i, cos(y) + sin(y)i]
    # freqs.shape = [seq_len, dim // 2] 
    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)
    return freqs_cis

def apply_rotary_emb(
    xq: torch.Tensor,
    xk: torch.Tensor,
    freqs_cis: torch.Tensor,
) -> Tuple[torch.Tensor, torch.Tensor]:
    # xq.shape = [batch_size, seq_len, dim]
    # xq_.shape = [batch_size, seq_len, dim // 2, 2]
    xq_ = xq.float().reshape(*xq.shape[:-1], -1, 2)
    xk_ = xk.float().reshape(*xk.shape[:-1], -1, 2)
    # >>> x=torch.randn(4, 2)
    # >>> x
    # tensor([[ 1.6116, -0.5772],
    #         [-1.4606, -0.9120],
    #         [ 0.0786, -1.7497],
    #         [-0.6561, -1.6623]])
    # >>> torch.view_as_complex(x)
    # tensor([(1.6116-0.5772j), (-1.4606-0.9120j), (0.0786-1.7497j), (-0.6561-1.6623j)])
    # 转为复数域
    #https://pytorch.org/docs/stable/generated/torch.view_as_complex.html
    xq_ = torch.view_as_complex(xq_)
    xk_ = torch.view_as_complex(xk_)
    
    # 应用旋转操作，然后将结果转回实数域
    # xq_out.shape = [batch_size, seq_len, dim]
    # 每两个元素之间：
        # $$(cos(\theta) + sin(\theta)i) * (x_{q1} - x_{q2}i)$$
        # = $$(\cos(\theta)x_{q1}-\sin(\theta)x_{q2})+(\cos(\theta)x_{q2}+\sin(\theta)x_{q1})i$$
        # = $$\begin{bmatrix}x_1'\\x_2'\end{bmatrix}=\begin{bmatrix}\cos\theta&-\sin\theta\\\sin\theta&\cos\theta\end{bmatrix}\begin{bmatrix}x_1\\x_2\end{bmatrix}$$
    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(2)
    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(2)
    return xq_out.type_as(xq), xk_out.type_as(xk)

class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        self.wq = Linear(...)
        self.wk = Linear(...)
        self.wv = Linear(...)
        
        self.freqs_cis = precompute_freqs_cis(dim, max_seq_len * 2)

    def forward(self, x: torch.Tensor):
        bsz, seqlen, _ = x.shape
        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)

        xq = xq.view(batch_size, seq_len, dim)
        xk = xk.view(batch_size, seq_len, dim)
        xv = xv.view(batch_size, seq_len, dim)

        # attention 操作之前，应用旋转位置编码
        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)
        
        # scores.shape = (bs, seqlen, seqlen)
        scores = torch.matmul(xq, xk.transpose(1, 2)) / math.sqrt(dim)
        scores = F.softmax(scores.float(), dim=-1)
        output = torch.matmul(scores, xv)  # (batch_size, seq_len, dim)
  # ......

```

**代码中巧妙是使用虚数乘法实现矩阵旋转**

$$(cos(\theta) + sin(\theta)i) * (x_{q1} - x_{q2}i)$$

等价于

$$(\cos(\theta)x_{q1}-\sin(\theta)x_{q2})+(\cos(\theta)x_{q2}+\sin(\theta)x_{q1})i$$

是下面矩阵乘法的虚数表示形式

$$\begin{bmatrix}x_1'\\x_2'\end{bmatrix}=\begin{bmatrix}\cos\theta&-\sin\theta\\\sin\theta&\cos\theta\end{bmatrix}\begin{bmatrix}x_1\\x_2\end{bmatrix}$$



## 整体概念

RoPE (Rotary Position Embedding) 的核心公式是通过**对词向量进行旋转变换来注入位置信息**。

RoPE（Rotary Position Embedding）即旋转位置编码，其核心公式主要涉及到对位置信息的旋转操作等，以下是其相关公式的具体介绍：

对于一个二维向量$$\mathbf{x}=(x_1,x_2)$$，将其逆时针旋转$$\theta$$角度后得到新的向量$$\mathbf{x}'=(x_1',x_2')$$，根据二维旋转矩阵的原理，有：

$$\begin{bmatrix}x_1'\\x_2'\end{bmatrix}=\begin{bmatrix}\cos\theta&-\sin\theta\\\sin\theta&\cos\theta\end{bmatrix}\begin{bmatrix}x_1\\x_2\end{bmatrix}$$

在RoPE中，对于一个$$d$$维向量$$\mathbf{x}=(x_1,x_2,\cdots,x_d)$$，假设位置索引为$$n$$，将其分成偶数维度和奇数维度两部分，分别进行旋转操作。通常定义$$\theta = n\cdot\frac{\pi}{10000^{2i/d}}$$，其中$$i$$表示维度索引（$$i = 0,1,\cdots,d/2 - 1$$）。

对于偶数维度$$2i$$和奇数维度$$2i + 1$$，旋转公式为：

$$\begin{cases}x_{2i}'=x_{2i}\cos\left(n\cdot\frac{\pi}{10000^{2i/d}}\right)-x_{2i + 1}\sin\left(n\cdot\frac{\pi}{10000^{2i/d}}\right)\\x_{2i+1}'=x_{2i}\sin\left(n\cdot\frac{\pi}{10000^{2i/d}}\right)+x_{2i + 1}\cos\left(n\cdot\frac{\pi}{10000^{2i/d}}\right)\end{cases}$$

用矩阵形式表示为：

$$\mathbf{x}'=\begin{bmatrix}\cos\theta_0&-\sin\theta_0&&&&\\\sin\theta_0&\cos\theta_0&&&&\\&&\cos\theta_1&-\sin\theta_1&&\\&&\sin\theta_1&\cos\theta_1&&\\&&&&\ddots&\\&&&&&\cos\theta_{d/2-1}&-\sin\theta_{d/2-1}\\&&&&&\sin\theta_{d/2-1}&\cos\theta_{d/2-1}\end{bmatrix}\mathbf{x}$$

其中$$\theta_i = n\cdot\frac{\pi}{10000^{2i/d}}$$，$$i = 0,1,\cdots,d/2 - 1$$。

RoPE通过这种旋转操作来对位置信息进行编码，使得模型能够更好地捕捉文本中的长期依赖关系等，在Transformer架构等中有着重要的应用。
