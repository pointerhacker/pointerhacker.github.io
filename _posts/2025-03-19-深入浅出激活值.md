---
layout: post
title: 激活内存：使用 PyTorch 的深入探讨
categories: [LLM]
tags: LLM
---
# 激活内存：使用 PyTorch 的深入探讨

首先，简要回顾一下激活值存储的来源。简单来说，模型参数是基于导数进行更新的。为了高效计算这些导数，必须缓存某些张量。激活记忆就是这些缓存张量的内存成本。

从更技术的角度来说，神经网络本质上是处理张量的数学函数。
对于输入$$ a $$，它们会产生一个输出$$ z = M(a) $$，其中 $$ M $$ 是模型。它们被训练以最小化某个标量损失函数
$$ L(z, \dots) $$，该函数依赖于模型输出和其他数据。
为了简洁起见，我们将省略张量索引，但张量可以具有几乎任意形状，并且在通过网络时会发生变化。通过基于损失的导数来更新模型 $$ M $$ 来最小化损失。这些导数包含了关于模型性能的信息。尽管我们最终只关心与可学习参数相关的导数，但在这些计算中还需要与非可学习的中间张量相关的导数。
具体的算法就是链式法则，也称为反向传播。

模型 $$ M $$ 是由许多单独的张量操作构建而成的，在最简单的情况下，这些操作的形式为
$$ y = f(x) $$，其中： 

- $$ f $$ 是一个操作，比如一个简单的逐元素激活函数，或者是一个包含可学习权重的矩阵乘法。

- $$ x $$ 和 $$ y $$ 是中间激活值。

如果我们知道损失相对于输出 $$ y $$ 的导数，那么我们也可以计算相对于 $$ x $$ 以及操作 $$ f $$ 内部任何张量的导数。

## 示例：矩阵乘法

具体来说，假设 $$ f $$ 是一个矩阵乘法操作：

$$ y = f(x) = W \cdot x $$，其中 $$ W $$ 是一个可学习的权重矩阵。
假设我们已经从之前的反向传播阶段得到了相对于输出的导数 $$ \frac{\partial L}{\partial y} $$，我们需要计算两个额外的梯度：

- 相对于 $$ W $$ 的导数，这样我们就可以更新这个权重。
- 相对于 $$ x $$ 的导数，这样我们就可以将反向传播算法继续向后传播到生成 $$ x $$ 的操作。
  前者的导数是（示意性地）

$$
\frac{\partial L}{\partial W} = \frac{\partial L}{\partial y} \frac{\partial y}{\partial W} = \frac{\partial L}{\partial y} \times x
$$

后者的导数是

$$
\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \cdot W
$$

因此，如下面的图所示，我们需要缓存输入张量 $$ x $$，以便能够计算我们关心的导数。
保存 $$ x $$ 的成本就是这个操作的激活内存来源。

<img src="http://pointerhacker.github.io/imgs/posts/activate_value/linear_act_mem.png" alt="linear_act_mem" style="zoom:25%;" />

> 在矩阵乘法（线性层）的前向传播过程中，输入 $$ x $$ 会被缓存起来，以便在反向传播时高效地计算相对于线性权重矩阵的导数。
> 为了进行这一计算，还需要相对于输出 $$ y $$ 的导数。
> 最后，为了继续进行反向传播，还会计算相对于输入的导数。
> 图中展示了这些操作在一个大型模型中的发生情况。

一般来说，在每个类型为 $$ y = f(x) $$ 的子操作中，可能会产生许多中间张量，这些张量是在生成输出 $$ y $$ 的过程中创建的，但并非所有中间张量都需要保存。高效的反向传播实现（例如 PyTorch）只会保存那些严格用于计算导数所必需的中间张量；其他临时张量将立即释放。下面这一点至关重要：我们可以仅根据某些激活函数的输出值来计算它们，而无需缓存其输入。

## 案例研究：Transformer MLP 层

我们将使用 Transformer 的 MLP 层（也称为前馈网络或 FFN 层）作为研究激活内存的详细测试对象。
下面可以找到示意图和相应的代码。

```python
class MLP(nn.Module):
    """
    Basic MLP (multi-layer perceptron) layer with optional Dropout.
    """

    def __init__(
        self,
        d_model: int,
        act_fn: nn.Module,
        dropout_prob: Optional[float] = None,
        device: Optional[Union[str, torch.device]] = None,
        dtype: Optional[torch.dtype] = None,
    ) -> None:
        super().__init__()
        self.d_model = d_model
        self.act_fn = act_fn
        self.dropout_prob = dropout_prob
        factory_kwargs = {"device": device, "dtype": dtype}

        self.lin_0 = nn.Linear(self.d_model, 4 * self.d_model, **factory_kwargs)
        self.lin_1 = nn.Linear(4 * self.d_model, self.d_model, **factory_kwargs)
        self.dropout = nn.Dropout(self.dropout_prob) if self.dropout_prob else None

    def forward(self, inputs: torch.Tensor) -> torch.Tensor:
        x = self.lin_0(inputs)
        x = self.act_fn(x)
        x = self.lin_1(x)
        if self.dropout is not None:
            x = self.dropout(x)
        return x
```

<img src="http://pointerhacker.github.io/imgs/posts/activate_value/mlp.png" alt="Transformer MLP layer." style="zoom:25%;" />



> 基本 MLP 层的示意图，如在《Reducing Activation Recomputation in Large Transformer Models》中分析的那样。 在第一个线性层之后，张量的大小扩大了四倍：大部分激活内存来自这些扩展后的中间张量。 在 2024 年的 LLM 中，Dropout 并不常用，因此我们省略了它。 是否为反向传播缓存各种激活张量已在图中表明。

### 经典分析

这一模块的激活内存是在《Reducing Activation Recomputation in Large Transformer Models》中分析的，结果仅仅是将所有中间张量的字节数相加。
他们对 GELU 激活函数的分析结果是准确的，但我们在接下来的部分将解释如何通过改变激活函数来大幅削减激活内存成本。

以下是对他们对批量大小（b）、序列长度（s）和模型维度（d）的推导的简要回顾。
相关张量的大小为：

- 第一个线性层的输入大小为 $$ (b, s, d) $$。
- 激活函数的输入大小为 $$ (b, s, 4 \times d) $$，因为第一个线性层将隐藏维度扩展了四倍。
- 最后一个线性层的输入大小为 $$ (b, s, 4 \times d) $$。
- 如果适用，Dropout 掩码的大小为 $$ (b, s, d) $$。
  前三项（总共有 $$ 9 \times b \times s \times d $$ 个元素）与初始输入具有相同的 dtype。
  假设前向传播以较低精度执行，例如 torch.bfloat16，每个元素占用 2 个字节，那么这些张量的激活内总共字节数为 $$ \text{act\_mem\_mlp} = 18 \times b \times s \times d $$。如果使用 Dropout，其掩码的类型为 torch.bool，其元素令人困惑地占用 1 个字节（而不是位），因此将额外增加 $$ b \times s \times d $$ 个字节。



### 内存最优激活函数

虽然 GELU 的输入需要保存用于反向传播，但并非所有激活函数都是如此。对于某些函数，导数可以完全从输出值计算得出。

对于计算 $$ y = f(x) $$ 的激活函数 $$ f $$，我们需要计算 $$ \frac{\partial y}{\partial x} $$。GELU 函数的（近似）形式如下：

$$ y = x \times \tanh\left(\sqrt{\frac{2}{\pi}}(x + 0.044715x^3)\right) $$

其导数同样复杂。在这种情况下，无法用 $$ y $$ 来表示 $$ \frac{\partial y}{\partial x} $$，因此我们必须缓存（或重新计算）输入以获取导数的值。

然而，对于像 ReLU 和 Tanh 这样的特殊激活函数，我们无需保存输入，因为可以用 $$ y $$ 单独表示 $$ \frac{\partial y}{\partial x} $$。ReLU 的形式为：

$$ y = \text{ReLU}(x) = \begin{cases} x & \text{if } x > 0 \\ 0 & \text{if } x < 0 \end{cases} $$

其导数非常简单：

$$ \frac{dy}{dx} = \frac{d\text{ReLU}(x)}{dx} = \begin{cases} 1 & \text{if } x > 0 \\ 0 & \text{if } x < 0 \end{cases} $$

实际上，我们也可以用输出 $$ y $$ 来表示：

$$ \frac{dy}{dx} = \begin{cases} 1 & \text{if } y > 0 \\ 0 & \text{if } y < 0 \end{cases} $$

Tanh 也具有这一特性，因为存在以下关系：

$$ \frac{dy}{dx} = \frac{d\text{tanh}(x)}{dx} = 1 - \text{tanh}(x)^2 = 1 - y^2 $$

一般来说，内存最优激活函数的导数可以表示为 $$ \frac{dy}{dx} = g(y) $$ 的形式，其中 $$ g $$ 是某个函数，这意味着它们也必须是单调的。它们是自治微分方程的一个特例，正如在 Math Stack Exchange 的这个帖子中所指出的。

在 Transformer 的 MLP 层中，我们已经需要保存激活函数的输出，因为它们成为最终线性层的输入，而从上一节我们知道，这些输入是计算线性权重导数所必需的。因此，如果我们使用具有上述特殊属性的激活函数，我们可以使用我们无论如何都需要缓存的数据来计算激活函数的导数，从而避免保存第一线性层相对较大的输出。这几乎可以节省一半的内存：非 dropout 激活内存将从 $$ 18 \times b \times s \times d $$ 减少到 $$ 10 \times b \times s \times d $$。

当然，实际的反向传播实现必须在代码中利用这些特殊属性才能实现这些节省。幸运的是，PyTorch 在很大程度上做到了这一点。ReLU 的导数在这些行中定义（这用于在构建时自动生成代码），并由简单的 threshold_backward(grad, result, 0) 实现，它强制执行上述数学逻辑，其中 result 是 ReLU 的输出。将其与在此处定义的 GELU 导数进行比较，后者引用了 self，即输入张量，而不是 result。

在实践中（在撰写本文时），LeakyReLU 默认的 inplace=False 设置可以默认使用较少的内存，但实际上并非如此。该函数为：

$$ y = \text{LeakyReLU}(x) = \begin{cases} x & \text{if } x > 0 \\ s \times x & \text{if } x < 0 \end{cases} $$

对于某个数 $$ s $$。如果 $$ s \geq 0 $$（如典型用法），则导数可以类似地表示为 ReLU 情况：

$$ \frac{dy}{dx} = \begin{cases} 1 & \text{if } y > 0 \\ s & \text{if } y < 0 \end{cases} $$

然而，将 inplace=True 设置在 LeakyReLU 中确实可以实现预期的内存节省。（在普通 ReLU 函数中设置 inplace=True 是不必要的。）





## 测量激活内存

以上是理论。现在我们转向代码：如何在 PyTorch 中跟踪缓存的张量并计算激活内存。

在正向传播过程中缓存的张量可以通过 `saved_tensors_hooks` API 访问，而整体内存读数（在 CUDA 上）可以通过 `torch.cuda.memory_stats` 访问。接下来我们将使用这两种工具。

### 测量 CUDA 内存

`torch.cuda.memory_stats` 包含了海量的信息，其中并非所有内容都与我们相关。我们将利用这一函数构建一个上下文管理器，其使用方式如下：

```python
with AllocatedMemContext() as mem:
    loss = Model(inputs)  # Some CUDA computation.

# Memory stats before the computation:
mem.before

# Memory stats after the computation:
mem.after

# Change in memory stats:
mem.delta
```



为了说明各种字典所包含的字段，考虑以下简单示例：

```python
with AllocatedMemContext() as mem:
    t1 = torch.randn(2**8, device="cuda")  # 1 KiB
    t2 = torch.randn(2**8, device="cuda")  # 1 KiB
    del t2
    t3 = torch.randn(2**8, device="cuda")  # 1 KiB
    del t3
print(f"{mem.delta=}")
```



它打印出 `mem.delta={'allocated': 3072, 'current': 1024, 'freed': 2048, 'peak': 2048}`，表示内存使用的变化。这些字段的含义如下：

- **allocated**：新分配的字节数
- **current**：新创建且仍然存活的张量所占用的字节数
- **freed**：释放的字节数
- **peak**：峰值内存使用的变化

我们可以看到这些读数是有意义的：在上面的代码中，我们分配了两个大小为 1 KiB 的张量 `t1` 和 `t2`（`allocated = 3072`），在任何给定时刻最多有两个张量存活（`peak = 2048`）。我们删除了其中两个张量（`freed = 2048`），只有一个张量存活下来（`current = 1024`）。关于 CUDA 内存的更多信息，可以参考 Zachary Devito 关于 PyTorch CUDA 缓存分配器的优秀博客文章。

> **警告**
> CUDA 库是延迟加载的，必须已经加载到设备上，才能获得准确的内存读数。例如，执行的第一个矩阵乘法操作会导致大约 8 MiB 的库字节被加载，可能会歪曲 `memory_stats` 的结果。`AllocatedMemContext` 在初始化时调用 `torch.cuda.current_blas_handle()`，这确保了在读取数据之前这些库已经被加载。

以下是上下文管理器的完整代码：

```python
class AllocatedMemContext:
    def __init__(self) -> None:
        # Ensure CUDA libraries are loaded:
        torch.cuda.current_blas_handle()

        self.before: dict[str, int] = {}
        self.after: dict[str, int] = {}
        self.delta: dict[str, int] = {}

    def _get_mem_dict(self) -> dict[str, int]:
        # Only need `allocated_bytes.all`-prefixed keys here
        key_prefix = "allocated_bytes.all."
        return {
            k.replace(key_prefix, ""): v
            for k, v in torch.cuda.memory_stats().items()
            if key_prefix in k
        }

    def __enter__(self) -> "AllocatedMemContext":
        self.before = self._get_mem_dict()
        return self

    def __exit__(self, *args: Any, **kwargs: Any) -> None:
        self.after = self._get_mem_dict()
        self.delta = {k: v - self.before[k] for k, v in self.after.items()}
```



### 保存的张量

现在我们将构建一个上下文管理器，用于捕获在反向传播中使用的已保存张量。`saved_tensors_hooks` API 将允许我们捕获对所有缓存张量的引用。

该 API 的使用方式如下：

```python
model = MyModel(...)
with SavedTensorContext(ignored_tensors=model.parameters()) as saved:
    outputs = model(inputs)

# A dictionary whose keys are the cached tensors
saved.saved_tensors_dict

# The bytes from the cached tensors
saved.saved_tensor_mem
```



主要的难点在于识别这些张量中哪些真正对应于单独的内存分配。

要了解这个问题，考虑某个线性层（`lin`）的权重。我们不希望其权重（`lin.weight`）计入激活内存成本，因为它们已经在参数内存预算中被计算过了。然而，由于在反向传播中需要使用权重（如上面的矩阵乘法示例所示），权重将被 `saved_tensors_hooks` 捕获的张量之一。我们希望从 `saved_tensor_mem` 中排除权重的字节（这就是 `ignored_tensors` 参数的作用），但问题在于在这种情况下，引用实际上将是权重矩阵的转置。这意味着简单的测试，如 `lin.weight is saved_tensor` 或 `lin.weight == saved_tensor`，将无法捕捉到 `saved_tensor` 实际上只是我们已经跟踪内存的对象的一个视图这一事实。

一般来说，PyTorch 尽可能使用视图来避免新的分配。在上面的例子中，`lin.weight` 及其转置 `lin.weight.T` 对应于同一块内存，只是以不同的方式索引该内存。再举一个例子，考虑将张量分割成几部分，如下所示：

```python
t = torch.randn(16, device="cuda")
split_t = t.split(4)  # A tuple of four tensors
```



`split_t` 中的四个张量只是原始张量 `t` 的视图。分割操作不会占用额外的 CUDA 内存（可以通过 `AllocatedMemContext` 检查）。

那么，我们如何判断两个张量是否代表同一块 CUDA 内存的切片呢？PyTorch 提供了一个简单的解决方案：每个张量都持有一个指向 `Storage` 类的引用，该类表示底层内存，而 `Storage` 类又有一个 `data_ptr` 方法，指向张量存储在内存中的第一个元素。如果两个张量的存储的 `data_ptr` 匹配，则它们来自同一块分配的内存。继续上面的例子，以下测试通过：

```python
assert all(
    s.untyped_storage().data_ptr() == t.untyped_storage().data_ptr() 
    for s in split_t
)
assert (
    lin.weight.untyped_storage().data_ptr() == lin.weight.T.untyped_storage().data_ptr()
)
```

> **警告**
> 张量本身也有 `data_ptr` 方法，但这些方法返回张量视图的第一个元素的内存索引，这通常与存储持有的第一个元素不同。例如，这会导致 `assert all(s.data_ptr() == t.data_ptr() for s in split_t)` 失败。

以下是我们的上下文管理器，它捕获所有用于反向传播的张量的引用，但只计算来自不同分配的内存：

```python
class SavedTensorContext:
    def __init__(
        self,
        ignored_tensors: Optional[Iterable[torch.Tensor]] = None,
    ) -> None:
        self._ignored_data_ptrs = (
            set()
            if ignored_tensors is None
            else {t.untyped_storage().data_ptr() for t in ignored_tensors}
        )

        self.saved_tensor_dict = torch.utils.weak.WeakTensorKeyDictionary()

        def pack_hook(saved_tensor: torch.Tensor) -> torch.Tensor:
            data_ptr = saved_tensor.untyped_storage().data_ptr()
            if data_ptr not in self._ignored_data_ptrs:
                self.saved_tensor_dict[saved_tensor] = data_ptr
            return saved_tensor

        def unpack_hook(saved_tensor: torch.Tensor) -> torch.Tensor:
            return saved_tensor

        self._saved_tensors_hook = torch.autograd.graph.saved_tensors_hooks(
            pack_hook, unpack_hook
        )

    def __enter__(self) -> "SavedTensorContext":
        self._saved_tensors_hook.__enter__()
        return self

    def __exit__(self, *args: Any, **kwargs: Any) -> None:
        self._saved_tensors_hook.__exit__(*args, **kwargs)

    @property
    def saved_tensor_mem(self) -> int:
        """
        The memory in bytes of all saved tensors, accounting for views into the same storage.
        """
        accounted_for = self._ignored_data_ptrs.copy()
        total_bytes = 0
        for t in self.saved_tensor_dict:
            data_ptr = t.untyped_storage().data_ptr()
            if data_ptr not in accounted_for:
                total_bytes += t.untyped_storage().nbytes()
                accounted_for.add(data_ptr)
        return total_bytes
```



### 示例：MLP 块

让我们使用这些工具来验证上面对 MLP 块的分析。使用 `torch.bfloat16` 格式（为简单起见避免混合精度），我们将：

1. 遍历 MLP 层的 GELU 和 ReLU 版本。
2. 测量生成的 CUDA 内存并捕获激活。
3. 检查保存的激活内存是否与测量的内存一致。
4. 打印内存读数及其比率。

代码如下：



```python
batch_size, seq_len, d_model = 2, 4096, 1024
dtype = torch.bfloat16
inputs = torch.randn(
    batch_size,
    seq_len,
    d_model,
    device="cuda",
    requires_grad=True,
    dtype=dtype,
)

act_fn_dict = {"ReLU": nn.ReLU(), "GELU": nn.GELU()}
# Append outputs to a list to keep tensors alive
outputs = []
mem_bytes = []

for name, act_fn in act_fn_dict.items():
    mlp = layers.MLP(
        d_model=d_model,
        act_fn=act_fn,
        device="cuda",
        dtype=dtype,
    )
    with act_mem.AllocatedMemContext() as mem, act_mem.SavedTensorContext(
        ignored_tensors=mlp.parameters()
    ) as saved:
        out = mlp(inputs)
        outputs.append(out)
    assert mem.delta["current"] == saved.saved_tensor_mem
    print(f"{name} bytes: {saved.saved_tensor_mem}")
    mem_bytes.append(saved.saved_tensor_mem)

print(f"ReLU/GeLU act mem ratio: {mem_bytes[0]/mem_bytes[1]}")
```



And the result:

```
ReLU bytes: 83886080
GELU bytes: 150994944
ReLU/GeLU act mem ratio: 0.5555555555555556
```



我们发现这与上文的分析完全一致：ReLU 利用微积分将内存几乎减半。如果我们查看两种情况下 `saved.saved_tensor_dict` 中的实际张量，我们会看到在 GELU 情况下额外缓存的具体张量。

### **Transformer 块分析**

最后，我们简要分析整个 Transformer 块（包括多头注意力和残差连接）的节省情况。当使用高效的注意力机制实现（如 `F.scaled_dot_product_attention`）时，注意力块的激活内存大约为 $$10 \times b \times s \times d$$（对于 `torch.bfloat16`）。残差连接不会增加额外的激活内存，因为它们是简单的加法运算，其导数与输入无关。

经过计算，在块级别用内存最优的激活函数替换 GELU 应该会节省大约 25% 的激活内存。用完整的 Transformer 块替换上述脚本中的 MLP 层运行，结果如下：

```
ReLU block bytes: 201523216
GELU block bytes: 268632080
ReLU/GeLU block act mem ratio: 0.7501829863358092
```

要运行上述代码，请查看 [GitHub 仓库](https://github.com/determined-ai/determined-examples/tree/main/blog/act-mem-2)。

最后一点：机器学习和生活一样，充满了权衡。尽管像 ReLU 和 Tanh 这样的激活函数可以节省大量内存，但 GELU 在实证上被认为表现更好。哪种激活函数适合你，取决于你的具体需求和资源。

## **总结**

在本篇博客文章中，我们展示了如何构建简单的工具来深入了解 PyTorch 自动微分反向传播引擎的内存使用情况，并对某些激活函数的内存优势进行了详细分析。

这些工具还可以做更多的事情，例如，它们可以用来深入了解 PyTorch 的即时混合精度自动转换（autocast）的工作原理，但今天我们就讲到这里。
